{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gkte68aIhTy"
      },
      "source": [
        "#COMP 565 Assignment 3 - Exploring genome language models with HyenaDNA\n",
        "\n",
        "This notebook is adapted from the Colab notebook available at https://github.com/HazyResearch/hyena-dna.\n",
        "\n",
        "In this assignment, you will directly modify the code blocks as instructed below (look for **A3 task xx **) to perform the following tasks:\n",
        "\n",
        "1.   Train on the GenomicBenchmarks dataset from scratch and fine-tune (sequence level classification tasks)\n",
        "2.   Do inference on sequences to examine the sequence embedding via t-SNE visualization\n",
        "3.   Implement next token prediction\n",
        "\n",
        "Open the \"Table of contents\" on the left to help you navigate the codeblocks and identify the ones that require modifications.\n",
        "\n",
        "For the rest of the codeblocks, simply click through them **without making any change**. You are encouraged to read and understand each codeblock. But we will only grade the assigned code blocks and the desired outputs.\n",
        "\n",
        "You are recommended to run the code using Colab. By the default Colab connects to a CPU node. To speed up the training, you may connect with a T4 GPU by choosing 'Runtime' in the top menu and then clicking on 'Change runtime type' and choose 'T4 GPU'. Note that for free Google account, we can keep the notebook connected to a GPU for at most 12 hours. Nonetheless, all the training and inference required to complete this assignment can be done within 5 minutes even with a CPU.\n",
        "\n",
        "#Deliverabe\n",
        "Submit your completed notebook to MyCourses.\n",
        "\n",
        "#Late submission policy\n",
        "For late submission, $2^k$ percent will be deducted per $k$ days of the delay.\n",
        "To use your 6-day quota as a team, submit your request by emailing Liam Hodgson (liam.hodgson@mcgill.ca) with subject title ``Assignment 3 extension request\" and in the email body specify the number of days (max at 6 days) you need to submit your assignment. You can only submit the request ONCE. Therefore, plan and use your quota wisely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JWUc70H7Iiwd",
        "outputId": "ce48c13a-ef6c-4b9a-b3e2-c75b8113a77c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->torchvision) (3.0.2)\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2024.8.30)\n",
            "Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.26.1\n",
            "Collecting genomic-benchmarks\n",
            "  Downloading genomic_benchmarks-0.0.9.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting biopython>=1.79 (from genomic-benchmarks)\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (2.32.3)\n",
            "Requirement already satisfied: pip>=20.0.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (24.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (4.66.6)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (6.0.2)\n",
            "Requirement already satisfied: gdown>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (5.2.0)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->genomic-benchmarks) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->genomic-benchmarks) (3.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (2024.8.30)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl->genomic-benchmarks) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl->genomic-benchmarks) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict>=4.0->yarl->genomic-benchmarks) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->genomic-benchmarks) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.2.0->genomic-benchmarks) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.2.0->genomic-benchmarks) (1.7.1)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: genomic-benchmarks\n",
            "  Building wheel for genomic-benchmarks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for genomic-benchmarks: filename=genomic_benchmarks-0.0.9-py3-none-any.whl size=22506 sha256=59faf7b16f858698f12c272219ee9f6c9977b780505ae9538c322912d109c69f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/78/72/de9d75be800b6d679b8814b9663b3f610c34188b443b8b9b77\n",
            "Successfully built genomic-benchmarks\n",
            "Installing collected packages: biopython, genomic-benchmarks\n",
            "Successfully installed biopython-1.84 genomic-benchmarks-0.0.9\n",
            "Collecting OmegaConf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from OmegaConf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (6.0.2)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=f8709a5d96a97479c528c112f13e107f9866cfc9193b3b39a5e5dd57fd5083cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, OmegaConf\n",
            "Successfully installed OmegaConf-2.3.0 antlr4-python3-runtime-4.9.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "c546ff013b754f78bb7b6e0423385727"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Installs\n",
        "!pip install einops\n",
        "!pip install torchvision\n",
        "!pip install transformers==4.26.1\n",
        "!pip install genomic-benchmarks\n",
        "!pip install OmegaConf\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WIH4-WEI4in"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "# for HyenaDNA specifically\n",
        "import torch\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from einops import rearrange\n",
        "from typing import Optional\n",
        "from functools import partial\n",
        "from torch import Tensor\n",
        "from torchvision.ops import StochasticDepth\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu3UYdFqKzTs"
      },
      "source": [
        "# HyenaDNA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWJZc3volbL6"
      },
      "outputs": [],
      "source": [
        "#@title Hyena layer\n",
        "\n",
        "\n",
        "def fftconv(u, k, D):\n",
        "    \"\"\"\n",
        "    We apply a convolution through the fourier domain (from the Convolution Theorem)\n",
        "\n",
        "    \"\"\"\n",
        "    seqlen = u.shape[-1]\n",
        "    fft_size = 2 * seqlen\n",
        "\n",
        "    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n",
        "    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n",
        "\n",
        "    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n",
        "    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n",
        "\n",
        "    out = y + u * D.unsqueeze(-1)\n",
        "    return out.to(dtype=u.dtype)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def mul_sum(q, y):\n",
        "    return (q * y).sum(dim=1)\n",
        "\n",
        "class OptimModule(nn.Module):\n",
        "    \"\"\" Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters \"\"\"\n",
        "\n",
        "    def register(self, name, tensor, lr=None, wd=0.0):\n",
        "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
        "\n",
        "        if lr == 0.0:\n",
        "            self.register_buffer(name, tensor)\n",
        "        else:\n",
        "            self.register_parameter(name, nn.Parameter(tensor))\n",
        "\n",
        "            optim = {}\n",
        "            if lr is not None: optim[\"lr\"] = lr\n",
        "            if wd is not None: optim[\"weight_decay\"] = wd\n",
        "            setattr(getattr(self, name), \"_optim\", optim)\n",
        "\n",
        "\n",
        "class Sin(nn.Module):\n",
        "    \"\"\"The Sin activation function for the Hyena Filter function.\"\"\"\n",
        "    def __init__(self, dim, w=10, train_freq=True):\n",
        "        super().__init__()\n",
        "        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.freq * x)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(OptimModule):\n",
        "    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):\n",
        "        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        # The time embedding fed to the filteres is normalized so that t_f = 1\n",
        "        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n",
        "\n",
        "        if emb_dim > 1:\n",
        "            bands = (emb_dim - 1) // 2\n",
        "        # To compute the right embeddings we use the \"proper\" linspace\n",
        "        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None] # scaled time component\n",
        "        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1\n",
        "\n",
        "        f = torch.linspace(1e-4, bands - 1, bands)[None, None] # The resulting tensor will contain bands number of values, evenly spaced between 1e-4 and bands - 1. [None, None] adds two extra dimensions to the tensor\n",
        "        # creates a tensor f representing a range of frequencies used in the positional embedding calculation.\n",
        "        z = torch.exp(-1j * f * w) # 1j is the imaginary unit. result z is a complex-valued tensor representing the positional embedding.\n",
        "        z = torch.cat([t, z.real, z.imag], dim=-1)\n",
        "        self.register(\"z\", z, lr=lr_pos_emb)\n",
        "        self.register(\"t\", t, lr=0.0)\n",
        "\n",
        "    def forward(self, L):\n",
        "        return self.z[:, :L], self.t[:, :L]\n",
        "\n",
        "\n",
        "class ExponentialModulation(OptimModule):\n",
        "    \"\"\"The window function applied to the output of the (MLP) filter function.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        fast_decay_pct=0.3,\n",
        "        slow_decay_pct=1.5,\n",
        "        target=1e-2,\n",
        "        modulation_lr=0.0,\n",
        "        modulate: bool=True,\n",
        "        shift: float = 0.05,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.modulate = modulate\n",
        "        self.shift = shift\n",
        "        max_decay = math.log(target) / fast_decay_pct\n",
        "        min_decay = math.log(target) / slow_decay_pct\n",
        "        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None] # this is the decay rates\n",
        "        self.register(\"deltas\", deltas, lr=modulation_lr)\n",
        "\n",
        "    def forward(self, t, x): # here t is \\delta t?\n",
        "        if self.modulate:\n",
        "            decay = torch.exp(-t * self.deltas.abs())\n",
        "            x = x * (decay + self.shift)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HyenaFilter(OptimModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model,\n",
        "            emb_dim=3, # dim of input to MLP, augments with positional encoding\n",
        "            order=16, # width of the implicit MLP -> num of hidden units\n",
        "            fused_fft_conv=False,\n",
        "            seq_len=1024,\n",
        "            lr=1e-3,\n",
        "            lr_pos_emb=1e-5,\n",
        "            dropout=0.0,\n",
        "            w=1, # frequency of periodic activations\n",
        "            wd=0, # weight decay of kernel parameters\n",
        "            bias=True,\n",
        "            num_inner_mlps=2,\n",
        "            normalized=False,\n",
        "            **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Implicit long filter with modulation.\n",
        "\n",
        "        Args:\n",
        "            d_model: number of channels in the input\n",
        "            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n",
        "            order: width of the FFN\n",
        "            num_inner_mlps: number of inner linear layers inside filter MLP\n",
        "\n",
        "        Note:\n",
        "            filter_dropout is not implemented\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.use_bias = bias\n",
        "        self.fused_fft_conv = fused_fft_conv\n",
        "        self.bias = nn.Parameter(torch.randn(self.d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        act = Sin(dim=order, w=w)\n",
        "        self.emb_dim = emb_dim\n",
        "        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n",
        "\n",
        "        # MLP(by default hidden layers = 2, hidden units = 16)\n",
        "        self.implicit_filter = nn.Sequential(\n",
        "            nn.Linear(emb_dim, order),\n",
        "            act,\n",
        "        )\n",
        "        for i in range(num_inner_mlps):\n",
        "            self.implicit_filter.append(nn.Linear(order, order))\n",
        "            self.implicit_filter.append(act)\n",
        "\n",
        "        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n",
        "\n",
        "        self.modulation = ExponentialModulation(d_model, **kwargs)\n",
        "\n",
        "        self.normalized = normalized\n",
        "\n",
        "        # Setting optimizer-specific hyperparams\n",
        "        for c in self.implicit_filter.children():\n",
        "            for name, v in c.state_dict().items():\n",
        "                optim = {\"weight_decay\": wd, \"lr\": lr}\n",
        "                setattr(getattr(c, name), \"_optim\", optim)\n",
        "\n",
        "    def filter(self, L, *args, **kwargs):\n",
        "        z, t = self.pos_emb(L)\n",
        "        h = self.implicit_filter(z)\n",
        "        h = self.modulation(t, h)\n",
        "        return h\n",
        "\n",
        "    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n",
        "        if k is None: k = self.filter(L)\n",
        "\n",
        "        # Ensure compatibility with filters that return a tuple\n",
        "        k = k[0] if type(k) is tuple else k\n",
        "\n",
        "        y = fftconv(x, k, bias)\n",
        "        return y\n",
        "\n",
        "\n",
        "class HyenaOperator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model,\n",
        "            l_max,\n",
        "            order=2,\n",
        "            filter_order=64,\n",
        "            dropout=0.0,\n",
        "            filter_dropout=0.0,\n",
        "            **filter_args,\n",
        "        ):\n",
        "        r\"\"\"\n",
        "        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimension of the input and output embeddings (width of the layer)\n",
        "            l_max: (int): Maximum input sequence length. Defaults to None\n",
        "            order: (int): Depth of the Hyena recurrence. Defaults to 2\n",
        "            dropout: (float): Dropout probability. Defaults to 0.0\n",
        "            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.l_max = l_max\n",
        "        self.order = order\n",
        "        inner_width = d_model * (order + 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.in_proj = nn.Linear(d_model, inner_width)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.short_filter = nn.Conv1d(\n",
        "            inner_width,\n",
        "            inner_width,\n",
        "            3,\n",
        "            padding=2,\n",
        "            groups=inner_width # This effectively makes the convolution operate as a depthwise convolution,\n",
        "                               # where each input channel is processed independently by a separate filter.\n",
        "        )\n",
        "        self.filter_fn = HyenaFilter(\n",
        "            d_model * (order - 1),\n",
        "            order=filter_order,\n",
        "            seq_len=l_max,\n",
        "            channels=1,\n",
        "            dropout=filter_dropout,\n",
        "            **filter_args\n",
        "        )\n",
        "\n",
        "    def forward(self, u, *args, **kwargs):\n",
        "\n",
        "        l = u.size(-2) # extract seq length from input tensor\n",
        "        l_filter = min(l, self.l_max)\n",
        "        u = self.in_proj(u) # find the linear projection of input seq (linear layer)\n",
        "        u = rearrange(u, 'b l d -> b d l') # rearranges the tensor dimensions for compatibility with the convolutional layer.\n",
        "\n",
        "        uc = self.short_filter(u)[...,:l_filter] # apply the short depthwise 1d conv filter\n",
        "        *x, v = uc.split(self.d_model, dim=1) # splitting along channel dim to x1,...,xn, v each with dim DxL\n",
        "\n",
        "        # ----------------- Long Filter ---------------------\n",
        "\n",
        "        k = self.filter_fn.filter(l_filter)[0] # takes the filter length and generates filter weights k for the long filter\n",
        "        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)\n",
        "        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)\n",
        "\n",
        "        for o, x_i in enumerate(reversed(x[1:])): #\n",
        "            v = self.dropout(v * x_i)\n",
        "            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])\n",
        "\n",
        "        y = rearrange(v * x[0], 'b d l -> b l d')\n",
        "\n",
        "        y = self.out_proj(y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2I7LjzVvr9fp"
      },
      "outputs": [],
      "source": [
        "#@title Self-Attention (alternative)\n",
        "\n",
        "\"\"\"\n",
        "If you'd like to try the HyenaDNA model using attention instead, you can. ie,\n",
        "use a regular decoder only Transformer.\n",
        "\n",
        "Borrowed from the FlashAttention library by Tri Dao.\n",
        "\"\"\"\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Implement the scaled dot product attention with softmax.\n",
        "    Arguments\n",
        "    ---------\n",
        "        softmax_scale: The temperature to use for the softmax attention.\n",
        "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
        "                      runtime)\n",
        "        attention_dropout: The dropout rate to apply to the attention\n",
        "                           (default: 0.0)\n",
        "    \"\"\"\n",
        "    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.causal = causal\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.dropout_p = attention_dropout\n",
        "\n",
        "    def forward(self, qkv, causal=None, key_padding_mask=None):\n",
        "        \"\"\"Implements the multihead softmax attention.\n",
        "        Arguments\n",
        "        ---------\n",
        "            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)\n",
        "            causal: if passed, will override self.causal\n",
        "            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,\n",
        "                False means to mask out. (B, S)\n",
        "        \"\"\"\n",
        "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
        "        causal = self.causal if causal is None else causal\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
        "        scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
        "        if key_padding_mask is not None:\n",
        "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype,\n",
        "                                      device=scores.device)\n",
        "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
        "            scores = scores + rearrange(padding_mask, 'b s -> b 1 1 s')\n",
        "        if causal:\n",
        "            # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
        "            # So we have to construct the mask in float\n",
        "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
        "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
        "        attention_drop = F.dropout(attention, self.dropout_p if self.training else 0.0)\n",
        "        output = torch.einsum('bhts,bshd->bthd', attention_drop, v)\n",
        "        return output\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    \"\"\"Multi-head self-attention and cross-attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, bias=True, dropout=0.0,\n",
        "                 softmax_scale=None, causal=False, layer_idx=None, dwconv=False,return_residual=False,device=None, dtype=None) -> None:\n",
        "        \"\"\"\n",
        "            return_residual: whether to return the input x along with the output. This is for\n",
        "                performance reason: for post-norm architecture, returning the input allows us\n",
        "                to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.causal = causal\n",
        "        self.layer_idx = layer_idx\n",
        "        self.dwconv = dwconv\n",
        "        self.return_residual = return_residual\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n",
        "        self.head_dim = self.embed_dim // num_heads\n",
        "\n",
        "        linear_cls = nn.Linear\n",
        "        linear_resid_cls = LinearResidual\n",
        "        inner_attn_cls =  SelfAttention\n",
        "\n",
        "        if not self.return_residual:\n",
        "            self.Wqkv = linear_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
        "        else:\n",
        "            self.Wqkv = linear_resid_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
        "        if self.dwconv:\n",
        "            self.dwconv_qkv = nn.Conv1d(3 * embed_dim, 3 * embed_dim, kernel_size=3, padding=2,\n",
        "                                        groups=3 * embed_dim)\n",
        "\n",
        "        self.inner_attn = inner_attn_cls(causal=causal, softmax_scale=softmax_scale,\n",
        "                                         attention_dropout=dropout)\n",
        "\n",
        "        # output projection always have the bias (for now)\n",
        "        self.out_proj = linear_cls(embed_dim, embed_dim, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
        "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
        "                is the is the sum of the sequence lengths in the batch.\n",
        "            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
        "                of the sequences in the batch, used to index into x. Only applicable when using\n",
        "                FlashAttention.\n",
        "            max_seqlen: int. Maximum sequence length in the batch.\n",
        "            key_padding_mask: boolean mask, True means to keep, False means to mask out.\n",
        "                (batch, seqlen). Only applicable when not using FlashAttention.\n",
        "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
        "                before applying the query projection. Useful for e.g., ViT where we only care\n",
        "                about the CLS token in the last layer.\n",
        "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
        "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
        "        \"\"\"\n",
        "\n",
        "        kwargs = ({'key_padding_mask': key_padding_mask, **kwargs})\n",
        "\n",
        "        if not self.return_residual:\n",
        "            qkv = self.Wqkv(x)\n",
        "        else:\n",
        "            qkv, x = self.Wqkv(x)\n",
        "        if self.dwconv:\n",
        "            qkv = rearrange(self.dwconv_qkv(rearrange(qkv, 'b s d -> b d s'))[..., :-2],\n",
        "                            'b d s -> b s d').contiguous()\n",
        "        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, d=self.head_dim)\n",
        "\n",
        "        context = self.inner_attn(qkv, **kwargs)\n",
        "\n",
        "        out = self.out_proj(rearrange(context, '... h d -> ... (h d)'))\n",
        "        return out if not self.return_residual else (out, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgkZggOetkbR"
      },
      "outputs": [],
      "source": [
        "#@title MLP layer\n",
        "\n",
        "\"\"\"\n",
        "The MLP layer after the mixer layer (HyenaOperator).\n",
        "\"\"\"\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,\n",
        "                 return_residual=False, device=None, dtype=None):\n",
        "        \"\"\"\n",
        "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py\n",
        "        \"\"\"\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.return_residual = return_residual\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)\n",
        "        self.activation = activation\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.fc1(x)\n",
        "        y = self.activation(y)\n",
        "        y = self.fc2(y)\n",
        "        return y if not self.return_residual else (y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAC-fIFFstei"
      },
      "outputs": [],
      "source": [
        "#@title Block layer (Hyena + MLP layers)\n",
        "\n",
        "\"\"\"\n",
        "A block consists of a Mixer layer (Hyena or attention), and a MLP layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class LinearResidual(nn.Linear):\n",
        "    \"\"\"Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return super().forward(input), input\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,\n",
        "                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,\n",
        "                 drop_path1=0., drop_path2=0.,\n",
        "                 return_residual=False,\n",
        "                 residual_in_fp32=False):\n",
        "        \"\"\"\n",
        "        can operate in either prenorm or postnorm configurations.\n",
        "            Prenorm applies normalization before applying the layer transformation.\n",
        "            Postnorm applies normalization after transformation.\n",
        "\n",
        "\n",
        "\n",
        "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py\n",
        "        For prenorm=True, this Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both\n",
        "        the hidden_states (output of the MLP) and the residual.\n",
        "        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        For prenorm=False, this Block has the same structure as a regular postnorm Transformer\n",
        "        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.\n",
        "        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.\n",
        "        This is for performance reason: for post-norm architecture, returning the input allows us\n",
        "        to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.prenorm = prenorm\n",
        "        self.return_residual = return_residual\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        if self.residual_in_fp32:\n",
        "            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'\n",
        "        if mixer_cls is None:\n",
        "            mixer_cls = partial(MHA, num_heads=dim // 64)\n",
        "        if mlp_cls is None:\n",
        "            mlp_cls = partial(Mlp, hidden_features=4 * dim)\n",
        "        self.mixer = mixer_cls() # class for mixer layer: HyenaOperator/MHA\n",
        "        self.dropout1 = dropout_cls(resid_dropout1) # dropout prob for residual connections\n",
        "        self.drop_path1 = StochasticDepth(drop_path1, mode='row') # drop path prob for stochastic depth\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.mlp = mlp_cls(dim)\n",
        "        if not isinstance(self.mlp, nn.Identity):\n",
        "            self.dropout2 = dropout_cls(resid_dropout2)\n",
        "            self.drop_path2 = StochasticDepth(drop_path2, mode='row')\n",
        "            self.norm2 = norm_cls(dim)\n",
        "\n",
        "    def forward(self, hidden_states, residual = None,\n",
        "                mixer_subset=None, mixer_kwargs=None):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))\n",
        "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
        "                before applying the query projection. Useful for e.g., ViT where we only care\n",
        "                about the CLS token in the last layer.\n",
        "        \"\"\"\n",
        "        if self.prenorm:\n",
        "            dropped = self.drop_path1(self.dropout1(hidden_states))\n",
        "            residual = (dropped + residual) if residual is not None else dropped\n",
        "            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "            if mixer_kwargs is None:\n",
        "                mixer_kwargs = {}\n",
        "            if mixer_subset is not None:\n",
        "                mixer_kwargs['mixer_subset'] = mixer_subset\n",
        "            hidden_states = self.mixer(hidden_states, **mixer_kwargs)\n",
        "            if mixer_subset is not None:\n",
        "                residual = residual[:, mixer_subset]\n",
        "            if not isinstance(self.mlp, nn.Identity):\n",
        "                dropped = self.drop_path2(self.dropout2(hidden_states))\n",
        "                residual = (dropped + residual) if residual is not None else dropped\n",
        "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
        "                if self.residual_in_fp32:\n",
        "                    residual = residual.to(torch.float32)\n",
        "\n",
        "                hidden_states = self.mlp(hidden_states)\n",
        "            return hidden_states, residual\n",
        "        else:\n",
        "            assert residual is None\n",
        "            mixer_out = self.mixer(\n",
        "                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})\n",
        "            )\n",
        "            if self.return_residual:  # mixer out is actually a pair here\n",
        "                mixer_out, hidden_states = mixer_out\n",
        "\n",
        "            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))\n",
        "                                        + hidden_states).to(dtype=self.norm1.weight.dtype))\n",
        "\n",
        "            if not isinstance(self.mlp, nn.Identity):\n",
        "                mlp_out = self.mlp(hidden_states)\n",
        "                if self.return_residual:  # mlp out is actually a pair here\n",
        "                    mlp_out, hidden_states = mlp_out\n",
        "\n",
        "                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))\n",
        "                                            + hidden_states).to(dtype=self.norm2.weight.dtype))\n",
        "\n",
        "            return hidden_states\n",
        "\n",
        "def create_mixer_cls(layer=None,\n",
        "                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,\n",
        "                     device=None, dtype=None):\n",
        "  '''\n",
        "\n",
        "  responsible for creating the class for the mixer layer used in the Block class.\n",
        "  It allows for flexibility in choosing between HyenaOperator and attention as the mixer layer,\n",
        "  depending on the configuration.\n",
        "\n",
        "  '''\n",
        "  factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "  if attn_layer_idx is not None and layer_idx in attn_layer_idx:\n",
        "      causal = True if attn_cfg is None else attn_cfg.pop('causal', True)\n",
        "\n",
        "      mha_cls = MHA\n",
        "\n",
        "      mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,\n",
        "                          **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)\n",
        "  else:\n",
        "      # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)\n",
        "\n",
        "      mixer_cls = partial(HyenaOperator, **layer)\n",
        "\n",
        "  return mixer_cls\n",
        "\n",
        "def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    inner_dim = d_inner if d_inner is not None else 4 * d_model\n",
        "\n",
        "    mlp_cls = partial(Mlp, hidden_features=inner_dim,\n",
        "                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)\n",
        "\n",
        "    return mlp_cls\n",
        "\n",
        "\n",
        "def create_block(d_model, d_inner=None,\n",
        "                 layer=None, attn_layer_idx=None,\n",
        "                 attn_cfg=None, layer_norm_epsilon=1e-5,\n",
        "                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,\n",
        "                 layer_idx=None,\n",
        "                 device=None, dtype=None):\n",
        "\n",
        "    '''\n",
        "    The create_block() function serves as a factory for creating blocks (instances of the Block class) that constitute the layers of the HyenaDNA model.\n",
        "    It handles the configuration and instantiation of these blocks, incorporating mixer layers (HyenaOperator or attention), MLP layers, normalization, and other components.\n",
        "\n",
        "    '''\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    mixer_cls = create_mixer_cls(layer=layer,\n",
        "                                 attn_layer_idx=attn_layer_idx,\n",
        "                                 attn_cfg=attn_cfg, layer_idx=layer_idx,\n",
        "                                 **factory_kwargs)\n",
        "    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,\n",
        "                             **factory_kwargs)\n",
        "    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)\n",
        "    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,\n",
        "                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,\n",
        "                  glu_act=False):\n",
        "  '''\n",
        "  is responsible for initializing the weights of the model's layers. Proper weight initialization is crucial for training deep learning models effectively,\n",
        "  as it can significantly impact convergence speed and overall performance.\n",
        "  This function applies specific initialization schemes to different types of layers within the HyenaDNA model.\n",
        "  '''\n",
        "\n",
        "  if isinstance(module, nn.Linear):\n",
        "      nn.init.normal_(module.weight, std=initializer_range)\n",
        "      if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "  elif isinstance(module, nn.Embedding):\n",
        "      nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "  if rescale_prenorm_residual:\n",
        "      # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "      #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "      #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
        "      #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "      #\n",
        "      # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "      for name, p in module.named_parameters():\n",
        "          if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "              # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "              nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
        "          # If using GLU activation for now, we scale the std by 2\n",
        "          elif name in [\"output_linear.0.weight\"]:\n",
        "              # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "              if not glu_act:\n",
        "                  nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
        "              else:\n",
        "                  out_features = p.shape[0]\n",
        "                  # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5\n",
        "                  # on average.\n",
        "                  nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJHCegJvtASc"
      },
      "outputs": [],
      "source": [
        "#@title Backbone model (stack of blocks)\n",
        "\n",
        "\"\"\"\n",
        "A backbone model consists of a stack of blocks. If you use attention, then\n",
        "positional embeddings are included. When using Hyena, then the pos emb\n",
        "revert to doing nothing.\n",
        "\"\"\"\n",
        "\n",
        "class GPT2Embeddings(nn.Module):\n",
        "  '''\n",
        "  The GPT2Embeddings class is responsible for creating the input embeddings for the HyenaDNA model,\n",
        "  similar to how embeddings are used in the GPT-2 language model.\n",
        "  It combines token embeddings, positional embeddings, and optional layer normalization to create\n",
        "  a rich representation of the input sequence.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,\n",
        "                word_embed_proj_dim=None, device=None, dtype=None):\n",
        "      \"\"\"\n",
        "          If max_position_embeddings <= 0, there's no position embeddings\n",
        "          If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n",
        "              the project up to embed_dim\n",
        "      \"\"\"\n",
        "      factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "      super().__init__()\n",
        "      if word_embed_proj_dim is None:\n",
        "          self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,\n",
        "                                              **factory_kwargs)\n",
        "          self.project_in = None\n",
        "      else:\n",
        "          self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,\n",
        "                                              padding_idx=padding_idx, **factory_kwargs)\n",
        "          self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,\n",
        "                                      **factory_kwargs)\n",
        "      self.max_position_embeddings = max_position_embeddings\n",
        "      if self.max_position_embeddings > 0:\n",
        "          self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,\n",
        "                                                  **factory_kwargs)\n",
        "\n",
        "  def forward(self, input_ids, position_ids=None):\n",
        "      \"\"\"\n",
        "          input_ids: (batch, seqlen)\n",
        "          position_ids: (batch, seqlen)\n",
        "      \"\"\"\n",
        "      batch_size, seqlen = input_ids.shape\n",
        "      embeddings = self.word_embeddings(input_ids)\n",
        "      if self.project_in is not None:\n",
        "          embeddings = self.project_in(embeddings)\n",
        "      if self.max_position_embeddings > 0:\n",
        "          if position_ids is None:\n",
        "              position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n",
        "          position_embeddings = self.position_embeddings(position_ids)\n",
        "          embeddings = embeddings + position_embeddings\n",
        "      return embeddings\n",
        "\n",
        "class LMBackbone(nn.Module):\n",
        "  '''\n",
        "  forms the core structure of the HyenaDNA language model.\n",
        "  It's responsible for processing the input embeddings through a series of blocks (instances of the Block class) and producing the final hidden states.\n",
        "  This backbone is then used for various downstream tasks, such as language modeling or text classification.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
        "                process_group=None, layer=None,\n",
        "                attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
        "                resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
        "                layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
        "                device=None, dtype=None, **kwargs) -> None:\n",
        "      factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "      super().__init__()\n",
        "      self.process_group = process_group\n",
        "      self.residual_in_fp32 = residual_in_fp32\n",
        "      # note max_position_embeddings is 0 for Hyena, and therefore isn't used\n",
        "      self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,\n",
        "                                            **factory_kwargs)\n",
        "\n",
        "      self.layers = nn.ModuleList([create_block(\n",
        "          d_model, d_inner=d_inner,\n",
        "          layer=layer, attn_layer_idx=attn_layer_idx,\n",
        "          attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,\n",
        "          resid_dropout1=embed_dropout if i == 0 else resid_dropout,\n",
        "          resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,\n",
        "          **factory_kwargs,\n",
        "      ) for i in range(n_layer)])\n",
        "\n",
        "      self.drop_f = nn.Dropout(resid_dropout)\n",
        "      self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)\n",
        "\n",
        "      self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                          **(initializer_cfg if initializer_cfg is not None else {})))\n",
        "\n",
        "  def forward(self, input_ids, position_ids=None):\n",
        "    '''\n",
        "    defines how the backbone of the HyenaDNA model processes input data. It takes the input embeddings,\n",
        "    passes them through a series of blocks (layers), and produces the final hidden states.\n",
        "    '''\n",
        "\n",
        "    hidden_states = self.embeddings(input_ids, position_ids=position_ids,)\n",
        "    residual = None\n",
        "\n",
        "    for layer in self.layers:\n",
        "        hidden_states, residual = layer(hidden_states, residual)\n",
        "\n",
        "    dropped = self.drop_f(hidden_states)\n",
        "    residual = (dropped + residual) if residual is not None else dropped\n",
        "    hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))\n",
        "\n",
        "    return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR0wFk-CawPB"
      },
      "outputs": [],
      "source": [
        "#@title Decoder head layer\n",
        "\n",
        "\"\"\"\n",
        "A simple decoder head (using MLP) to predict a sequence level classification.\n",
        "You have the option to average across all the tokens in a sequence or using the\n",
        "\"last\" token to classify.  At least, those 2 worked best for us, but we provide\n",
        "other \"modes\" as well.\n",
        "\n",
        "We only need this for classification.  Otherwise we'll use the hidden\n",
        "states of the backbone as embeddings.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SequenceDecoder(nn.Module):\n",
        "  def __init__(\n",
        "      self, d_model, d_output=None, l_output=None, use_lengths=False, mode=\"last\"\n",
        "  ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)\n",
        "\n",
        "      if l_output is None:\n",
        "          self.l_output = None\n",
        "          self.squeeze = False\n",
        "      elif l_output == 0:\n",
        "          # Equivalent to getting an output of length 1 and then squeezing\n",
        "          self.l_output = 1\n",
        "          self.squeeze = True\n",
        "      else:\n",
        "          assert l_output > 0\n",
        "          self.l_output = l_output\n",
        "          self.squeeze = False\n",
        "\n",
        "      self.use_lengths = use_lengths\n",
        "      self.mode = mode\n",
        "\n",
        "      if mode == 'ragged':\n",
        "          assert not use_lengths\n",
        "\n",
        "  def forward(self, x, state=None, lengths=None, l_output=None):\n",
        "      \"\"\"\n",
        "      x: (n_batch, l_seq, d_model)\n",
        "      Returns: (n_batch, l_output, d_output)\n",
        "\n",
        "      The forward method takes the input hidden states (hidden_states) and applies the following operations:\n",
        "        Normalization: Applies the ln layer to normalize the hidden states.\n",
        "        Projection: Applies the linear layer to project the normalized hidden states to the vocabulary size, producing logits.\n",
        "      It returns the logits, which represent the model's predictions for the next token in the sequence.\n",
        "      \"\"\"\n",
        "\n",
        "      if self.l_output is None:\n",
        "          if l_output is not None:\n",
        "              assert isinstance(l_output, int)  # Override by pass in\n",
        "          else:\n",
        "              # Grab entire output\n",
        "              l_output = x.size(-2)\n",
        "          squeeze = False\n",
        "      else:\n",
        "          l_output = self.l_output\n",
        "          squeeze = self.squeeze\n",
        "\n",
        "      if self.mode == \"last\":\n",
        "          restrict = lambda x: x[..., -l_output:, :]\n",
        "      elif self.mode == \"first\":\n",
        "          restrict = lambda x: x[..., :l_output, :]\n",
        "      elif self.mode == \"pool\":\n",
        "          restrict = lambda x: (\n",
        "              torch.cumsum(x, dim=-2)\n",
        "              / torch.arange(\n",
        "                  1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
        "              ).unsqueeze(-1)\n",
        "          )[..., -l_output:, :]\n",
        "\n",
        "          def restrict(x):\n",
        "              L = x.size(-2)\n",
        "              s = x.sum(dim=-2, keepdim=True)\n",
        "              if l_output > 1:\n",
        "                  c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n",
        "                  c = F.pad(c, (0, 0, 1, 0))\n",
        "                  s = s - c  # (B, l_output, D)\n",
        "                  s = s.flip(-2)\n",
        "              denom = torch.arange(\n",
        "                  L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n",
        "              )\n",
        "              s = s / denom\n",
        "              return s\n",
        "\n",
        "      elif self.mode == \"sum\":\n",
        "          restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n",
        "          # TODO use same restrict function as pool case\n",
        "      elif self.mode == \"pool\":\n",
        "          # might be buggy, will come back later\n",
        "          restrict = lambda x: (\n",
        "              torch.cumsum(x, dim=-2)\n",
        "              / torch.arange(\n",
        "                  1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
        "              ).unsqueeze(-1)\n",
        "          )[..., -l_output:, :]\n",
        "      elif self.mode == 'ragged':\n",
        "          assert lengths is not None, \"lengths must be provided for ragged mode\"\n",
        "          # remove any additional padding (beyond max length of any sequence in the batch)\n",
        "          restrict = lambda x: x[..., : max(lengths), :]\n",
        "      else:\n",
        "          raise NotImplementedError(\n",
        "              \"Mode must be ['last' | 'first' | 'pool' | 'sum']\"\n",
        "          )\n",
        "\n",
        "      # Restrict to actual length of sequence\n",
        "      if self.use_lengths:\n",
        "          assert lengths is not None\n",
        "          x = torch.stack(\n",
        "              [\n",
        "                  restrict(out[..., :length, :])\n",
        "                  for out, length in zip(torch.unbind(x, dim=0), lengths)\n",
        "              ],\n",
        "              dim=0,\n",
        "          )\n",
        "      else:\n",
        "          x = restrict(x)\n",
        "\n",
        "      if squeeze:\n",
        "          assert x.size(-2) == 1\n",
        "          x = x.squeeze(-2)\n",
        "\n",
        "      x = self.output_transform(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "  def step(self, x, state=None):\n",
        "      # Ignore all length logic\n",
        "      return self.output_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM3g4ImZtKNu"
      },
      "outputs": [],
      "source": [
        "#@title Model (backbone + head)\n",
        "\n",
        "\"\"\"\n",
        "Putting it all together, the model consists of a backbone model\n",
        "and a decoder head (you can turn off head for embeddings only too).\n",
        "\n",
        "Here we use a simple head to do multi-classification, but\n",
        "can also swap the head to do next token prediction too.  We defer to the main\n",
        "HyenaDNA for that code, since pretraining with next token prediction isn't quite\n",
        "feasible on colab.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class HyenaDNAModel(nn.Module):\n",
        "  '''\n",
        "  Smth I did not know before: in nn.Module base class, the class variable state_dict\n",
        "  s a dictionary that maps each layer's name to its trainable parameters (weights and biases).\n",
        "  It's essential for saving and loading models.\n",
        "\n",
        "  '''\n",
        "  def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
        "                layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
        "                resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
        "                layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
        "                pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
        "                device=None, dtype=None, **kwargs) -> None:\n",
        "      factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "      super().__init__()\n",
        "      if vocab_size % pad_vocab_size_multiple != 0:\n",
        "          vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "\n",
        "      self.use_head = use_head\n",
        "\n",
        "      # check if layer (config) has d_model (HF code differs from main Safari code)\n",
        "      if 'd_model' not in layer:\n",
        "          layer['d_model'] = d_model\n",
        "\n",
        "      self.backbone = LMBackbone(\n",
        "          d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
        "          layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
        "          max_position_embeddings=max_position_embeddings,\n",
        "          resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
        "          layer_norm_epsilon=layer_norm_epsilon,\n",
        "          initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
        "          **factory_kwargs, **kwargs\n",
        "      )\n",
        "\n",
        "      # we only need a head if doing classification, otherwise we'll use the\n",
        "      # hidden states as embeddings\n",
        "      if self.use_head:\n",
        "          self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
        "\n",
        "      # Initialize weights and apply final processing\n",
        "      self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                          **(initializer_cfg if initializer_cfg is not None else {})))\n",
        "\n",
        "      # if self.use_head:\n",
        "      #     self.tie_weights()\n",
        "\n",
        "  # def tie_weights(self):\n",
        "  #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
        "\n",
        "  def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
        "      hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
        "\n",
        "      if self.use_head:\n",
        "          return self.head(hidden_states)\n",
        "      else:\n",
        "          return hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 1 (1 point): Enable the option of training from scratch\n",
        "Modify the `HyenaDNAPreTrainedModel` class method `from_pretrained` below to enable training from scratch. Hint: this takes only very few small changes. So do not overthink it. You can introduce a new boolean argument 'from_scratch' to instruct the method to load the pre-trained weights (from_scratch=False) or not (from_scratch=True)."
      ],
      "metadata": {
        "id": "e25LdfpoEXbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gGiI05HX-AU"
      },
      "outputs": [],
      "source": [
        "#@title Huggingface Pretrained Wrapper\n",
        "# for Huggingface integration, we use a wrapper class around the model\n",
        "# to load weights\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import transformers\n",
        "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
        "import re\n",
        "\n",
        "def inject_substring(orig_str):\n",
        "    \"\"\"Hack to handle matching keys between models trained with and without\n",
        "    gradient checkpointing.\"\"\"\n",
        "\n",
        "    # modify for mixer keys\n",
        "    pattern = r\"\\.mixer\"\n",
        "    injection = \".mixer.layer\"\n",
        "\n",
        "    modified_string = re.sub(pattern, injection, orig_str)\n",
        "\n",
        "    # modify for mlp keys\n",
        "    pattern = r\"\\.mlp\"\n",
        "    injection = \".mlp.layer\"\n",
        "\n",
        "    modified_string = re.sub(pattern, injection, modified_string)\n",
        "\n",
        "    return modified_string\n",
        "\n",
        "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
        "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
        "\n",
        "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
        "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
        "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
        "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
        "    that if used.\n",
        "\n",
        "    return:\n",
        "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
        "\n",
        "    # loop thru state dict of scratch\n",
        "    # find the corresponding weights in the loaded model, and set it\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # need to do some state dict \"surgery\"\n",
        "    for key, value in scratch_dict.items():\n",
        "        if 'backbone' in key:\n",
        "            # the state dicts differ by one prefix, '.model', so we add that\n",
        "            key_loaded = 'model.' + key\n",
        "            # breakpoint()\n",
        "            # need to add an extra \".layer\" in key\n",
        "            if checkpointing:\n",
        "                key_loaded = inject_substring(key_loaded)\n",
        "            try:\n",
        "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
        "            except:\n",
        "                raise Exception('key mismatch in the state dicts!')\n",
        "\n",
        "    # scratch_dict has been updated\n",
        "    return scratch_dict\n",
        "\n",
        "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "    base_model_prefix = \"hyenadna\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        return self.model(input_ids, **kwargs)\n",
        "\n",
        "    # A3 task 1: TO BE MODIFIED\n",
        "    @classmethod\n",
        "    def from_pretrained(cls='',\n",
        "                        path='',\n",
        "                        model_name='',\n",
        "                        download=False,\n",
        "                        config=None,\n",
        "                        device='cpu',\n",
        "                        use_head=False,\n",
        "                        n_classes=2,\n",
        "                        training_from_scratch=False,\n",
        "                      ):\n",
        "      # first check if it is a local path\n",
        "      pretrained_model_name_or_path = os.path.join(path, model_name)\n",
        "      if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
        "          if config is None:\n",
        "              config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
        "      else:\n",
        "          hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
        "\n",
        "          subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
        "          command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
        "          subprocess.run(command, shell=True)\n",
        "\n",
        "          if config is None:\n",
        "              config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
        "\n",
        "      scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
        "      # Uodate the pre-trained weights (unnecessary for training from scratch)\n",
        "      if not training_from_scratch:\n",
        "        loaded_ckpt = torch.load(\n",
        "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
        "            map_location=torch.device(device)\n",
        "        )\n",
        "\n",
        "        # need to load weights slightly different if using gradient checkpointing\n",
        "        if config.get(\"checkpoint_mixer\", False):\n",
        "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
        "        else:\n",
        "            checkpointing = False\n",
        "\n",
        "        # grab state dict from both and load weights\n",
        "\n",
        "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
        "\n",
        "        # scratch model has now been updated\n",
        "        scratch_model.load_state_dict(state_dict)\n",
        "        print(\"Loaded pretrained weights ok!\")\n",
        "      else:\n",
        "        print(\"Training from Scratch!\")\n",
        "      return scratch_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsRya6hEryfg"
      },
      "source": [
        "# Data pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsL1Afn5flo0"
      },
      "outputs": [],
      "source": [
        "#@title Tokenizer\n",
        "\n",
        "\"\"\"\n",
        "Just a simple character level tokenizer.\n",
        "\n",
        "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
        "\n",
        "CharacterTokenzier for Hugging Face Transformers.\n",
        "This is heavily inspired from CanineTokenizer in transformers package.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Union\n",
        "\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class CharacterTokenizer(PreTrainedTokenizer):\n",
        "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
        "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
        "        Args:\n",
        "            characters (Sequence[str]): List of desired characters. Any character which\n",
        "                is not included in this list will be replaced by a special token called\n",
        "                [UNK] with id=6. Following are list of all of the special tokens with\n",
        "                their corresponding ids:\n",
        "                    \"[CLS]\": 0\n",
        "                    \"[SEP]\": 1\n",
        "                    \"[BOS]\": 2\n",
        "                    \"[MASK]\": 3\n",
        "                    \"[PAD]\": 4\n",
        "                    \"[RESERVED]\": 5\n",
        "                    \"[UNK]\": 6\n",
        "                an id (starting at 7) will be assigned to each character.\n",
        "            model_max_length (int): Model maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.characters = characters\n",
        "        self.model_max_length = model_max_length\n",
        "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
        "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
        "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
        "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
        "\n",
        "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=sep_token,\n",
        "            sep_token=sep_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            unk_token=unk_token,\n",
        "            add_prefix_space=False,\n",
        "            model_max_length=model_max_length,\n",
        "            padding_side=padding_side,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._vocab_str_to_int = {\n",
        "            \"[CLS]\": 0,\n",
        "            \"[SEP]\": 1,\n",
        "            \"[BOS]\": 2,\n",
        "            \"[MASK]\": 3,\n",
        "            \"[PAD]\": 4,\n",
        "            \"[RESERVED]\": 5,\n",
        "            \"[UNK]\": 6,\n",
        "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
        "        }\n",
        "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self._vocab_str_to_int)\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self._vocab_int_to_str[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        result = cls + token_ids_0 + sep\n",
        "        if token_ids_1 is not None:\n",
        "            result += token_ids_1 + sep\n",
        "        return result\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None,\n",
        "        already_has_special_tokens: bool = False,\n",
        "    ) -> List[int]:\n",
        "        if already_has_special_tokens:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0,\n",
        "                token_ids_1=token_ids_1,\n",
        "                already_has_special_tokens=True,\n",
        "            )\n",
        "\n",
        "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
        "        if token_ids_1 is not None:\n",
        "            result += ([0] * len(token_ids_1)) + [1]\n",
        "        return result\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "\n",
        "        result = len(cls + token_ids_0 + sep) * [0]\n",
        "        if token_ids_1 is not None:\n",
        "            result += len(token_ids_1 + sep) * [1]\n",
        "        return result\n",
        "\n",
        "    def get_config(self) -> Dict:\n",
        "        return {\n",
        "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
        "            \"model_max_length\": self.model_max_length,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
        "        cfg = {}\n",
        "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
        "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
        "        return cls(**cfg)\n",
        "\n",
        "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        cfg = self.get_config()\n",
        "        with open(cfg_file, \"w\") as f:\n",
        "            json.dump(cfg, f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        with open(cfg_file) as f:\n",
        "            cfg = json.load(f)\n",
        "        return cls.from_config(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2_Uw320e6mk"
      },
      "outputs": [],
      "source": [
        "#@title GenomicBenchmark dataset\n",
        "\n",
        "\"\"\"\n",
        "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
        "There are 8 datasets to choose from.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from random import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from genomic_benchmarks.loc2seq import download_dataset\n",
        "from genomic_benchmarks.data_check import is_downloaded\n",
        "\n",
        "\n",
        "# helper functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def coin_flip():\n",
        "    return random() > 0.5\n",
        "\n",
        "\n",
        "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
        "# augmentation\n",
        "def string_reverse_complement(seq):\n",
        "    rev_comp = ''\n",
        "    for base in seq[::-1]:\n",
        "        if base in string_complement_map:\n",
        "            rev_comp += string_complement_map[base]\n",
        "        # if bp not complement map, use the same bp\n",
        "        else:\n",
        "            rev_comp += base\n",
        "    return rev_comp\n",
        "\n",
        "\n",
        "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    '''\n",
        "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
        "    Returns a generator that retrieves the sequence.\n",
        "\n",
        "    Genomic Benchmarks Dataset, from:\n",
        "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split,\n",
        "        max_length,\n",
        "        dataset_name='human_enhancers_cohn',\n",
        "        d_output=2, # default binary classification\n",
        "        dest_path=\"/content\", # default for colab\n",
        "        tokenizer=None,\n",
        "        tokenizer_name=None,\n",
        "        use_padding=None,\n",
        "        add_eos=False,\n",
        "        rc_aug=False,\n",
        "        return_augs=False,\n",
        "    ):\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.use_padding = use_padding\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.return_augs = return_augs\n",
        "        self.add_eos = add_eos\n",
        "        self.d_output = d_output  # needed for decoder to grab\n",
        "        self.rc_aug = rc_aug\n",
        "\n",
        "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
        "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
        "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
        "        else:\n",
        "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
        "\n",
        "        # use Path object\n",
        "        base_path = Path(dest_path) / dataset_name / split\n",
        "\n",
        "        self.all_paths = []\n",
        "        self.all_labels = []\n",
        "        label_mapper = {}\n",
        "\n",
        "        for i, x in enumerate(base_path.iterdir()):\n",
        "            label_mapper[x.stem] = i\n",
        "\n",
        "        for label_type in label_mapper.keys():\n",
        "            for x in (base_path / label_type).iterdir():\n",
        "                self.all_paths.append(x)\n",
        "                self.all_labels.append(label_mapper[label_type])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt_path = self.all_paths[idx]\n",
        "        with open(txt_path, \"r\") as f:\n",
        "            content = f.read()\n",
        "        x = content\n",
        "        y = self.all_labels[idx]\n",
        "\n",
        "        # apply rc_aug here if using\n",
        "        if self.rc_aug and coin_flip():\n",
        "            x = string_reverse_complement(x)\n",
        "\n",
        "        seq = self.tokenizer(x,\n",
        "            add_special_tokens=False,\n",
        "            padding=\"max_length\" if self.use_padding else None,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )  # add cls and eos token (+2)\n",
        "        seq = seq[\"input_ids\"]  # get input_ids\n",
        "\n",
        "        # need to handle eos here\n",
        "        if self.add_eos:\n",
        "            # append list seems to be faster than append tensor\n",
        "            seq.append(self.tokenizer.sep_token_id)\n",
        "\n",
        "        # convert to tensor\n",
        "        seq = torch.LongTensor(seq)\n",
        "\n",
        "        # need to wrap in list\n",
        "        target = torch.LongTensor([y])\n",
        "\n",
        "        return seq, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 task 2 (1 point): record test accuracy\n",
        "Add to the `test` function below to record and return the test accuracy."
      ],
      "metadata": {
        "id": "3N5bxBqHG1dt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vMJu0wQWUBN"
      },
      "source": [
        "# Training! (and fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMNRAa4gKJOK"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\"\"\"\n",
        "We provide simple training code for the GenomicBenchmark datasets.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, loss_fn, log_interval=10):\n",
        "    \"\"\"Training loop.\"\"\"\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "# A3 task 2: TO BE MODIFIED\n",
        "def test(model, device, test_loader, loss_fn):\n",
        "    \"\"\"Test loop.\"\"\"\n",
        "    model.eval() # set the model to eval mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    batch_size = test_loader.batch_size\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_fn(output, target.squeeze()).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # target.view_as(pred) reshapes the target tensor to have the same shape as pred tensor so the two can be compared element-wise\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        test_acc))\n",
        "    return test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 3 (1 point) Modify `run_train` function\n",
        "Modify `run_train` function to train on the dummy_mouse_enhancers_ensembl sequences. The `run_train` should be able to incorporate the modifications you made in task 1 and 2:\n",
        "1. Take new Boolean argument `from_scratch` to instruct the HyenaDNA whether to load the pre-trained weights. For the fine-tuning task, we use only the pre-trained `hyenadna-tiny-1k-seqlen` model. Other pre-trained models are too large to fine-tune with free colab account.\n",
        "2. Return a list of test accuracies for the first 10 epochs of either training from scratch or fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "fPagLGxrHdYW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6sguA32WZE3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import transformers\n",
        "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
        "\n",
        "# A3 task 3: TO BE MODIFIED\n",
        "def run_train(from_scratch=False):\n",
        "\n",
        "    '''\n",
        "    Main entry point for training.  Select the dataset name and metadata, as\n",
        "    well as model and training args, and you're off to the genomic races!\n",
        "\n",
        "    ### GenomicBenchmarks Metadata\n",
        "    # there are 8 datasets in this suite, choose 1 at a time, with their corresponding settings\n",
        "    # name                                num_seqs        num_classes     median len    std\n",
        "    # dummy_mouse_enhancers_ensembl       1210            2               2381          984.4\n",
        "    # demo_coding_vs_intergenomic_seqs    100_000         2               200           0\n",
        "    # demo_human_or_worm                  100_000         2               200           0\n",
        "    # human_enhancers_cohn                27791           2               500           0\n",
        "    # human_enhancers_ensembl             154842          2               269           122.6\n",
        "    # human_ensembl_regulatory            289061          3               401           184.3\n",
        "    # human_nontata_promoters             36131           2               251           0\n",
        "    # human_ocr_ensembl                   174756          2               315           108.1\n",
        "\n",
        "    '''\n",
        "    # experiment settings:\n",
        "    num_epochs = 10  # ~100 seems fine\n",
        "    max_length = 500  # max len of sequence of dataset (of what you want)\n",
        "    use_padding = True\n",
        "    dataset_name = 'dummy_mouse_enhancers_ensembl' # 'dummy_mouse_enhancers_ensembl' 'human_enhancers_cohn'\n",
        "    batch_size = 256\n",
        "    learning_rate = 6e-4  # good default for Hyena\n",
        "    rc_aug = True  # reverse complement augmentation\n",
        "    add_eos = False  # add end of sentence token\n",
        "    weight_decay = 0.1\n",
        "\n",
        "    # for fine-tuning, only the 'tiny' model can fit on colab\n",
        "    pretrained_model_name = 'hyenadna-tiny-1k-seqlen'  # use None if training from scratch\n",
        "\n",
        "    # we need these for the decoder head, if using\n",
        "    use_head = True\n",
        "    n_classes = 2\n",
        "\n",
        "    # you can override with your own backbone config here if you want,\n",
        "    # otherwise we'll load the HF one by default\n",
        "    backbone_cfg = None\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # instantiate the model (pretrained here)\n",
        "    args = {\n",
        "        'path': './checkpoints',\n",
        "        'model_name': pretrained_model_name,\n",
        "        'download': True,\n",
        "        'config': backbone_cfg,\n",
        "        'device': device,\n",
        "        'use_head': use_head,\n",
        "        'n_classes': n_classes,\n",
        "        'training_from_scratch': from_scratch\n",
        "    }\n",
        "    model = HyenaDNAPreTrainedModel.from_pretrained(**args)\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = CharacterTokenizer(\n",
        "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
        "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
        "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
        "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
        "    )\n",
        "\n",
        "    # create datasets\n",
        "    ds_train = GenomicBenchmarkDataset(\n",
        "        max_length = max_length,\n",
        "        use_padding = use_padding,\n",
        "        split = 'train',\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_name=dataset_name,\n",
        "        rc_aug=rc_aug,\n",
        "        add_eos=add_eos,\n",
        "    )\n",
        "\n",
        "    ds_test = GenomicBenchmarkDataset(\n",
        "        max_length = max_length,\n",
        "        use_padding = use_padding,\n",
        "        split = 'test',\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_name=dataset_name,\n",
        "        rc_aug=rc_aug,\n",
        "        add_eos=add_eos,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
        "    # loss function\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    # set_trace()\n",
        "    model.to(device)\n",
        "    test_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train(model, device, train_loader, optimizer, epoch, loss_fn)\n",
        "        test_accuracies.append(test(model, device, test_loader, loss_fn))\n",
        "        optimizer.step()\n",
        "\n",
        "    return model, test_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call `run_train` function twice once for fine-tuning on pre-trained hyenadna-tiny-1k-seqlen model and once for training from scratch on the same model. Your function should return both the fine-tuned or trained model and the test accraucies for the subsequent analyses."
      ],
      "metadata": {
        "id": "5VJXUu8HLx1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhYYO_NP-Au4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8101fbd-1583-4b30-fde0-1744cf883ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-69bc72104001>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_ckpt = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights ok!\n",
            "already downloaded train-dummy_mouse_enhancers_ensembl\n",
            "already downloaded test-dummy_mouse_enhancers_ensembl\n",
            "Train Epoch: 0 [0/968 (0%)]\tLoss: 0.687506\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 166/242 (68.60%)\n",
            "\n",
            "Train Epoch: 1 [0/968 (0%)]\tLoss: 0.632533\n",
            "\n",
            "Test set: Average loss: 0.0025, Accuracy: 163/242 (67.36%)\n",
            "\n",
            "Train Epoch: 2 [0/968 (0%)]\tLoss: 0.605300\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 167/242 (69.01%)\n",
            "\n",
            "Train Epoch: 3 [0/968 (0%)]\tLoss: 0.598458\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 170/242 (70.25%)\n",
            "\n",
            "Train Epoch: 4 [0/968 (0%)]\tLoss: 0.536906\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 167/242 (69.01%)\n",
            "\n",
            "Train Epoch: 5 [0/968 (0%)]\tLoss: 0.525139\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 171/242 (70.66%)\n",
            "\n",
            "Train Epoch: 6 [0/968 (0%)]\tLoss: 0.540488\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 169/242 (69.83%)\n",
            "\n",
            "Train Epoch: 7 [0/968 (0%)]\tLoss: 0.588601\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 181/242 (74.79%)\n",
            "\n",
            "Train Epoch: 8 [0/968 (0%)]\tLoss: 0.525465\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 184/242 (76.03%)\n",
            "\n",
            "Train Epoch: 9 [0/968 (0%)]\tLoss: 0.472606\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 175/242 (72.31%)\n",
            "\n",
            "Using device: cuda\n",
            "Training from Scratch!\n",
            "already downloaded train-dummy_mouse_enhancers_ensembl\n",
            "already downloaded test-dummy_mouse_enhancers_ensembl\n",
            "Train Epoch: 0 [0/968 (0%)]\tLoss: 0.670583\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 144/242 (59.50%)\n",
            "\n",
            "Train Epoch: 1 [0/968 (0%)]\tLoss: 0.662566\n",
            "\n",
            "Test set: Average loss: 0.0026, Accuracy: 153/242 (63.22%)\n",
            "\n",
            "Train Epoch: 2 [0/968 (0%)]\tLoss: 0.615206\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 165/242 (68.18%)\n",
            "\n",
            "Train Epoch: 3 [0/968 (0%)]\tLoss: 0.640386\n",
            "\n",
            "Test set: Average loss: 0.0025, Accuracy: 158/242 (65.29%)\n",
            "\n",
            "Train Epoch: 4 [0/968 (0%)]\tLoss: 0.574754\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 166/242 (68.60%)\n",
            "\n",
            "Train Epoch: 5 [0/968 (0%)]\tLoss: 0.582540\n",
            "\n",
            "Test set: Average loss: 0.0025, Accuracy: 159/242 (65.70%)\n",
            "\n",
            "Train Epoch: 6 [0/968 (0%)]\tLoss: 0.589239\n",
            "\n",
            "Test set: Average loss: 0.0025, Accuracy: 159/242 (65.70%)\n",
            "\n",
            "Train Epoch: 7 [0/968 (0%)]\tLoss: 0.613121\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 163/242 (67.36%)\n",
            "\n",
            "Train Epoch: 8 [0/968 (0%)]\tLoss: 0.606525\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 167/242 (69.01%)\n",
            "\n",
            "Train Epoch: 9 [0/968 (0%)]\tLoss: 0.606446\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 154/242 (63.64%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# launch it!\n",
        "finetuned, test_acc_list_ft = run_train()  # fine-tune pre-trained model\n",
        "fromscratch, test_acc_list_sc = run_train(from_scratch=True)  # train from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 4 (1 point): Comparing fine-tuning vs training from scratch\n",
        "Plot the test accuracies for both the fine-tuned and trained-from-scratch models. Your plot might not look the same as the one below but in general fine-tuning should start with test accruacy around 65% and reach to ardoun 75% whereas the training from scratch should start the test accuracy around 50% (i.e., no different from random guess) and then reaches to around 70%-75%.\n",
        "\n",
        "If you train for more epochs, eventually the training from scratch will reach similar or slightly higher test accuracy as the fine-tuning because of the simple task here."
      ],
      "metadata": {
        "id": "NS_l8tpaM5cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE THE PLOT FUNCTION HERE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(test_acc_list_ft) + 1)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, test_acc_list_ft, marker='o', linestyle='-', color='b', label='Fine-Tuning')\n",
        "plt.plot(epochs, test_acc_list_sc, marker='s', linestyle='--', color='r', label='From Scratch')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Test Accuracy Over Epochs for Two Configurations')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OV1JFTsQHk8M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "b16eafb4-a6a0-4180-9a34-3836ff1c6a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC1x0lEQVR4nOzdd1hTZxsG8DtslOEABVwouHfdA0cdOOuete5R97ZW695aR9VarXW1jlq3tS607l23dSFuxFkRFys53x/PRyAyBEw4Ae7fdeUiOTlJniQnh/Oc932fV6MoigIiIiIiIiL6JBZqB0BERERERJQWMLkiIiIiIiIyAiZXRERERERERsDkioiIiIiIyAiYXBERERERERkBkysiIiIiIiIjYHJFRERERERkBEyuiIiIiIiIjIDJFRERERERkREwuSIiIoqDp6cnGjVqZNLX8Pf3R926deHs7AyNRoOtW7ea9PUott27d6NUqVKws7ODRqNBcHAwOnfuDE9PT7VD+yTjx4+HRqNROwyidIfJFVE6oNFoEnU5ePDgJ7/Wu3fvMH78+GQ9186dO6HRaODh4QGdTvfJsaQ3L168wPDhw1GwYEHY2dkhS5Ys8PX1xY4dO9QOLU6enp7xbov16tVTO7wU0alTJ1y+fBlTpkzBb7/9hrJly5rkdWrUqJGofcD48eNN8vofc/DgQTRv3hxubm6wsbFBtmzZ0LhxY2zevNmkr/vixQu0bt0a9vb2+PHHH/Hbb78hY8aMJn1NY/qU/S0RmYaV2gEQken99ttvBrd//fVX+Pn5xVpeuHDhT36td+/eYcKECQDkgC4p1qxZA09PT9y9exd///03ateu/cnxpBc3btxArVq18OzZM3Tp0gVly5ZFcHAw1qxZg8aNG2PYsGGYNWuW2mHGUqpUKQwdOjTWcg8PDxWiSVnv37/HiRMnMHr0aPTr18+krzV69Gh0795df/vMmTOYP38+Ro0aZfC7L1GihEnjiMu4ceMwceJE5M+fH7169UKePHnw4sUL7Ny5Ey1atMCaNWvQvn17k7z2mTNn8Pr1a0yaNMlgf7N06dJUcYInof3td999h5EjR6oQFVH6xuSKKB3o0KGDwe2TJ0/Cz88v1nI1vX37Ftu2bcO0adOwYsUKrFmzxmyTq7dv35rV2e2IiAi0bNkSL1++xOHDh1GhQgX9fYMHD8aXX36J77//HmXLlkWbNm1SLK7IyEjodDrY2NjEu06OHDnMajtMSc+ePQMAZMqUyWjPGd+2WadOHYPbdnZ2mD9/PurUqZPkkyDGtHHjRkycOBEtW7bE2rVrYW1trb9v+PDh2LNnDyIiIkz2+k+fPgUQ+zuIGUdKSsxvJrGsrKxgZcXDPKKUxm6BRAQA0Ol0mDdvHooWLQo7Oztkz54dvXr1wsuXLw3W++eff+Dr6wsXFxfY29sjb9686Nq1KwDg7t27cHV1BQBMmDAhSV2NtmzZgvfv36NVq1Zo27YtNm/ejNDQ0FjrhYaGYvz48ShQoADs7Ozg7u6O5s2bIyAgwOC9/PDDDyhevDjs7Ozg6uqKevXq4Z9//tHHqdFosHLlyljP/2G8UeMWrl69ivbt2yNz5syoWrUqAODSpUvo3Lkz8uXLBzs7O7i5uaFr16548eJFrOcNDAxEt27d4OHhAVtbW+TNmxe9e/dGeHg4bt++DY1Gg7lz58Z63PHjx6HRaLBu3bp4P7tNmzbhypUrGDlypEFiBQCWlpZYsmQJMmXKpH9fT548gZWVlf6Md0w3btyARqPBwoUL9cuCg4MxaNAg5MqVC7a2tvD29saMGTMMzuxHfabff/895s2bBy8vL9ja2uLq1avxxp1YnTt3hoODA27fvg1fX19kzJgRHh4emDhxIhRFMVj37du3GDp0qD7WggUL4vvvv4+1HgCsXr0a5cuXR4YMGZA5c2ZUq1YNe/fujbXe0aNHUb58edjZ2SFfvnz49ddfDe6PiIjAhAkTkD9/ftjZ2SFr1qyoWrUq/Pz84n1P48ePR548eQBIEqHRaAzG+Jw/fx7169eHk5MTHBwcUKtWLZw8edLgOVauXAmNRoNDhw6hT58+yJYtG3LmzPnRzzMu8+fPh6WlJYKDg/XLZs+eDY1GgyFDhuiXabVaODo64ptvvtEvS8pn/qExY8YgS5YsWL58eZwJja+vr8G4t6dPn6Jbt27Inj077OzsULJkSaxatcrgMTG3xZ9//lm/LZYrVw5nzpzRr1ejRg106tQJAFCuXDloNBp07twZAOIcc/XixQt89dVXcHJyQqZMmdCpUydcvHgx1r6kRo0acSasHz5nQr+Z8PBwjB07FmXKlIGzszMyZswIHx8fHDhwwODxCe1v4xpzFRkZiUmTJulfy9PTE6NGjUJYWJjBelHjDU2x7ROldTylQUQAgF69emHlypXo0qULBgwYgDt37mDhwoU4f/48jh07Bmtrazx9+hR169aFq6srRo4ciUyZMuHu3bv6cRGurq746aef0Lt3bzRr1gzNmzcHkLiuRmvWrEHNmjXh5uaGtm3bYuTIkfjzzz/RqlUr/TparRaNGjXC/v370bZtWwwcOBCvX7+Gn58frly5Ai8vLwBAt27dsHLlStSvXx/du3dHZGQkjhw5gpMnTyZ7TEurVq2QP39+TJ06VX/Q6Ofnh9u3b6NLly5wc3PDv//+i59//hn//vsvTp48qT+wefToEcqXL4/g4GD07NkThQoVQmBgIDZu3Ih3794hX758qFKlCtasWYPBgwfH+lwcHR3RpEmTeGP7888/AQAdO3aM835nZ2c0adIEq1atwq1bt+Dt7Y3q1avjjz/+wLhx4wzWXb9+PSwtLfWf+7t371C9enUEBgaiV69eyJ07N44fP45vv/0WQUFBmDdvnsHjV6xYgdDQUPTs2RO2trbIkiVLgp9rREQEnj9/Hmt5xowZYW9vr7+t1WpRr149VKxYETNnzsTu3bsxbtw4REZGYuLEiQAARVHwxRdf4MCBA+jWrRtKlSqFPXv2YPjw4QgMDDRIXidMmIDx48ejcuXKmDhxImxsbHDq1Cn8/fffqFu3rn69W7duoWXLlujWrRs6deqE5cuXo3PnzihTpgyKFi0KQA5ip02bhu7du6N8+fIICQnBP//8g3PnzsVqMYrSvHlzZMqUCYMHD0a7du3QoEEDODg4AAD+/fdf+Pj4wMnJCSNGjIC1tTWWLFmCGjVq4NChQ7ES6D59+sDV1RVjx47F27dvE/y84+Pj4wOdToejR4/qk5kjR47AwsICR44c0a93/vx5vHnzBtWqVUvyZ/4hf39/XL9+HV27doWjo+NHY3z//j1q1KiBW7duoV+/fsibNy82bNiAzp07Izg4GAMHDjRYf+3atXj9+jV69eoFjUaDmTNnonnz5rh9+zasra0xevRoFCxYED///DMmTpyIvHnz6vchH9LpdGjcuDFOnz6N3r17o1ChQti2bZs+OfsUcf1mQkJC8Msvv6Bdu3bo0aMHXr9+jWXLlsHX1xenT59GqVKlkrW/7d69O1atWoWWLVti6NChOHXqFKZNm4Zr165hy5YtBuuaatsnSvMUIkp3+vbtq8T8+R85ckQBoKxZs8Zgvd27dxss37JliwJAOXPmTLzP/ezZMwWAMm7cuETH8+TJE8XKykpZunSpflnlypWVJk2aGKy3fPlyBYAyZ86cWM+h0+kURVGUv//+WwGgDBgwIN517ty5owBQVqxYEWudD2MfN26cAkBp165drHXfvXsXa9m6desUAMrhw4f1yzp27KhYWFjE+blFxbRkyRIFgHLt2jX9feHh4YqLi4vSqVOnWI+LqVSpUoqzs3OC68yZM0cBoGzfvt3g9S5fvmywXpEiRZTPP/9cf3vSpElKxowZlZs3bxqsN3LkSMXS0lK5f/++oijRn6mTk5Py9OnTBGOJkidPHgVAnJdp06bp1+vUqZMCQOnfv79+mU6nUxo2bKjY2Ngoz549UxRFUbZu3aoAUCZPnmzwOi1btlQ0Go1y69YtRVEUxd/fX7GwsFCaNWumaLVag3Wjvo+Y8cX8Lp8+farY2toqQ4cO1S8rWbKk0rBhw0S955iiPrNZs2YZLG/atKliY2OjBAQE6Jc9evRIcXR0VKpVq6ZftmLFCgWAUrVqVSUyMjJJr71hwwYFgHLgwAFFURRFq9UqTk5OyogRIxRFkc8ha9asSqtWrRRLS0vl9evXiqLIdmRhYaG8fPlSUZTEf+Zx2bZtmwJAmTt3bqJinjdvngJAWb16tX5ZeHi4UqlSJcXBwUEJCQlRFCX6c82aNavy33//xXq9P//8U78s6jP88LfZqVMnJU+ePPrbmzZtUgAo8+bN0y/TarXK559/HmtfUr16daV69eqx4v/wORP6zURGRiphYWEGy16+fKlkz55d6dq1q35ZQvvbqH1XlAsXLigAlO7duxusN2zYMAWA8vfff+uXmXrbJ0rL2C2QiLBhwwY4OzujTp06eP78uf5SpkwZODg46LuiRI1L2LFjh1HHQfz++++wsLBAixYt9MvatWuHXbt2GXRL3LRpE1xcXNC/f/9YzxHVSrRp0yZoNJpYLTIx10mOr7/+OtaymC0roaGheP78OSpWrAgAOHfuHAA5471161Y0btw4zlazqJhat24NOzs7rFmzRn/fnj178Pz584+OSXr9+vVHz/xH3R8SEgJAWk6srKywfv16/TpXrlzB1atXDcZlbdiwAT4+PsicObPBtlG7dm1otVocPnzY4HVatGih76qUGBUqVICfn1+sS7t27WKtG7Pog0ajQb9+/RAeHo59+/YBkGqTlpaWGDBggMHjhg4dCkVRsGvXLgDA1q1bodPpMHbsWFhYGP4b/HAbKVKkCHx8fPS3XV1dUbBgQdy+fVu/LFOmTPj333/h7++f6PcdH61Wi71796Jp06bIly+ffrm7uzvat2+Po0eP6r/DKD169IClpeUnva6FhQUqV66s/z6vXbuGFy9eYOTIkVAUBSdOnAAgrVnFihXT7wsS+5nHJep9JKbVKuq13NzcDLYNa2trDBgwAG/evMGhQ4cM1m/Tpg0yZ86svx31Pcb87hJr9+7dsLa2Ro8ePfTLLCws0Ldv3yQ/14fi+s1YWlrqx13pdDr8999/iIyMRNmyZfX7lqTauXMnABh08wSgLyjz119/GSxP6W2fKK1gckVE8Pf3x6tXr5AtWza4uroaXN68eaMf9F29enW0aNECEyZMgIuLC5o0aYIVK1bE6q+fVFFjX168eIFbt27h1q1bKF26NMLDw7Fhwwb9egEBAShYsGCCg7QDAgLg4eHx0e5oSZU3b95Yy/777z8MHDgQ2bNnh729PVxdXfXrvXr1CoAULQgJCUGxYsUSfP5MmTKhcePGWLt2rX7ZmjVrkCNHDnz++ecJPtbR0RGvX79OcJ2o+6MOZF1cXFCrVi388ccf+nXWr18PKysrffciQLaN3bt3x9ouooqNRG0bUeL6nBLi4uKC2rVrx7pEjUeKYmFhYZBsAECBAgUAyNgTALh37x48PDxiHaxHVcO7d+8eANlGLCwsUKRIkY/Glzt37ljLMmfObJD0T5w4EcHBwShQoACKFy+O4cOH49KlSx997rg8e/YM7969Q8GCBWPdV7hwYeh0Ojx48MBgeVI/8/j4+Pjg7NmzeP/+PY4cOQJ3d3d89tlnKFmypL5r4NGjRw0OuBP7mcfFyckJAD667cZ8rfz588dKiON7rQ+/u6hE68NxpIl9bXd3d2TIkMFgube3d5Kf60PxfX+rVq1CiRIl9GOZXF1d8ddff+n3LUl17949WFhYxIrZzc0NmTJl+ujnB5h22ydKKzjmioig0+mQLVs2g1aTmKLOqmo0GmzcuBEnT57En3/+iT179qBr166YPXs2Tp48qR8zkhT+/v76Qeb58+ePdf+aNWvQs2fPJD9vQuJrwdJqtfE+JmYrVZTWrVvj+PHjGD58OEqVKgUHBwfodDrUq1cvWWWcO3bsiA0bNuD48eMoXrw4tm/fjj59+sQ6mPxQ4cKFceHCBdy/fz/OAyIA+gOemAlF27Zt0aVLF1y4cAGlSpXCH3/8gVq1asHFxUW/jk6nQ506dTBixIg4nzcqwYkS1+eUmsXXIqTEKNZQrVo1BAQEYNu2bdi7dy9++eUXzJ07F4sXLzYof24qxvrMq1atioiICJw4cQJHjhzRJ1E+Pj44cuQIrl+/jmfPnhkkV5+iUKFCAIDLly8b5fk+lJjvzhQ0Gk2crxHf/iWu72/16tXo3LkzmjZtiuHDhyNbtmywtLTEtGnTDIr3JDe+xEgN2z6ROWJyRUTw8vLCvn37UKVKlUQdqFWsWBEVK1bElClTsHbtWnz55Zf4/fff0b179yR3vVuzZg2sra3x22+/xfpnfvToUcyfP1+fNHh5eeHUqVOIiIiIt1Syl5cX9uzZg//++y/e1quoM9gxK6MBCZ9l/9DLly+xf/9+TJgwAWPHjtUv/7B7jKurK5ycnHDlypWPPme9evXg6uqKNWvWoEKFCnj37h2++uqrjz6uUaNGWLduHX799Vd89913se4PCQnBtm3bUKhQIYOz1k2bNkWvXr30XQNv3ryJb7/91uCxXl5eePPmjepl8XU6HW7fvm2QzN28eRMA9BXY8uTJg3379sXqJnn9+nX9/YC8J51Oh6tXr6JUqVJGiS9Llizo0qULunTpoi/4MH78+CQfYLq6uiJDhgy4ceNGrPuuX78OCwsL5MqVyygxf6h8+fKwsbHBkSNHcOTIEQwfPhyAHEAvXboU+/fv19+OktjPPC4FChRAwYIFsW3bNvzwww8fPTmTJ08eXLp0CTqdzuCEQ2Je61PlyZMHBw4cwLt37wxar27duhVr3cyZM8fZ9TAp+5eNGzciX7582Lx5s8E+9cPuzknZ3+bJkwc6nQ7+/v4Gc5s9efIEwcHByf78jLXtE6UV7BZIRGjdujW0Wi0mTZoU677IyEh9EvLy5ctYZ2SjDk6jugZGHXh8mLjEZ82aNfDx8UGbNm3QsmVLg0vUwV1UGfIWLVrg+fPnBmXCo0TF1aJFCyiKEmeZ8ah1nJyc4OLiEmu80KJFixIVMxB9VvfDz+PD6nkWFhZo2rQp/vzzT30p+LhiAmRemnbt2uGPP/7AypUrUbx48URVWmzZsiWKFCmC6dOnx3oNnU6H3r174+XLl7EOzDJlygRfX1/88ccf+P3332FjY4OmTZsarNO6dWucOHECe/bsifW6wcHBiIyM/Gh8xhLze1cUBQsXLoS1tTVq1aoFAGjQoAG0Wm2s7WPu3LnQaDSoX78+AEkqLSwsMHHixFgtjMlp1fiw9L6DgwO8vb2T1V3W0tISdevWxbZt2/TdHQE5AF67di2qVq2q705nbHZ2dihXrhzWrVuH+/fvG7RcvX//HvPnz4eXlxfc3d31j0nsZx6fCRMm4MWLF/qqnh/au3cvduzYoX+tx48fG4wTjIyMxIIFC+Dg4IDq1asn+71/jK+vLyIiIrB06VL9Mp1Ohx9//DHWul5eXvpWvigXL17EsWPHEv16ce1fTp06pR/7FiUp+9sGDRoAiL2PmjNnDgCgYcOGiY4vijG3faK0gi1XRITq1aujV69emDZtGi5cuIC6devC2toa/v7+2LBhA3744Qe0bNkSq1atwqJFi9CsWTN4eXnh9evXWLp0KZycnPT/uO3t7VGkSBGsX78eBQoUQJYsWVCsWLE4xxydOnVKX1Y5Ljly5MBnn32GNWvW4JtvvkHHjh3x66+/YsiQITh9+jR8fHzw9u1b7Nu3D3369EGTJk1Qs2ZNfPXVV5g/fz78/f31XfSOHDmCmjVr6l+re/fumD59Orp3746yZcvi8OHD+paQxHByckK1atUwc+ZMREREIEeOHNi7dy/u3LkTa92pU6di7969qF69Onr27InChQsjKCgIGzZswNGjRw0mMO3YsSPmz5+PAwcOYMaMGYmKxcbGBhs3bkStWrVQtWpVdOnSBWXLlkVwcDDWrl2Lc+fOYejQoWjbtm2sx7Zp0wYdOnTAokWL4OvrG2sy1eHDh2P79u1o1KiRvgzz27dvcfnyZWzcuBF379416EaYVIGBgVi9enWs5Q4ODgaJnp2dHXbv3o1OnTqhQoUK2LVrF/766y+MGjVK3221cePGqFmzJkaPHo27d++iZMmS2Lt3L7Zt24ZBgwbpy2x7e3tj9OjRmDRpEnx8fNC8eXPY2trizJkz8PDwwLRp05L0HooUKYIaNWqgTJkyyJIlC/755x9s3Lgx3u36YyZPngw/Pz9UrVoVffr0gZWVFZYsWYKwsDDMnDkzWc+ZWD4+Ppg+fTqcnZ1RvHhxAEC2bNlQsGBB3LhxQz8PVJTEfubxadOmDS5fvowpU6bg/PnzaNeuHfLkyYMXL15g9+7d2L9/v34cYs+ePbFkyRJ07twZZ8+ehaenJzZu3Ihjx45h3rx5iS6MkRxNmzZF+fLlMXToUNy6dQuFChXC9u3b8d9//wEwbEHq2rUr5syZA19fX3Tr1g1Pnz7F4sWLUbRo0VjFSOLTqFEjbN68Gc2aNUPDhg1x584dLF68GEWKFMGbN2/06yVlf1uyZEl06tQJP//8M4KDg1G9enWcPn0aq1atQtOmTVGzZs0kfy7G3vaJ0oSUL1BIRGr7sBR7lJ9//lkpU6aMYm9vrzg6OirFixdXRowYoTx69EhRFEU5d+6c0q5dOyV37tyKra2tki1bNqVRo0bKP//8Y/A8x48fV8qUKaPY2NgkWJa9f//+CgCDktMfGj9+vAJAuXjxoqIoUv589OjRSt68eRVra2vFzc1NadmypcFzREZGKrNmzVIKFSqk2NjYKK6urkr9+vWVs2fP6td59+6d0q1bN8XZ2VlxdHRUWrdurTx9+jTeUuxR5b5jevjwodKsWTMlU6ZMirOzs9KqVSvl0aNHcb7ne/fuKR07dlRcXV0VW1tbJV++fErfvn1jlVtWFEUpWrSoYmFhoTx8+DDezyUuT58+VYYMGaJ4e3srtra2SqZMmZTatWvry6/HJSQkRLG3t49V4jqm169fK99++63i7e2t2NjYKC4uLkrlypWV77//XgkPD1cUJf6y4glJqBR7zJLVnTp1UjJmzKgEBAQodevWVTJkyKBkz55dGTduXKxS6q9fv1YGDx6seHh4KNbW1kr+/PmVWbNmGZRYj7J8+XKldOnSiq2trZI5c2alevXqip+fn0F8cZWZ/rDU9uTJk5Xy5csrmTJlUuzt7ZVChQopU6ZM0X828UnoMzt37pzi6+urODg4KBkyZFBq1qypHD9+3GCd+MqIJ8aHpdij/PXXXwoApX79+gbLu3fvrgBQli1bFuu5kvKZx2f//v1KkyZNlGzZsilWVlaKq6ur0rhxY2Xbtm0G6z158kTp0qWL4uLiotjY2CjFixePNaVCQp/rh7/NxJZiVxQpe96+fXvF0dFRcXZ2Vjp37qwcO3ZMAaD8/vvvBuuuXr1ayZcvn2JjY6OUKlVK2bNnT7yl2OOKU6fTKVOnTlXy5Mmj2NraKqVLl1Z27NgRZ1zx7W8/LMWuKIoSERGhTJgwQb//zJUrl/Ltt98qoaGhBuuZetsnSss0imLikZ1ERJQkpUuXRpYsWfRjXNK7zp07Y+PGjQZn7InMwdatW9GsWTMcPXoUVapUUTscIjIDHHNFRGRG/vnnH1y4cAEdO3ZUOxQiiuH9+/cGt7VaLRYsWAAnJyd89tlnKkVFROaGY66IiMzAlStXcPbsWcyePRvu7u4GE/kSkfr69++P9+/fo1KlSggLC8PmzZtx/PhxTJ06Nc1NQUBEycfkiojIDGzcuBETJ05EwYIFsW7dOtjZ2akdEhHF8Pnnn2P27NnYsWMHQkND4e3tjQULFrB4AxEZ4JgrIiIiIiIiI+CYKyIiIiIiIiNgckVERERERGQEHHMVB51Oh0ePHsHR0dFgYkAiIiIiIkpfFEXB69ev4eHhAQuLhNummFzF4dGjR8iVK5faYRARERERkZl48OABcubMmeA6TK7i4OjoCEA+QCcnJ5WjoeSKiIjA3r17UbduXVhbW6sdDqVx3N4opXGbo5TE7Y1SmjltcyEhIciVK5c+R0gIk6s4RHUFdHJyYnKVikVERCBDhgxwcnJS/UdJaR+3N0pp3OYoJXF7o5RmjttcYoYLsaAFERERERGRETC5IiIiIiIiMgImV0REREREREbAMVfJpCgKIiMjodVq1Q6F4hEREQErKyuEhoaq+j1ZW1vD0tJStdcnIiIiopTB5CoZwsPDERQUhHfv3qkdCiVAURS4ubnhwYMHqs5XptFokDNnTjg4OKgWAxERERGZHpOrJNLpdLhz5w4sLS3h4eEBGxsbTjRspnQ6Hd68eQMHB4ePTvhmKoqi4NmzZ3j48CHy58/PFiwiIiKiNIzJVRKFh4dDp9MhV65cyJAhg9rhUAJ0Oh3Cw8NhZ2enWnIFAK6urrh79y4iIiKYXBERERGlYSxokUxqHqxT6sKWTSIiIqL0gRkCERERERGRETC5IiIiIiIiMgImVyrRaoGDB4F16+SvmhXda9SogUGDBqkXwCe6e/cuNBoNLly4oHYoRERERJSOMblSwebNgKcnULMm0L69/PX0lOWm1LlzZ2g0mliXmTNnYtKkSSZ5zYMHD8b5mjEvBw8e/KTXyJUrF4KCglCsWDHjBE1ERERElAysFpjCNm8GWrYEFMVweWCgLN+4EWje3HSvX69ePaxYscJgmaurq8mq2FWuXBlBQUH62wMHDkRISIhBDFmyZPmk17C0tISbm9snPQcRERER0adiy5URKArw9u3HLyEhwIABsROrqOcAgIEDZb3EPF9cz/Mxtra2cHNzM7jUqlXLoFugp6cnpk6diq5du8LR0RG5c+fGzz//bPA8Dx48QOvWrZEpUyZkyZIFTZo0wd27d2O9no2NjcFr2dvbG8TQtm1bjBgxwuAxTZs2RefOnRMdz4fdAqNay/bv34+aNWvCwcEBlStXxo0bNwxeZ/LkyciWLRscHR3RvXt3jBw5EqVKlUr6h0pEREQUB3MaBkIpg8mVEbx7Bzg4fPzi7CwtVPFRFODhQ1kvMc/37p3p3tPs2bNRtmxZnD9/Hn369EHv3r31yUlERAR8fX3h6OiII0eO4NixY3BwcEC9evUQHh6e4vHEZ8yYMZg8eTJOnz4NKysrdO3aVX/fmjVrMGXKFMyYMQNnz55F7ty58dNPP5kkdiIiIkp/1BoGQupicpXO7NixAw4ODvpLq1at4lyvQYMG6NOnD7y9vfHNN9/AxcUFBw4cAACsX78eOp0Ov/zyC4oXL47ChQtjxYoVuH///iePn4pPQvHEZ9KkSahSpQqKFCmCkSNH4vjx4wgNDQUALFiwAN26dUOXLl1QoEABjB07FsWLFzdJ7ERERJS+RA0DefjQcHnUMBAmWGkXkysjyJABePPm45edOxP3fDt3Ju75MmRIeqw1a9bEhQsX9Jf58+fHuV6JEiX01zUaDdzc3PD06VMAwMWLF3Hr1i04Ojrqk7QsWbIgNDQUAQEBOHLkiEECt2bNmqQHmoR4EvMYd3d3ANA/5saNGyhfvrzB+h/eJiIiIkoqrVaGeSQ0DGTQIHYRTKtY0MIINBogY8aPr1e3LpAzp5y1iOsHp9HI/XXrAiaqL4GMGTPC29v7o+tZW1t/EJsGOp0OAPDmzRuUKVMmzqTJ1dUVNjY2BmXRs2fPHu/rWFhYQPngw4iIiEhSPIl5DxqNBgA++hgiIiKiT3HkSOwWq5gUBXjwQNarUSPFwqIUwparFGRpCfzwg1z//7G+XtTtefNMl1gZy2effQZ/f39ky5YN3t7eBhdnZ2fY29sbLHN0dIz3uVxdXQ2qCWq1Wly5csXk76FgwYI4c+aMwbIPbxMREREl1tu3wLJlQLduiVs/xuEPpSFMrlJY8+ZSbj1HDsPlOXOavgy7sXz55ZdwcXFBkyZNcOTIEdy5cwcHDx7EgAED8DChUzVx+Pzzz/HXX3/hr7/+wvXr19G7d28EBwebJvAY+vfvj2XLlmHVqlXw9/fH5MmTcenSJX0LFxEREVFiXLoE9O0LeHgA3bsDt28n7nH/H7FAaQy7BaqgeXOgSRNpDg4Kkh+Xj4/5t1hFyZAhAw4fPoxvvvkGzZs3x+vXr5EjRw7UqlULTk5OSXqurl274uLFi+jYsSOsrKwwePBg1KxZ00SRR/vyyy9x+/ZtDBs2DKGhoWjdujU6d+6M06dPm/y1iYiIKHV79w744w9gyRLg5Mno5V5ekmDNnw88fhz/tDm5csmxH6U9GuXDAS+EkJAQODs749WrV7GShdDQUNy5cwd58+aFnZ2dShFSYuh0OoSEhMDJyQkWFh9vpK1Tpw7c3Nzw22+/GTUObjPpQ0REBHbu3IkGDRrEGiNIZArc5iglcXsT//4rCdWvvwKvXskyKyugaVOgVy/g888BC4voaoFA3AnWzz8DPXqkWNipkjltcwnlBh9iyxWlS+/evcPixYvh6+sLS0tLrFu3Dvv27YOfn5/aoREREZEZef9ehm4sWQIcOxa9PG9eSZC6dAHc3AwfEzUMZOBAw+IWVlZAZKQkVx06APb2KfMeKOUwuaJ0SaPRYOfOnZgyZQpCQ0NRsGBBbNq0CbVr11Y7NCIiIjID169LQrVqFfDypSyztAS++EJaqerUkVaq+MQ1DCRHDqBSJeCff4CuXYG1a2MXOaPUjckVpUv29vbYt2+f2mEQERGRGQkLAzZtkqTq8OHo5blzSytV165SuCKxLC1jl1vftAmoXRv4/XegaFHgu++MEjqZCSZXRERERJSu3bwpXfVWrgRevJBlFhZAo0bSSuXra7zCY9WrA4sWAT17AmPGAEWKpI5q0ZQ4TK6IiIiIKN0JDwe2bJFWqgMHopfnzCkV/7p1k+um0KMHcOWKVBX86isgXz6gVCnTvBalLCZXRERERJRuBARIK9WKFcCzZ7JMowEaNJBWqvr1pfCEqc2eDVy7Bvj5yTiuM2eA7NlN/7pkWkyuiIiIiChNi4gAtm2TVqqYQ67d3aWVqnt3GVeVkqysgPXrgYoVpVtis2bSgmZrm7JxkHExuSIiIiKiNOnOHWDpUmD5cuDJE1mm0cgYql69ZExVSrRSxSdzZuDPP4EKFYATJ2Qc1sqVrCCYmjG5IiIiIqI0IyIC2LFDWqn27o2exDd7dhlH1b27zFFlLgoUAP74Q7oj/vorULw4MGyY2lFRcjG5Smn37wPPn8d/v4tLyrdLU5JoNBps2bIFTZs2VTsUIiIi+r9794BffgGWLZN5paLUqSOtVF98AVhbqxdfQurUAebOBQYMAEaMAAoXBho2VDsqSo4Epj4jo7t/HyhYEChTJv5LwYKynpF17twZGo0m1uXWrVtGf62kunPnDtq3bw8PDw/Y2dkhZ86caNKkCa5fv27S17179y40Gg0uXLhg0tchIiIi04iMBLZvl0Qkb15g8mRJrFxdgW++AW7dktarFi3MN7GK0q+fdAtUFKBdO+Dff9WOiJKDLVcp6flzIDQ04XVCQ2U9E7Re1atXDytWrDBY5urqGmu98PBw2NjYGP314xIREYE6deqgYMGC2Lx5M9zd3fHw4UPs2rULwcHByXrOlIyfiIiIUt7Dh9GtVA8fRi+vWVNaqZo1A1LboYBGAyxYANy4ARw6JC1tp05JpyZKPdhyZUxv38Z/+VhSFdOH68b1fMlga2sLNzc3g4ulpSVq1KiBfv36YdCgQXBxcYGvry8A4NChQyhfvjxsbW3h7u6OkSNHIjIyUv98NWrUQP/+/TFo0CBkzpwZ2bNnx9KlS/H27Vt06dIFjo6O8Pb2xq5du+KN6d9//0VAQAAWLVqEihUrIk+ePKhSpQomT56MihUr6td7+PAh2rVrhyxZsiBjxowoW7YsTp06BQAYP348SpUqhV9++QV58+aFnZ0dAGD37t2oV68esmTJgqxZs6JRo0YICAjQP2fe/3e4Ll26NDQaDWrEmEJ9+fLlKFq0qP699+vXzyDu58+fo1mzZsiQIQPy58+P7du3J+s7ISIiosTRaoG//pKkI08eYMIESayyZgWGDpWk5O+/gTZtUl9iFcXGBti4Uea9un0baNlS5uOi1IPJlTE5OMR/adEi8c/Tv7/hbU/P2M9nZKtWrYKNjQ2OHTuGxYsXIzAwEA0aNEC5cuVw8eJF/PTTT1i2bBkmT54c63EuLi44ffo0+vfvj969e6NVq1aoXLkyzp07h7p16+Krr77Cu3fv4nxdV1dXWFhYYOPGjdBqtXGu8+bNG1SvXh2BgYHYvn07Ll68iBEjRkCn0+nXuXXrFjZt2oTNmzfru/m9ffsWffv2xenTp7F//35YWFigWbNm+sedPn0aALBv3z4EBQVh8+bNAICffvoJffv2Rc+ePXH58mVs374d3t7eBjFNmDABrVu3xqVLl9CgQQN8+eWX+O+//5L+wRMREVGCHj0CJk2ShKNRI6mup9MB1asDa9ZIgvX991IYIi1wcZGujo6O0oLVv390UQ5KBRSK5dWrVwoA5dWrV7Hue//+vXL16lXl/fv3sR8o237clwYNFOXs2YTXibp89pnh87q4xF4niTp16qRYWloqGTNm1F9atmypKIqiVK9eXSldurTB+qNGjVIKFiyo6HQ6/bIff/xRcXBwULRarf5xVatW1d8fGRmpZMyYUfnqq6/0y4KCghQAyokTJ+KNbeHChUqGDBkUR0dHpWbNmsrEiROVgIAA/f1LlixRHB0dlRcvXsT5+HHjxinW1tbK06dPDZZrtVrl5cuX+nifPXumAFAuX76sKIqi3LlzRwGgnD9/3uBxHh4eyujRo+ONF4Dy3Xff6W+/efNGAaDs2rUrzvUT3GYozQgPD1e2bt2qhIeHqx0KpRPc5iglpfT2ptUqyq5ditK0qaJYWkYf/mTOrCiDByvKtWspEoaqduxQFI1G3veCBWpHk/LMaR+XUG7wIbZcGdObN/FfNm1K/PMsWGB4++7d2M+XDDVr1sSFCxf0l/nz5+vvK1OmjMG6165dQ6VKlaCJMdFClSpV8ObNGzyM0bm5RIkS+uuWlpbImjUrihcvrl+W/f9TjT99+jTeuPr27YvHjx9jzZo1qFSpEjZs2ICiRYvCz88PAHDhwgWULl0aWbJkifc58uTJE2v8mL+/P7p16wZvb284OTnB09MTAHA/gYIhT58+xaNHj1CrVq141wEM33fGjBnh5OSU4HskIiKij3v8GJg6FfDyktLkW7dKd8AqVaRMeWAgMGcOUKiQ2pGaXsOGwMyZcn3QIOD/h0Vk5ljQwpgyZjTO8/x/zJCxnzdjxoyxurfFvC85rD8ovaPRaAyWRSVnMbvwxcXR0RGNGzdG48aNMXnyZPj6+mLy5MmoU6cO7O3tPxpHXPE3adIEOXLkwJIlS5AzZ07odDoUK1YM4Ql0Xk7MawFxv++PvUciIiKKTacD9u+Xeam2bZMKgADg7Ax07CgFKooWVTdGtQwdCly5AqxaBbRuLQUu0kr3x7SKLVcUp8KFC+PEiRNQYnTyPXbsGBwdHZEzZ06TvrZGo0GhQoXw9v+FO0qUKIELFy4kaUzTixcvcOPGDQwdOhS1atVC4cKF8fLlS4N1oioKxhzr5ejoCE9PT+zfv98I74SIiIji8/QpMGOGJAt160onn8hIoGJFYMUKGWs1f376TawAqSC4ZAlQqRIQHAw0bgx8cDhDZkbV5MrT0zPOuZf69u0LQKrRfXjf119/neBzKoqCsWPHwt3dHfb29qhduzb8/f1T4u18nItL7FapD9nZmUXNzT59+uDBgwfo378/rl+/jm3btmHcuHEYMmQILCyMt9lcuHABTZo0wcaNG3H16lXcunULy5Ytw/Lly9GkSRMAQLt27eDm5oamTZvi2LFjuH37NjZt2oQTJ07E+7yZM2dG1qxZsWrVKty6dQt///03hgwZYrBOtmzZYG9vj927d+PJkyd49eoVAKk+OHv2bMyfPx/+/v44d+4cFnzYVZOIiIiSTFGiK/rlzAmMHAkEBABOTkCfPsDFi8CJE0DnzkCGDGpHax5sbYEtW4BcuYCbN+Wzi1G8mcyMqt0Cz5w5Y9BqcOXKFdSpUwetWrXSL+vRowcmTpyov53hI7+0mTNnYv78+Vi1ahXy5s2LMWPGwNfXF1evXtWX6FZN7txSJ/T58/jXcXExyRxXSZUjRw7s3LkTw4cPR8mSJZElSxZ069YN3333nVFfJ2fOnPD09MSECRP0k/pG3R48eDAAaWHau3cvhg4digYNGiAyMhJFihTBjz/+GO/zWlhYYO3atRgwYABKlCiBggULYv78+Qbl1q2srDB//nxMnDgRY8eOhY+PDw4ePIhOnTohNDQUc+fOxbBhw+Di4oKWLVsa9X0TERGlJ8+fAytXAj//DMQ8512unHT7a9vWeKMr0qLs2aWCYJUqMvZq6FDghx/UjoriolEU8ynuOGjQIOzYsQP+/v76eYdKlSqFefPmJerxiqLAw8MDQ4cOxbBhwwAAr169Qvbs2bFy5Uq0bds2Uc8TEhICZ2dnvHr1Ck5OTgb3hYaG4s6dOwbzKZF50ul0CAkJgZOTk1Fb25KK20z6EBERgZ07d6JBgwaxxuQRmQK3OUpJydneFAU4fFi6tW3aFD1fk4MD8OWXklSVLm3CoNOgzZujZ/dZsgTo2VPdeEzJnPZxCeUGHzKbghbh4eFYvXo1hgwZYlChbs2aNVi9ejXc3NzQuHFjjBkzJt7Wqzt37uDx48eoXbu2fpmzszMqVKiAEydOxJtchYWFISwsTH87JCQEgHypERERButGRERAURTodDoWMDBzUecNor4vteh0OiiKgoiICFhaWqoWB5lW1L7iw30Gkalwm6OUlJTt7b//gNWrLbB0qQVu3Ig+pitdWkGPHlq0aaPA0THqeU0SbprVuDEwfrwFxo+3RN++Cry8tKhWzWzaSYzKnPZxSYnBbJKrrVu3Ijg4GJ07d9Yva9++PfLkyQMPDw9cunQJ33zzDW7cuKGf7PVDjx8/BhBd/jtK9uzZ9ffFZdq0aZgwYUKs5Xv37o2VyFlZWcHNzQ1v3rxJsOocmY/Xr1+r+vrh4eF4//49Dh8+jEh2kk7z/Fgrl1IYtzkyNa0WuHo1K16+zIHLl8+hSJEX+PBcoaIA169nwZ49njh2zAMREbKCnV0kfHwewtf3Hry9gwEAR46k8BtIY0qWBHx8yuDIkZxo1kyLWbMOw83tndphmYw57OPevUv852s23QJ9fX1hY2ODP//8M951/v77b9SqVQu3bt2Cl5dXrPuPHz+OKlWq4NGjR3B3d9cvb926NTQaDdavXx/n88bVcpUrVy48f/48zm6BDx48gKenJ7t4mTlFUfD69Ws4OjoatIamtNDQUNy9exe5cuXiNpOGRUREwM/PD3Xq1FG9+wKlD9zmKCVs2aLBkCGWCAyM/j+aI4eCOXO0aNZMwcuXwNq10kp19Wr0OiVKKOjRQ4d27XT4SC8qSob374HPP7fE2bMWKFJEweHDkWnuczanfVxISAhcXFxST7fAe/fuYd++ffG2SEWpUKECAMSbXLm5uQEAnjx5YpBcPXnyBKVKlYr3eW1tbWFraxtrubW1dawvU6vVQqPRwMLCQtVxPPRxUV0Bo74vtVhYWOjn/1J750Cmx++ZUhq3OTKVzZul0MSHp+EfPdKgTRsr1Kgh8y69fy/L7e1l/V69gPLlNdBoLAGwO7wpWFvLnGDlygFXr2rQubM1tm5FrBbFtMAc9nFJeX2zyA5WrFiBbNmyoWHDhgmud+HCBQAwSJxiyps3L9zc3AzmKAoJCcGpU6dQqVIlo8ULAGbS4EepALcVIiJKbbRaYODA2IkVEL3s4EFJrIoVAxYskHmpli8HKlSQ+ZnItHLkkATLzg7YsQMYNUrtiAgwg+RKp9NhxYoV6NSpE6ysohvSAgICMGnSJJw9exZ3797F9u3b0bFjR1SrVg0lSpTQr1eoUCFs2bIFgLRQDBo0CJMnT8b27dtx+fJldOzYER4eHmjatKlR4o3KXJPS95LSt6ixeSxmQUREqcWRI8DDhx9fb8EC4NIloF8/IFMmk4dFHyhXTiZcBoCZM4Fff1U3HjKDboH79u3D/fv30bVrV4PlNjY22LdvH+bNm4e3b98iV65caNGiRax5lm7cuKGf/BUARowYgbdv36Jnz54IDg5G1apVsXv3bqONdbG0tESmTJnw9OlTADLvlprjeSh+Op0O4eHhCA0NVa1boE6nw7Nnz5AhQwaDkwdERETmLCgocetlzcpWKrW1bQv8+y8weTLQoweQPz9g5A5blASqH+3VrVs3zm5TuXLlwqFDhz76+A8fq9FoMHHiRIOJh40tamxXVIJF5klRFLx//x729vaqJsAWFhbInTs3k3AiIko14hmBkez1yLQmTJAEa8sWoGlT4MwZIHdutaNKn1RPrlIjjUYDd3d3ZMuWzSxq71PcIiIicPjwYVSrVk3VgZA2NjYsfkJERKmKjw+QM2f8XQM1Grnfxydl46K4WVhIl8CqVYGLF4EmTYCjR4GMGdWOLP1hcvUJLC0tOY7GjFlaWiIyMhJ2dnaqV5khIiJKTSwtgfHjge7dY98X1RFj3ry0WZ0utXJwkAIX5csDFy4AnToBf/whiRelHH7cRERERBTLvn3y98PzkzlzAhs3As2bp3xMlLA8eaRroI0NsGmTdBeklMXkioiIiIgM7N0L/P67tHocOwb4+UViyJB/4OcXiTt3mFiZs8qVgSVL5PrEicD69erGk96wWyARERER6b1/D/TuLdf795dy3xERCt6+DUT16iXZFTAV6NxZClx8/71c9/ICypZVO6r0gS1XRERERKQ3ZQpw+7ZMUjtpktrRUHJNnw40aACEhkqBi0eP1I4ofWByRUREREQAgGvXZDJaAJg/H3B0VDceSj5LS2DdOqBIEUmsmjaVVkkyLSZXRERERARFAb7+GoiIABo1Apo1Uzsi+lROTsD27UCWLDL3Vbdu8j2T6TC5IiIiIiKsXAkcPgxkyAAsXBhdcp1SNy8vqRxoZSUtWdOmqR1R2sbkioiIiCide/4cGD5cro8fLyW9Ke2oUQP48Ue5Pnq0lGsn02ByRURERJTODR8OvHgBFC8ODBqkdjRkCj17SvVHAPjqK+DiRXXjSauYXBERERGlY4cOSZdAjUbmR/pw0mBKO+bMAWrXBt6+Bb74Anj6VO2I0h4mV0RERETpVFiYFLEApGWjUiV14yHTsrIC/vgDyJ8fuH9fJoMOC1M7qrSFyRURERFROjVrFnD9OpAtGwsdpBeZMwN//gk4OwPHjsmE0awgaDxMroiIiIjSoVu3gMmT5frcuXLQTelDwYLSgmVhAaxYId8/GQeTKyIiIqJ0RlGkxSIsDKhTB2jXTu2IKKXVrRudVA0fDuzcqW48aQWTKyIiIqJ0Zt06YN8+wNYWWLSIc1qlV/37Az16ADqdJNhXr6odUerH5IqIiIgoHXn5Ehg8WK5/9x3g7a1uPKQejUYmjK5WDQgJkQqCL16oHVXqxuSKiIiIKB359lspwV2oUPTEwZR+2dgAmzYBnp5AQADQsiUQEaF2VKkXkysiIiKidOLECZnLCgAWL5ZugUQuLlJB0MEBOHhQuguygmDyMLkiIiIiSgciIoBeveR6585A9eqqhkNmplgxYO3a6MmkFy1SO6LUickVERERUTowbx5w+TKQNavMb0X0ocaNgenT5frAgVL0hJKGyRURERFRGnfvHjB+vFyfNUu6gRHFZfhwoGNHQKsFWrUC/P3Vjih1YXJFRERElIYpCtCvH/DunVSF69xZ7YjInEV1C6xUCQgOltas4GC1o0o9mFwRERERpWFbtgA7dgDW1lLEgnNa0cfY2QGbNwO5cgE3bgBt2wKRkWpHlTowuSIiIiJKo16/BgYMkOsjRgCFC6sbD6Uebm7Atm1AhgzAnj0s259YTK6IiIiI0qgxY4DAQMDLCxg9Wu1oKLUpXRr49Ve5Pm8e8MsvqoaTKjC5IiIiIkqDzp0DFiyQ64sWAfb26sZDqVOLFsDEiXK9Tx/g8GF14zF3TK6IiIiI0hitVua00ulkvEzdumpHRKnZd98BbdrIXGktWgB37qgdkflickVERESUxixaBPzzD+DsDMydq3Y0lNppNMDy5UCZMsDz58AXX8h4PoqNyRURERFRGhIYGD2+ato0KUxA9KkyZJACF+7uwJUrwJdfSgspGWJyRURERJSGDBokrQoVKkjXQCJjyZED2LoVsLUF/vxTuguSISZXRERERGnEzp3Axo2ApaVMBGvBIz0ysvLlpYsgAEyfDqxerW485oY/OSIiIqI04O1bqeYGSOtVyZKqhkNpWPv2wKhRcr17d+DkSXXjMSdMroiIiIjSgIkTgXv3gNy5gfHj1Y6G0rpJk4CmTYGwMPn74IHaEZkHJldEREREqdzly8CcOXJ94ULAwUHdeCjts7AAfvsNKFECePIEaNJEWk/TOyZXRERERKmYTieFKyIjgWbNgMaN1Y6I0gsHB2D7dsDVFTh/HujUSbbH9IzJFREREVEq9ssvwIkTcqA7f77a0VB6kycPsGULYG0NbNok3VPTMyZXRERERKnUkyfAN9/I9UmTgJw51Y2H0qcqVYCff5brEyYAf/yhbjxqYnJFRERElEoNHQoEBwOlSwP9+qkdDaVnnTvL9hh1/exZNaNRD5MrIiIiolRo3z5gzRpAo5E5rays1I6I0rsZM4D69YH376XARVCQ2hGlPCZXRERERKlMaGj0nFZ9+wLlyqkbDxEgk1evWwcULgwEBkqJ9vfv1Y4qZTG5IiIiIkplpk0D/P0Bd3dg8mS1oyGK5uwsFQSzZAFOnwZ69AAURe2oUg6TKyIiIqJU5MYNYPp0uf7DD3IwS2ROvL2BjRulq+qaNdJdML1gckVERESUSigK8PXXQHi4jG1p2VLtiIjiVrMmsGCBXB81Cti2Td14UgqTKyIiIqJU4rffgIMHAXt74McfpZgFkbn6+msZE6gowJdfApcuqR2R6TG5IiIiIkoFXryILnU9diyQN6+68RAlxty5QK1awNu3wBdfAM+eqR2RaTG5IiIiIkoFvvkGeP4cKFo0OskiMnfW1sCGDUD+/MC9e0Dz5tKtNa1ickVERERk5o4cAZYtk+tLlsgBK1FqkTmzVBB0dgaOHgV69067FQSZXBERERGZsfBwGbsCAN27A1WqqBsPUXIUKgSsXw9YWADLlwPz5qkdkWkwuSIiIiIyY7NnA1evAq6u6aukNaU9vr7AnDlyfdgwYNcudeMxBSZXRERERGbq9m1g4kS5Pnu2TMxKlJoNGCAtsDod0LYtcO2a2hEZF5MrIiIiIjOkKFLGOjQU+PxzoEMHtSMi+nQajUwj4OMDhIQAjRtLJcy0QtXkytPTExqNJtalb9+++O+//9C/f38ULFgQ9vb2yJ07NwYMGIBXr14l+JydO3eO9Xz16tVLoXdEREREZBwbNgC7dwM2NsBPP3FOK0o7bGyATZsAT08gIABo3RqIiFA7KuOwUvPFz5w5A61Wq7995coV1KlTB61atcKjR4/w6NEjfP/99yhSpAju3buHr7/+Go8ePcLGjRsTfN569ephxYoV+tu2trYmew9ERERExhYcDAwcKNdHjQIKFFA1HCKjc3WVCoKVKwN//w0MGiQtWqmdqsmVq6urwe3p06fDy8sL1atXh0ajwaZNm/T3eXl5YcqUKejQoQMiIyNhZRV/6La2tnBzc0t0HGFhYQgLC9PfDgkJAQBEREQgIq2k0elQ1HfH75BSArc3Smnc5tK2b7+1wOPHlsifX8HQoZGqn9Xn9kamUKgQsGqVBi1bWmLRIg0KFdLi6691AMxrm0tKDKomVzGFh4dj9erVGDJkCDTxtHu/evUKTk5OCSZWAHDw4EFky5YNmTNnxueff47Jkycja9as8a4/bdo0TJgwIdbyvXv3IkOGDEl7I2R2/Pz81A6B0hFub5TSuM2lPTdvZsKSJdUAAB07Hsf+/c9VjigatzcyNktL4KuvvPHrr0UxaJAGwcGnUbToc1y9mhUvX+bA5cvnUKTIC1haqhfju3fvEr2uRlHMYwqvP/74A+3bt8f9+/fh4eER6/7nz5+jTJky6NChA6ZMmRLv8/z+++/IkCED8ubNi4CAAIwaNQoODg44ceIELOP5VuJqucqVKxeeP38OJyenT39zpIqIiAj4+fmhTp06sOZsi2Ri3N4opXGbS5siI4FKlaxw8aIGX36pw4oV2o8/KAVweyNTUhSgSxdLrF1rgYwZFTg4AE+eRDe25MihYM4cLZo1UydtCQkJgYuLi76hJyFm03K1bNky1K9fP87EKiQkBA0bNkSRIkUwfvz4BJ+nbdu2+uvFixdHiRIl4OXlhYMHD6JWrVpxPsbW1jbOcVnW1tbcgaQB/B4pJXF7o5TGbS5tWbAAuHgRyJwZmDPHAtbW5lXYmdsbmcqyZcCZM4C/vwZv3xre9+iRBm3bWmHjRqB585SPLSnbvFn8Yu/du4d9+/ahe/fuse57/fo16tWrB0dHR2zZsiXJP+h8+fLBxcUFt27dMla4REREREZ3/z4wdqxcnzkTyJZN3XiIUpK1NfDmTdz3RfWzGzQI0JpHY268zCK5WrFiBbJly4aGDRsaLA8JCUHdunVhY2OD7du3w87OLsnP/fDhQ7x48QLu7u7GCpeIiIjI6AYMAN6+BapUAbp2VTsaopR15AgQFBT//YoCPHgg65kz1ZMrnU6HFStWoFOnTgaFKqISq7dv32LZsmUICQnB48eP8fjxY4Py7YUKFcKWLVsAAG/evMHw4cNx8uRJ3L17F/v370eTJk3g7e0NX1/fFH9vRERERImxbZtcrKyAJUsAC9WP0IhSVkKJVXLWU4vqY6727duH+/fvo+sHp2jOnTuHU6dOAQC8vb0N7rtz5w48PT0BADdu3NBPLGxpaYlLly5h1apVCA4OhoeHB+rWrYtJkyZxrisiIiIyS2/eAP37y/Vhw4CiRdWNh0gNie1kZu6d0VRPrurWrYu4ChbWqFEjzuUfirmOvb099uzZY9T4iIiIiExp3Djp7pQ3LzBmjNrREKnDxwfImRMIDIweYxWTRiP3+/ikfGxJwUZnIiIiIpVcuAD88INc//FHgNNrUnplaRn9W/hwytuo2/PmQdX5rhKDyRURERGRCrRaoFcv+duqFVC/vtoREamreXNg40YgRw7D5TlzQrUy7EmlerdAIiIiovRoyRLg9GnAyUnOyBORJFBNmgAHDkRi164LqF+/FGrWtDL7FqsoTK6IiIiIUlhQEPDtt3J9yhTAw0PdeIjMiaUlUL26grdvA1G9eslUk1gB7BZIRERElOIGDwZCQoCyZYHevdWOhoiMhckVERERUQraswdYv17mslqyxPwH6BNR4jG5IiIiIkoh798DffrI9YEDgc8+UzceIjIuJldEREREKWTyZOD2bal+NnGi2tEQkbExuSIiIiJKAVevArNmyfUFCwAHB3XjISLjY3JFREREZGI6ncxpFREBfPEF0LSp2hERkSkwuSIiIiIysRUrgKNHgYwZpdWKiNImJldEREREJvTsGTBihFyfMAHInVvdeIjIdJhcEREREZnQsGHAf/8BJUtKhUAiSruYXBERERGZyIEDwK+/AhqNzGllZaV2RERkSkyuiIiIiEwgLAz4+mu5/vXXQIUK6sZDRKbH5IqIiIjIBGbMAG7eBNzcgKlT1Y6GiFICkysiIiIiI/P3j06o5s4FMmVSNRwiSiFMroiIiIiMSFGA3r2lW2DdukCbNmpHREQphckVERERkRGtXQvs3w/Y2QGLFkkxCyJKH5hcERERERnJy5fAkCFy/bvvAC8vdeMhopTF5IqIiIjISEaOBJ4+BQoXBoYPVzsaIkppTK6IiIiIjOD4ceDnn+X64sWAjY268RBRymNyRURERPSJIiKAXr3kepcuQLVq6sZDROpgckVERET0iebOBa5cAbJmBWbOVDsaIlILkysiIiKiT3D3LjB+vFyfPRtwcVEzGiJSE5MrIiIiomRSFKBfP+D9e6BGDaBjR7UjIiI1MbkiIiIiSqbNm4G//gKsrYGffuKcVkTpHZMrIiIiomQICQEGDJDrI0cChQqpGw8RqY/JFREREVEyjBkDPHoEeHsDo0apHQ0RmQMmV0RERERJ9M8/wMKFcv2nnwA7O3XjISLzwOSKiIiIKAkiI2VOK50OaN8eqF1b7YiIyFwwuSIiIiJKgh9/BM6dAzJlAubMUTsaIjInTK6IiIiIEunhQ+C77+T69OlA9uzqxkNE5oXJFREREVEiDRwIvHkDVKoE9OihdjREZG6YXBEREaUyWi1w6JAGhw/nwKFDGmi1akeUPuzYIfNaWVoCixcDFjyKIqIPcLdARESUimzeDHh6AnXqWGHOnLKoU8cKnp6ynEzn7Vugb1+5PmQIUKKEuvEQkXlickVERJRKbN4MtGwp435iCgyU5UywTGfCBOD+fSBPHmDcOLWjISJzxeSKiIgoFdBqZbyPosS+L2rZoEFgF0ETuHQpuirgwoVAxozqxkNE5ovJFRERUSpw5EjsFquYFAV48EDWI+PR6WROK60WaN4caNRI7YiIyJwxuSIiIkoFgoISt95ffwGvX5s2lvRk6VLg5EnAwQH44Qe1oyEic8fkioiIyMydOwfMnp24db//HsiSBahWDZg4EThxAoiMNG18adWTJ8DIkXJ98mQgZ0514yEi88fkioiIyEzdugW0bQuUKQOcPfvx9TNmlEqCkZHSPXDcOKByZSBrVqBpU+DHH4GbN+Met0WxDRkCBAcDn30G9OundjRElBowuSIiIjIzjx9L2e/ChYH16wGNBujQAVi0SK5rNIbrRy379Vfgzh0gIEDmYWrRAsicGQgJAbZtkwShYEGpeNetG/D778CzZ+q8R3Pn5wesXStzWf38s8xtRUT0MVZqB0BEREQiJES69c2ZI/MqAUD9+sC0aUDJknI7e3apGhizuEXOnMC8eVJwAQDy5ZMiDFGFGM6dk2Rh3z7g2DEpfLF8uVwAoFQpoE4doHZtwMcHsLdPqXdsnkJDgT595Hq/ftJySESUGEyuiIiIVBYWJi1NkycDz5/LsvLlgRkzgBo1DNdt3hxo0gQ4cCASu3ZdQP36pVCzplW8LSuWlkC5cnIZNUqStqNHJdny85My4xcuyGXWLMDWFqhaVRKtOnWA0qWl9SY9mTpVumR6eACTJqkdDRGlJkyuiIg+kVYLHDqkweHDOZAxowY1a7ILESWOVitdz8aOBe7elWUFCkhLVbNmsbv/RbG0BKpXV/D2bSCqVy+ZpO0tY0bA11cugBRt2L8/OtkKDJTb+/cD334r47U+/1wSrTp1ZExXWnb9OjB9ulyfPx9wclI3HiJKXZhcERF9gs2bo7poWQEoizlzpIvWDz9Ed9Ei+pCiALt2SfJy6ZIsc3cHJkwAunQBrFLwv3P27ED79nJRFODGjeguhAcOAC9eABs2yAUAvLyiE62aNWVMV1qhKMDXXwMREUDDhvwNE1HSMbkiIkqmzZuBli1jV14LDJTlGzfy4IxiO3kS+OYb4PBhue3sLOW+BwwAMmRQNzaNBihUSC79+0uScfp0dLJ18qQUy4gqmGFhAZQtG51sVaoE2Nio+x4+xa+/AocOyZizhQvjbzkkIopPOutFTURkHFqttFjFVdI6atmgQbIeESDdzZo3lwTk8GEZ2zRsGHD7tiRXaidWcbG2BqpUAcaPl3Fa//0nVQf795cETKeT5GvKFBkbljkz0KABMHcucPly6ir5/vw5MHSoXB8/Pu13fyQi02DLFRFRMhw5Ylit7UOKIhXZjhyJXZCA0pfAQDlYX75ckhELC6BzZ1mWK5fKwSWRkxPwxRdyAeQ3sG9fdMvW06fS3XHXLrnfzU0KY0RdcuRQL/aPGTFCukAWLw4MHqx2NESUWjG5IiJKouvXgZkzE7fuwYNA9ersXpQevXwphRHmz5fS3oBU+Zs6FShSRN3YjCVnTkkUO3eWxPHKlehE69Ahma9r9Wq5APK+o6oQVq8OODqqGX20w4eBFSvk+uLF0mJHRJQcTK6IiBIhLEzGWC1ZIgeNiTVhgoy96tkT+OqrtDX4n+L2/j2wYIFU/AsOlmVVq0pZ9cqVVQ3NpCwsgBIl5DJ0qPxmjh+Pbtn65x/g6lW5zJ8vRTsqVower1WuXMoW8ogSHi5FLAD5nabl74iITE/VMVeenp7QaDSxLn379gUAhIaGom/fvsiaNSscHBzQokULPHnyJMHnVBQFY8eOhbu7O+zt7VG7dm34+/unxNshojTI3x8YPlzO0LdvL4mVhQXQqBHg4pJwi1TGjDIw/t9/ZXyWh4ec4T9xInWNRaHEiYwEli0D8ueXghXBwUCxYsCff0rLSHo7aLe1lWqCU6bIuKznz+VEQ69eMslxZKSM4xo3Tj6brFmBpk2BH3+UioUp9RuZNQu4dg3Ili26BDsRUXKpmlydOXMGQUFB+oufnx8AoFWrVgCAwYMH488//8SGDRtw6NAhPHr0CM0/Unpr5syZmD9/PhYvXoxTp04hY8aM8PX1RWhUnwwioo8IDwf++AOoVUvmHPr+ezkwzJFDDgTv3pUD5iVLZP0PEyyNRi6//goEBUnVseLFpWvYqlVyIFmypBxEvnqV4m+PjExRgK1bpcWme3cZY5U7N7BypUzM26gRu4UCQJYsQIsW0u0uquLgkiVSWTNzZiAkRIpl9OsnxTLy5AG6dQPWrQOePTNNTAEBMnEzAMyZw5ZlIjICxYwMHDhQ8fLyUnQ6nRIcHKxYW1srGzZs0N9/7do1BYBy4sSJOB+v0+kUNzc3ZdasWfplwcHBiq2trbJu3bpEx/Hq1SsFgPLq1avkvxlSXXh4uLJ161YlPDxc7VAolQgIUJRvvlGUbNkURQ6ZFUWjUZQGDRRl2zZFiYiI/ZhNmxQlZ87o9QFFyZVLlsek0ynK8eOK0qmTotjZRa+bIYOidO2qKKdOyTqUuhw6pCiVKkV/n1myKMrs2Yry/r3pXzst7eMiIxXlzBlFmTpVUT7/XFFsbAx/U4CilCqlKMOGKcqePYry7t2nv6ZOpyh168pz167N39/HpKXtjVIHc9rmkpIbmM2Yq/DwcKxevRpDhgyBRqPB2bNnERERgdq1a+vXKVSoEHLnzo0TJ06gYsWKsZ7jzp07ePz4scFjnJ2dUaFCBZw4cQJt27aN87XDwsIQFhamvx0SEgIAiIiIQEREhLHeIqWwqO+O3yElJCIC+PNPDX75xQL79kU35ru7K+jcWYeuXXXIk0eWKYqsH1PjxlJ6+uBBLfz8rqBOnWKoUcMSlpax1y1bVi4zZwJr1lhg6VILXLumwfLlUkmuZEkFPXro0K6dzmwG+lPcLl0CxoyxxK5dss1kyKBgwAAdhg7VwdlZ1jH1riet7eNKlpTLsGHAu3fA0aMa/P23Bvv2WeDSJQ0uXJCWwO+/B2xtFVSpouDzzxXUrq1DqVLSXfdjtFp53qAg4Pp1DfbutYStrYIffohEZKSJ32Aql9a2NzJ/5rTNJSUGjaKYR8//P/74A+3bt8f9+/fh4eGBtWvXokuXLgZJDwCUL18eNWvWxIwZM2I9x/Hjx1GlShU8evQI7u7u+uWtW7eGRqPB+vXr43zt8ePHY8KECbGWr127FhnMceIRIvpkT57Yw8/PE/v358bLl3YAAI1GQalST+Hrew9lyz6GlZVpd4+KAly7lgV79nji+HEPRERYAgDs7CLh4/MQvr534e3NfoPm5MkTe6xbVxiHDuWEomhgYaFD3br30Lr1DWTJEvbxJ6BkCQ62xaVLLrh40RUXLmTDixf2Bvc7OoajePFnKFXqGUqWfIbs2d/Feo4TJ9zxyy/FYz22atWHGDbsrEnjJ6LU7d27d2jfvj1evXoFJyenBNc1m+TK19cXNjY2+PPPPwEgRZOruFqucuXKhefPn3/0AyTzFRERAT8/P9SpUwfWrKtLkAH0f/0lrVR792qgKDIQJnt2BZ066dCtmw558ybvuT91e3vxAli9Wlqzbt6MHqBTpowO3bvr0KaNAgeH5MVGn+75c2D6dAssXmyB8HD5flq21GHCBC3y51cnpvS6j1MU4OZNYP9+C+zbp8GhQxq8fm04qM3LS0GtWjrUqqWgRg0FBw9q0Lat5f+LZMRcV4FGA/z+uxbNmpnF4ZDZSq/bG6nHnLa5kJAQuLi4JCq5Motugffu3cO+ffuwefNm/TI3NzeEh4cjODgYmTJl0i9/8uQJ3Nzc4nyeqOVPnjwxSK6ePHmCUqVKxfv6tra2sLW1jbXc2tpa9S+TPh2/R7p/H/jlF6nk9uhR9PJataRyWZMmGtjYWAKw/OTXSu725uYm3aGGDpXKckuWAJs2AWfPWuDsWQuMGAF8+aXEm8DujIzs7Vtg7lzpyvn6tSz7/HMpq162rAVUrgsFIH3u44oVk8vAgXLS5PTp6Pm1Tp4EAgI0CAiwxM8/SzERa+v4qg9KojVsmBVatAAsP30XkOalx+2N1GUO21xSXl/9/woAVqxYgWzZsqFhw4b6ZWXKlIG1tTX279+vX3bjxg3cv38flSpVivN58ubNCzc3N4PHhISE4NSpU/E+hojSJq1WKvo1agTkzQtMmiSJlasrMGKElFjftw9o1QqwsVE72mgajUyuunYt8PChlInOn18O7BcvBkqXBipUkDFab9+qHW3aFREB/PQT4OUFjBkjn3/p0sCePbLdlC2rdoQUxcpKKnCOGwccOSKtwNu3AwMGAIULS1IVHh7/4xUFePBAHktE9KlUT650Oh1WrFiBTp06wSrG7IHOzs7o1q0bhgwZggMHDuDs2bPo0qULKlWqZFDMolChQtiyZQsAQKPRYNCgQZg8eTK2b9+Oy5cvo2PHjvDw8EDTpk1T+q0RkQoCA2XiXk9P4IsvgL/+AnQ6mW/n99/lIGrGDMDbW+1IP87VVVqzrl8H9u8HWreWM/CnT0uJ6hw5gP79gStX1I407dDpgPXrgSJFgD59gCdPZE6mdetkEty6dVlW3dw5OUmhmR9+kAmLFyxI3OOCgkwbFxGlD6p3C9y3bx/u37+Prl27xrpv7ty5sLCwQIsWLRAWFgZfX18sWrTIYJ0bN27gVYyJYkaMGIG3b9+iZ8+eCA4ORtWqVbF7927Y2dmZ/L0QkTq0WmlRWLIE2LFDDpABmVenSxegZ0+Zryq1srCQrmiffw48fQqsWAH8/DNw+7bMobVwoZy579VLWuLs7T/+nBTbvn3AyJHA2f/XNsiWDRg7FujRw7xaNylpihVL3HoxRhMQESWb2RS0MCchISFwdnZO1KA1Ml8RERHYuXMnGjRooHpfXTKNoCAZR/XLL8C9e9HLfXwk0WjRAkip8yopvb3pdNKatWSJTLwaVUY6c2agY0d5/4ULmzyMNOHsWUmq9u2T2w4OwPDhwJAhMOsiItzHJY5WKy3ZgYFxj7vSaICcOYE7dzjmKiHc3iilmdM2l5TcQPVugURESaHTSStV8+ZArlwyHubePSBTJhnc/u+/UhDiyy9TLrFSg4UFUKcOsHGjFOyYMkUOIF++lO5QRYoA1aoBa9YAoaFqR2uebt0C2raV8VP79kmXy4EDpUVw7FjzTqwo8Swt5TcBxO7SGXV73jwmVkRkHEyuiChVePIEmDZNxkrVqwds2SJnpCtXBlatkmIV8+ZJUpHeuLsDo0ZJsrBrF9C0qRwoHjkCdOggY7OGDgVu3FA7UvPw+DHQt6+07K1fLwfYHTrI5zNvnox1o7SleXM5EZEjh+HynDllefPm6sRFRGmP6mOuiIjio9MBBw5I17ctW6K7vjk7A199JV3fEjueIj2wtJTEs1496QIV1WXywQNgzhy51Kghn1uzZkAcM1CkaSEhUn1xzhzg3f/nmK1fX5L2kiXVjY1Mr3lzoEkTOekQFCQnJXx82GJFRMbF5IqIzM6zZ8DKlVK04dat6OUVKkhi0KYNkCGDauGlCjlySNe20aOB3bslQf3rL+DgQbm4uEQX+0gNlRM/RViYlFWfMkUmAwaA8uWlamSNGqqGRinM0pLfORGZVpK7BY4bNw73Yo4cJyIyAkWRg/527aSrzogRklg5OgK9ewMXLsjkoF26MLFKCktLoGFDmffn7l2ZCyhHDkkyoubQql0b2LAh4bmAUiOtFvjtN6BgQWDwYHnPBQvK5MwnT/Igm4iIjC/JydW2bdvg5eWFWrVqYe3atQgLCzNFXESUTrx4Id20ChWKnosqPFyKDCxdKmOpFi1ity1jyJULGD9ekqxt26RLnEYTPYdWrlzAt99KQYfUTFGkla50aamceO8e4OEhLaFXrkj3MM5VRUREppDk5OrChQs4c+YMihYtioEDB8LNzQ29e/fGmTNnTBEfEaVBihK72MLNm1KdrWdPKY195gzQvTsrtpmClZVMsLxzpyRSo0cDbm4yh9b06YCXF+DrC2zeDEREqB1t0kS1SDVqBFy+LFUkp08H/P1lviordoYnIiITSla1wNKlS2P+/Pl49OgRli1bhocPH6JKlSooUaIEfvjhB4NJfYmIokSVCS9aNLpMeFiYtDAsXiytVEuWAJ99pnak6YenJzB5spRz37QJqFtXlu/dK/OE5c4NfPed4Txi5ujaNSnSUamSlOK3tZW5qgICgG++YVdSIiJKGZ9Uil1RFERERCA8PByKoiBz5sxYuHAhcuXKhfXr1xsrRiJKxRQFOH4c6NRJumYNGiQHwhkyAN26AadPS0tVr14yvorUYW0t3eX27JGEZORIIFs2KVs+ZQqQNy/QoIHhhMXm4OFDaeEsVgzYulXm/+raVVqqZs4EsmRRO0IiIkpPkpVcnT17Fv369YO7uzsGDx6M0qVL49q1azh06BD8/f0xZcoUDBgwwNixElEqEhwMLFwIlCgBVKkC/PqrTGZbogTw44/SSvXLL0C5chz/Ym7y5ZPy5A8eAH/8AdSqJUly1Bxanp5SGOPBA/VifPlSWqTy55eS8zqdxHb5stzOlUu92IiIKP1KcnJVvHhxVKxYEXfu3MGyZcvw4MEDTJ8+Hd4xavm2a9cOz549M2qgRGT+FAU4dUpaDjw8gP79pYCAvT3QuTNw4oRU/evTR+aqIvNmYwO0agXs2yctQcOHSwn3wEBg4kRJsho3BnbskMp8KeH9e2mRypdP/oaGylxFx47JXGjpcRJpIiIyH0ke2tu6dWt07doVOT6c5jwGFxcX6HS6TwqMiFKPkBAZP7VkCXDxYvTyokWlu99XX0lhAUq9vL0lmZk0SZKYJUukdP6OHXLJlUu653XrJkVKjC0yUuY+Gz9ekjtAugJOny7dFdn6SURE5iDJLVdjxoxJMLEiovTjn3+kApuHh7RGXbwohQS++go4elS6aPXvz8QqLbG1Bdq2BQ4cAK5fB4YMkXFNDx5IV8E8eaR73q5dxmnNUhRJ5ooXl20tMFCKbKxaJa2gDRsysSIiIvOR5JarFi1aoHz58vjmm28Mls+cORNnzpzBhg0bjBYcEaUsrVZKpAcFAe7u0t3K0tJwndevgXXrpOXi3Lno5YUKSStVx44sIpBeFCwIzJ4tBS82bZJt4sgRKXqxbZskWj16SDdRd/fYj//Y9nbokBTWOHlSbmfNKmXje/cG7OxS5j0SESXL/fsyc3l8XFzkTBGlOUlOrg4fPozx48fHWl6/fn3Mnj3bGDERkQo2bwYGDpTqa1Fy5pTS6c2bA+fPy8HzmjXAmzdyv40N0LKlJFU+PmxBSK/s7IAvv5TL1asyWe+qVVK+/bvvpCvfF1/IdlK7tlT0S2h78/aWyYx37pTlGTJIC9mwYRyrR0SpwP37cvYpNDT+dezsgBs3mGClQUlOrt68eQMbG5tYy62trRESEmKUoIgoZW3eLEmSohguDwyUuY68vKQ8d5QCBWSy306d5OQbUZQiRYB586Ta4IYNkpAfPy7b2ObNUoiiUiVg7dr4t7coVlbS8jVmTNwtX0REZun584QTK0Duf/6cyVUalKxqgXHNYfX777+jCMs0EaU6Wq20IHx4oAtELwsIkAPdNm2Av/+WsTZDhzKxovjZ20sX0WPHZOxdv37S6nT7trR+JrS9AZLsX70KLFrExIqIiFKPJLdcjRkzBs2bN0dAQAA+//xzAMD+/fuxbt06jrciSoWOHDHsmhWfP/4AmjUzfTyU9hQrBixYAMyYAUyYIFUHP6ZvX5nDioiIKDVJcnLVuHFjbN26FVOnTsXGjRthb2+PEiVKYN++fahevbopYiQiEwoKStx6H+vhQPQxGTIApUolbt3EbpdERGaH0xGla0lOrgCgYcOGaNiwobFjIaIUdvs2sGxZ4tZl1ywyhsRuR9zeiChV8vOTOUgo3UrymCsiSv2ePpV9f6FCwP79Ca+r0cgEsT4+KRMbpW0+PlIVML7KktzeiCjVWr4cqFtXqgBSupXk5Eqr1eL7779H+fLl4ebmhixZshhciMh8vX4tZbG9vICFC4GICMDXF5g1Sw5qPzzgjbo9b17s+a6IksPSUsqtA9zeiCgNiIyMvt6yJZAjB9C+vXrxkOqSnFxNmDABc+bMQZs2bfDq1SsMGTIEzZs3h4WFRZzzXxGR+sLDpaCAl5cUFHjzBihbVlqtdu+W+YM2bpT/CTHlzCnLmzdXJ25Km5o35/ZGRKlcUJDMaF6zZnSpUycn4NYtmYviYzOd29mx5G4aleQxV2vWrMHSpUvRsGFDjB8/Hu3atYOXlxdKlCiBkydPYsCAAaaIk4iSQacDfv9dJnK9c0eW5c8PTJkiJ9hithw0bw40aSLVA4OCZMyLjw9bEMg0uL0RUar06pV095g7F3j3TpYdPRrdl9nOTuauunFD5rGKaeRIGZNVpgywaRPnuEqjkpxcPX78GMWLFwcAODg44NWrVwCARo0aYcyYMcaNLp3TanngQcmjKMCePcC33wIXLsgyNzfpEti1K2BtHffjLC2BGjVSKEhKv+7fB54/hyWAGk4AnP6//OL//7q48KCDiMxLaKhMvDd1KvDihSyrWFHmmIhrkGju3LH3Yz//LIOdz56VS548po+bUlySk6ucOXMiKCgIuXPnhpeXF/bu3YvPPvsMZ86cga2trSliTJc2b5aJXWPOP5Qzp4xVYJcZSsjp03Jy7MABue3kBHzzjWxPGTOqGxsR7t8HChZMuLa/nZ2c9WWCRUTm4O5doHp12X8BkiBNnQo0bRp/dZ64eHpKP/wpU+RvgwYf7z5IqU6Sx1w1a9YM+/9fXqx///4YM2YM8ufPj44dO6Jr165GDzA92rxZumx9OLFrYKAs37xZnbjIvN24IdtHhQqSWNnYAEOGSLn1UaOYWJGZeP7845OmhYbG7k5DRKSW3LmBzJlloOgvvwCXLwPNmiUtsYoycqR0R7pzJ7q6D6UpSW65mj59uv56mzZtkCdPHhw/fhz58+dH48aNjRpceqTVSgtD1NjImBRFfseDBslYBXYRJAB49EiKVCxbJtuPRgN06iTLeOKfiIgoiY4fB2bPBn79Vc5MWlhEV+Gxt/+053ZwAKZPl3/UkyfLXzc348RNZiFJLVcRERHo2rUr7kSNjAdQsWJFDBkyhImVkRw5ErvFKiZFAR48kPUofQsOlhYpb2/pxq3VAo0bA5cuAStWMLEiIiJKkqtXpatflSrSTShmy5K396cnVlE6dADKl5fr584Z5znJbCQpubK2tsamTZtMFQtBilckxs6dcjBN6U9oKPD990C+fFLt9f17oHJlSbi3bweKFVM7QiIiolTkwQOp9lS8OLBtm7RUde8OdOxomtezsABWrgRu3pRxV5SmJHnMVdOmTbF161YThEKAdMNNjFmz5OB60iTpFkZpn1YrLVIFCgDDhwMvXwJFisj/gaNHgapV1Y6QiIgoFdHpgBEjZI6SFSvkdrNmwJUrwNKlUknMVAoXTvxBH6UqSR5zlT9/fkycOBHHjh1DmTJlkPGDUfKc5+rT+PjIbzkwMO5xVxqNdP+1tpaiNWPHytiaxo2BXr2AunXlhAilHYoC/PmndAH8919ZljMnMHGinFTj2DtKVXS6xK0XHGzSMIiIYGEhhSXCwoBq1aSsesWKKR/H/v1yBrVu3ZR/bTK6JCdXy5YtQ6ZMmXD27FmcPXvW4D6NRsPk6hNZWkoX36gJXmMmWFFFaVatklbkjRuBJUuk1WLrVrl4egI9ekjrNsdHpn5Hj0phoWPH5HbmzJJk9e1rvK7fRClGUYA5cxK3bvfu8gPw8DBtTESUfkRGSgtV3brRc0xNmwZ06QLUr5+86n+fat06oH17iefaNf5zTwOS3MZx586deC+3b982RYzpTvPm0UVpYsqZU5Y3by7TInToIONsrlwBBgwAMmWSqRhGjwZy5ZIEzc8v8SeKyXxcuQJ88YW0ZB47Jvvab7+VsurDhnHfS6nUhQvA778nbt07d6SvK/+vENGnUhQpUFGsGNCzJzBuXPR93t5yxlqNxAqQ8s85cwL37iX+5BOZtSS3XFHKaN5cfm9HjkiRC3d3OdCOqwtY0aLS2jVtGrBhg7RmnTgBbNokFy8vac3q0gXIli3l3wslXlRXz19/lf8FlpZAt27yf8CkJ/Dv3094XiEXF5YfpE9XurTspK5eBRo2jH+99++lPHFAgCRYfn6yoyMiSqqDB6ULyKlTcjtrVqBMGVVDMpAhg3RH/PLL6FY0ttinaklOrj42UfDy5cuTHQwZsrQEatRI/PoZMsjxSKdOMr/dkiXAb7/J8cnIkcCYMTJOs1cvoGZN9U7SUGwvXshk7z/+KF2/AaBFC5nEvWBBE7/4/fvyIglN7GpnJ7MUM8Gi5IiMBKz+/++mRQu5fMyRI9J158oVGQtx9qz0eyYiSoyLF6XLx65dcjtDBmDoUOn+4eSkbmwfatcOWLhQzoyPGiWVBCnVSnK3wJcvXxpcnj59ir///hubN29GMAcgm43ixeV3+uiRTC5bvjwQEQH88QdQq5YcS3//fcKNFWR6b99KUpUvn/QGCAuThPrUKekCavLECpCNIKHECpD7ubFQcmzbBpQqJUl8Uri7A4cOyc7riy+Y2BNR0mzYIImVlRXQp4+caZ440fwSK0DOds+bJ9dXrQLOnFE1HPo0SW652rJlS6xlOp0OvXv3hpeXl1GCIuPJmFGKW3TtKsMdliwBVq8G/P2lnPfo0XISuVcvOTnM1qyUEREBLF8OjB8PPH4sy0qWlEnbfX35PVAaceAA0KaNnDVYsEDmkEiKLFmkipadHcugElHCnj2TbiCFCsnt4cNlXMW338q4KnNXvjzw1VfS5WjQICnow4OBVMko/60sLCwwZMgQzJ071xhPRyZSqhTw00+yr/n5Z+lyHB4uhWpq1JA5k+bOBf77T+1I0y5FkZNpRYsCX38tiVXevJLwnjsH1KvHfSmlEWfOSItTWBjQtKmMJUgOB4foLoWRkdJ9JrFFMYgo7XvzRlqk8uWT8UpRZZadnaXrTmpIrKJMmyYHCIMGqR0JfQKjnQoMCAhAZGSksZ6OTMjBQQpc/POPXHr0kBau69eBIUNkHOVXX8lJk7jm2qLk+ftvoEIFoHVraTl0dQXmz5fP/csveWKe0pCrV6Ws8Zs3wOefyxkcKyPUT1q5UhKr9u3lDBERpV/h4TL+wctLqj69eSPdQl68UDuy5MuRQwbNt2rFM62pWJL/2w0ZMsTgtqIoCAoKwl9//YVOnToZLTBKGWXKyDHK998Da9dKt8ELF6QlZfVqOYHSs6ckW5kzqx1t6nT+vBQU2btXbjs4yJjaoUMBR0d1YyMyurt3pRDFixfSzWXrVunWZwxdu0phi8WLpS/zq1fS9YeI0g+dDli/Hvjuu+ipGry9pQJUy5ap/0xlzKRKq427TDSZtSRvgefPnze4XLp0CQAwe/ZszIsajEepjpOTdFM7d06KKXTtKnMp/fsvMHCgnEzp3FkK2bA1K3ECAuQE+2efSWJlbQ307y/Lx49nYkVpVO/eQGCgnJnZudO4G7qFBbBokZytAIARI2TgKHdKROnH9u3yz/X2bSB7dhnvcPWqdAtJ7YlVFJ1Oznx7ewMPHqgdDSVRkluuDhw4YIo4yExoNHKyuXx5qV63erW0Zl2+LAVsVq0CSpSQ1qwOHaRLMxl68gSYPFlOrkf1lG3fHpg0SbqEp1qvX6sdAaUGK1bImZpFi2Q+GWPTaGRcgrOzDFSfOlVasObPTzsHVkRkKDgYyJRJrjduLPPf1asnY5MyZlQxMBPRaOQA7O5d2c+tXq12RJQESf5PdOfOHfj7+8da7u/vj7t37xojJjITzs5A374yVcTx4zJ/lp0dcOkS0K+fjM3q1g04fZonjgHJPcaNk+7fCxdKYuXrK62Ba9aYcWKV2APS3r0lcyT6UMwdgJubdAU09SSYI0fKGWuNRkpvXr9u2tcjopR386aMPypWDHj3TpZZWgKHD0urdVpMrADZr82dK3/XrAFOnlQ7IkqCJCdXnTt3xvHjx2MtP3XqFDp37myMmMjMaDRApUoylvzRI+CHH6Sy4Lt3ckxToYJ0fVu8OH02boSFyUlzLy8pWPT2LVCunFSQ3r0bKF1a7Qg/olQp+XLr15cKb2fPGl5+/x1wcQGuXQN8fJI+XxGlbWFhQKNGwK+/pvxrf/21nNHdvFl2SkSUNgQFye+7SBGZ9PHRI6kKFSU9FHsoU0bGYwAyPkOnUzUcSrxkjbmqUqVKrOUVK1bEhQsXjBETmbHMmYEBA4ArV4AjR6RroK2tFMHo3Vvm/ezZU47J0zqdTk4oFS4s+71nz4ACBaTU+qlTUiQt1ejUScbHlC0rmXLMS5s20nSZO7eUOaxaVc4mEmm1shPYuVOauZ8+TfkY2reX7kFRbtyQqmFElPq8eiUtUl5eMiZBq5WTNxcvyt/0ZupUqYJ1+rQccFCqkOTkSqPR4HUczROvXr2CVqs1SlBk/jQaOcb+7TcZuz5nDlCwoLTaLF0qx+jlysn1tHacoyjSIvXZZ3JceeeOJJWLF0vS2bJlKjmp9uaNnB1MjPz5pTZ/wYIyuNbHRzJqSr8URSr2bdwI2NgAW7YA2bKpG5O/P1C9OlCnDifsI0ptnj+XpGrqVOD9e+kyc/gw8OefQPHiakenDjc3STYB6Qr99q268VCiJDm5qlatGqZNm2aQSGm1WkybNg1Vq1Y1anCUOmTNCgweLL3GDh6UOT5tbGQOrZ49ZehF795p41g8qkWqfn05kebkJNVf/f3lONPaWu0Ik2DcOJnJfu3axK2fK5c0V5YuLS0UMbtoUPqiKFKpb9kyGbO3bh1Qu7baUcmg9/BwGZ9Qo4bM0k1EqYOLi5wcKVxYxm0eOyYn8tK7QYOAvHnlZGjUnC5k1pJcLXDGjBmoVq0aChYsCJ//b/RHjhxBSEgI/ubBVrqm0ch+sXp16SK3apVUEvX3l1adxYulCmGvXtLTLDWNQ71xAxg1SoZ2ANIVsl8/KeJjioJoJnfhggye02qBLFkS/zhXV0mq1q+XL5LSp2nTZHI8QJqnmzdXN54o5crJme46daTEqY8PsG8fkCeP2pERUUyKAuzYIQOVN22SbueA7E+cnIwz6XhaYWcnA9wdHKRbEJm9JLdcFSlSBJcuXULr1q3x9OlTvH79Gh07dsT169dRrFgxU8RIqZCrKzBsmBTw2r9fpp+wtpZuw926ybxZ/ftLNzpzFhgorW9Fi0piZWEh40tv3pRjy1SZWGm1khhptfLFxByvkhiZMhkmVq9fA35+Rg2RzNihQ9HdVGbPlknxzEmxYtKF1dMTuHVL+i+zkiCR+YhqkfriC+niMn169H1ZsjCxikuNGkysUpFkbcEeHh6YOnWqsWOhNMjCQrrRff65VPFeuVJas27flnLlCxcClSvLsXqrVjJxsTkIDgZmzJDGnffvZdkXX0hX8KJFVQ3t0y1eLFmukxPwqRN/h4bKB3P4sJxZ69TJKCGSGatWTZpxNRpgyBC1o4mbl5d0Ya1TRxKratWkzzIrChKp599/Zd+xfbvctrOTalDffKNuXKnNrVtSrrlECbUjoXgkueVqxYoV2LBhQ6zlGzZswKpVq4wSFKVN2bPLPtTfX7oNt2ghJ6ii5tDKkUO6Fl+9ql6MoaHSIpUvn5xMe/8eqFJFjtO2bUsDidWjR/LPDZBM0d39057Pykr6gut00qS3YMEnh0hmTqORgYaTJqkdScJy5pSk/7PP5AedK5faERGlX/37SzKwfbucde3eXZKE6dOlDDElTtS0E127sjS7GUtycjVt2jS4uLjEWp4tWza2ZlGiWFjICeWNG2XKpClTpAfPy5fSUlS0qPQYWL1akp2UoNUCK1ZIUbzhwyWWIkUkoTpyRHoWpQmDBwMhITI25euvP/35rKyAX36RrBiQOv2TJ3NW6bTmyBFpWo6axBNIHSUxo8YI7twJODqqHQ1R+uXoKMlAs2YyHmDpUjmjSklTtaq0+J09KwPbySwlObm6f/8+8ubNG2t5njx5cD8Zk4sGBgaiQ4cOyJo1K+zt7VG8eHH8888/+vs1Gk2cl1mzZsX7nOPHj4+1fqFChZIcG5meu7s0pNy6BezaBTRtKpOvHz0KfPWV7HuHDpWCEqagKJJAlSghJ4IePpQT3CtWAJcuSY+31HAMmSharXygNjYyf4ilpXGe18JCavGPHy+3x4yRSnJMsNKGc+dkfpmNG6WQRWrj7GxYtGXGDBlAT0Sm8e6d7CuOH49eNmIEcOKEtLwULqxebKldtmzyPxaQg6c4pkYi9SU5ucqWLRsuXboUa/nFixeRNYmj+1++fIkqVarA2toau3btwtWrVzF79mxkjtFEHBQUZHBZvnw5NBoNWrRokeBzFy1a1OBxR48eTVJslLIsLaWuwpYtwL17UkAoVy6ZqmbOHKkYXrMm8PvvQFiYcV7z6FE5CdS0qXRFzJJFugTevCk93IyVe5gNS0v5MO/dk3LqxqTRSGn3uXPl9vffSylFSt1u3JAfZkhI9Fir1GzXLpkrpnVrGQBKRMYTGSmDqr29ZV8xfHj0SbZMmYCKFVUNL80YMEDGlT5+LN37yewkuaBFu3btMGDAADg6OqJatWoAgEOHDmHgwIFo27Ztkp5rxowZyJUrF1asWKFf9mGrmJubm8Htbdu2oWbNmsiXL1+Cz21lZRXrsZQ65MghJ2ZGjZLJepcsAf76S8ajHzwoU2F06QL06CHd+OKi1QKHDmlw+HAOZMyoQc2a0cnSlSty3L9jh9y2t5fecsOHy/4/zTPl72LQIGkp+OYb4MsvTfc6ZHr370v/3WfPZNzSn3+aT8WZ5KpbV8Z6/PKL7ERevZIB9UQUt/v3ZXJfAIiMhHNAAHD+fHRFPxcXORO6ebP80755U5Z7esoEl4qShrp/mAlbW6nU2rSpnDDt2VPGPpPZSHJyNWnSJNy9exe1atWC1f9/XDqdDh07dsSUKVOS9Fzbt2+Hr68vWrVqhUOHDiFHjhzo06cPevToEef6T548wV9//ZWowhn+/v7w8PCAnZ0dKlWqhGnTpiF31DwKHwgLC0NYjOaQkJAQAEBERAQiIiKS9J7IuOrWlcuDB8CKFRZYscICgYEazJoFzJoF1KypQ/fuOjRposDGRh6zZYsGQ4ZYIjDQCkBZzJkD5Mih4NtvtTh1ygKrV2ugKBpYWiro2lWH0aN18PCQx6bJr/vff2E5ZAi0c+akTEWODh2kP6WTU/QHmg7+wUbtK9LEPuPpU1jVrg3NgwdQChRAZFRilRbe248/wsLBAZbz5gGDBkH733/QjR6dKrfPNLXNkfm5fx9WxYpB8//Bz9YAanywimJjA6VQIVj8v0eT4uIC3ahR0PXoIUmAVisXMq769WH5+eew+Ptv6IYOhXb9erUjMglz2sclJQaNoiRvYIS/vz8uXLigHyeVJxmTNNrZ2QEAhgwZglatWuHMmTMYOHAgFi9ejE5xlHSeOXMmpk+fjkePHukfG5ddu3bhzZs3KFiwIIKCgjBhwgQEBgbiypUrcIxjUPP48eMxYcKEWMvXrl2LDBkyJPl9kelotRqcPZsdu3d74vz5bFAUOSBydg5F7dr34eLyHkuWRJUnjXmwpBgsq1w5EF9+eR05crxJsdhVodOh6ujRyHrtGh5VrIgzI0emeAhZr1xB/s2b8c+wYYjk78n8KQqqfPcdXP79F+9cXXFk6lSEurqqHZVxKQoKbNiAwmvXAgBuffEF/u3SJVUmWESm4hwQgBpDhyZq3Ug7O9xq0gQBTZpwP59CHO/ehc+oUbjVrBlutmzJ/ZeJvXv3Du3bt8erV6/g5OSU4LrJTq5iCgkJwZo1a7Bs2TKDYhQfY2Njg7Jly+J4jEGPAwYMwJkzZ3DixIlY6xcqVAh16tTBgiSWew4ODkaePHkwZ84cdOvWLdb9cbVc5cqVC8+fP//oB0jquXcPWL7cAitXWiAo6MNEKu6djK2tAj8/LSpWTB/FFjQrVsCqVy8oGTMi8uJFIJ7WW5MJC4NVkSLQPHgA3WefQbtjh3QjSYMiIiLg5+eHOnXqwNraWu1wPonmzBlYduuGyI0bgQIF1A7HZCwWLoTl/+fqity7F0qNGuoGlERpaZsjM3T+PKwrVPjoatqePaEbM0bmW6GU9fp1mq6Eak77uJCQELi4uCQqufqkabAPHDiA5cuXY/PmzXB2dkazZs2S9Hh3d3cU+WBSx8KFC2NTHJWcjhw5ghs3bmB9Mpo+M2XKhAIFCuDWrVtx3m9rawtbW9tYy62trVX/Mil+3t4ylnPCBBk/NW0acOYMEF9iBQBhYRpotVZIF1/rs2f6ohKaCRNg7eWV8jFYW0s5xrp1YXHuHCxq1QL8/NJ0Cd40sd+oXBm4cgXWaa6qywcGD5ZKNk+ewKpOHbWjSbY0sc2R+bFK3CGiZa9esMyZ08TBUJxiVkJNw8xhH5eU109ytcDAwEBMmTIF3t7eaNWqFdauXYvly5cjMDAQP/74Y5Keq0qVKrjxQY3tmzdvxtnFcNmyZShTpgxKliyZ1JDx5s0bBAQEwP1TJ0wls2RtLVNnDB6cuPWDgkwbj9kYNkzKLZYsqe6g/dKlZZ6knDmBa9ekRGNAgHrxUGw6HdC3b9TZCZHWE6sonTpJmego//1nOJ8XEZG5O3BABqj/v2YAqSvRydWmTZvQoEEDFCxYEBcuXMDs2bPx6NEjWFhYoHjx4tAko6/n4MGDcfLkSUydOhW3bt3C2rVr8fPPP6Nv374G64WEhGDDhg3o3r17nM9Tq1YtLFy4UH972LBhOHToEO7evYvjx4+jWbNmsLS0RLt27ZIcI6Ueic2d00WOfeAA8Ouv0gd7yZJEn4E0mUKFpPa9tzdw964kWFeuqBsTCUUB+vUDFi2SsuuvXqkdkXpCQgBfX7mk58+BKDxc5j4h86fVSmVGPz9g8mS1oyEkIblq06YNSpcujaCgIGzYsAFNmjSBTVR5tmQqV64ctmzZgnXr1qFYsWKYNGkS5s2bhy8/KOH8+++/Q1GUeJOjgIAAPI8qFQrg4cOHaNeuHQoWLIjWrVsja9asOHnyJFzT2qBsMuDjI40j8eX5Go1UjPXxSdm4VPHTT/L366+BRPSZTxF58kgLVvHiMj/H/PlqR0QA8N13sr1oNJJgOTurHZF6AgIAf385EVCzpnStJUqPLlyQkrxk/iwtpTQ7AMybB8QzBIZSkJJIPXv2VJydnZXKlSsrP/30k/Lff/8piqIoVlZWyr///pvYp0kVXr16pQBQXr16pXYolESbNimKRiMXOSUvl6hlmzapHWEKiYhQlAULFOXlS7Ujie3FC0UZMUJRwsLUjsSowsPDla1btyrh4eFqh5J4s2ZF/0iWLFE7GvNw/ryiZMsmn0mhQory4IHaEcUrVW5zZJ50OkXx9zdc1qKF4T/S+C5nz6oTM0XT6RSlbl35Ppo2VTsaozGnfVxScoNEt1wtWbIEQUFB6NmzJ9atWwd3d3c0adIEiqJAp9OZLvsjSoLmzYGNG2PXS8iZU5Y3b65OXCnOykq6epnjrMhZsgAzZkA/MZlOB5w9q25M6dEvv8jM2QAwfbpMRElAqVLSwporF3D9unRh9fdXOyoi0zlzBqhVS3oVPHwYvXzOHCCBaW8AyP1ptAJsqqLRAHPnSivW1q3A33+rHVG6lqSCFvb29ujUqRMOHTqEy5cvo2jRosiePTuqVKmC9u3bY/PmzaaKkyjRmjeXYT1+fpEYMuQf+PlF4s6ddJBYKQqwciUQY1oBs6coUmyjQgXg/3MOUQrYtw/o1UuujxgBfPONuvGYmwIFpGtg/vwy54OPD3D5stpRERnXzZtAq1ZA+fIyTlenA2JMjYPcuYEbN+Tk19mziDh1Cgdnz0bEqVP6ZbhxI+Wn+KC4FSkiY68AYNAgIDJS1XDSsyRXC4ySP39+TJ06FQ8ePMDq1avx7t07Fowgs2FpCVSvrqBatUBUr66kj8Jna9YAXboAFSvKANfUQKeTwgFaLdChA7B4sdoRpQ+VK0vRhh49pNWKYsudW1qwSpaUs8L29mpHRGQcQUEyHrdIEenSodFI1cybN4HWrQ3XzZ0b+OwzuZQujVdeXlIBNmoZEyvzMn48kDmznAzaskXtaNKtTy4hZmFhgcaNG6Nx48Z4+vSpMWIioqT67z/g/5OholWr1FNG29JSWtucnIAff5Szbq9esSXF1DJkkK4jlpbxV4AhmRT1wAEpwOLtrXY0RJ/u/Xvp/vfihdxu1EgmjCxeXN24yDiyZgV++EHmqGnZUu1o0q1kt1zFJVu2bMZ8OiJKrJEjpbJZkSIyv1VqYmEBLFgAjBolt0eOlMmPFUXduNKaS5eAKVOiP1cbm9SThKspc2agcOHo2zt3ysTYRKlFRET0dXt7oHt3oFIl4PBh4M8/mVilNV99BbRtyxNnKjJqckVEKjh2DFi6VK4vXhxdKCI10WjkwH/GDLk9fXriZ4Wmj7t1SyaY/O47OatJyXPpEtCihVxWr1Y7GqKEabXSM8DLS/5PRJk4UW6ni3lJ0rmQEODRI7WjSHeYXBGlZhER0nceALp2Tf3/LEeMkEmPra1T/3sxF4GBQO3awJMnMn6oc2e1I0q9ihSRMSlarZwdXrRI7YiIYlMUYPt2+b136QI8eCDzH0WxsWGrRnqwd68U5YkqXkQphskVUWo2dy5w5YqUwp05U+1ojKNnTyl93aKF2pGkfs+fA3XqSMU7b29gzx7zLM+fWlhZAStWAP37y+2+fYFp09SNiSimo0flxFSTJsC//0q31lmzgF9/VTsySml58sh47B07JNGiFJPk5Cpfvnx4ETUQMobg4GDky5fPKEERUSK1aCFV377/XgayphV58kRfv38f6NgReP1avXhSo9evgQYNgGvXZOK3ffukQAN9GgsL6Vr53Xdye9QoGSfIMYKktj59JLE6dkzGVo0cCdy+LeNwWe0y/SlYUOa7BKSbPUuzp5gkJ1d3796FNo4yz2FhYQgMDDRKUESUSF5ewK5dknykRYoiCeRvv0kLzH//qR1R6qDTAU2byuSgWbMCfn6GCSt9Go0GmDRJWgQAGSu4Zo26MRGVKydFanr2lHGW06axpTq9GztW/gdcvSpd7ilFJLoU+/bt2/XX9+zZA2dnZ/1trVaL/fv3w9PT06jBEVE8QkKkfDmQtvvOazQyrqVePeDUKaB6dene4O6udmTmzcJCEu5z54Dduw2r3ZHxDBsGODsDf/8NcJ5HSkkvXkjyVKqUzBEIyG++alUZZ0MESLfQSZOkVXPsWNlPZcmidlRpXqKTq6ZNmwIANBoNOnXqZHCftbU1PD09MXv2bKMGR0RxCAkBihaVhGP27OgkK60qV05KBtepI+PLfHykixtP5iSsUycZd8Ez16bVo4eUto46yREZKRc7O3XjorTp3TvpljpjhswJmCuXzG1oayutVkys6EM9eshJyitXgAkTWDE2BSS6W6BOp4NOp0Pu3Lnx9OlT/W2dToewsDDcuHEDjRo1MmWsRAQAY8YADx8CBw+mzrLryVG0qAzUzpsXCAiQs7PXrqkdlXlRFBl79+RJ9DImVikjKrHS6STRql+fYwTJuCIjgZ9/lsI0o0ZJYlWihHT1Si//Byh5rKyk+BUAvHnD8aEpIMljru7cuQMXFxeDZcHBwcaKh4gS8s8/MuEuAPz0U/o6O54vH3DkiJTDDgyMHqhLYsIEYPhwoFo1IDRU7WjSp1u3gM2b5cRHrVrSdYvoUx08KCeYevUCgoKk1f6334Dz5yWRT8tdw8k4atcGrl8Hli3j9pICkpxczZgxA+vXr9ffbtWqFbJkyYIcOXLg4sWLRg2OiGKIjJR/rooCtG8vO8v0JkcO4NAhoE0bFhCI6YcfJLkCpEx4ekq6zUmBAsCBAzKA/MwZGSMYFKR2VJTaWVsDN2/KlBs//CAHyR06yNhKosQqWFDtCNKNJP8yFy9ejFy5cgEA/Pz8sG/fPuzevRv169fH8OHDjR4gEf3fjz9KgYJMmYA5c9SORj0uLsDvvwNubtHLHjxQLx61rVoFDBok1ydOZIue2sqUkTGCHh4yz1DVqsCdO2pHRanJ+fPyu45SpQqwerV0iR4wQMZXESXX7duyHUVEqB1JmpXk5Orx48f65GrHjh1o3bo16tatixEjRuDMmTNGD5CIIGOsoubVmTGD8xXF9OuvMg5hwwa1I0l5W7cC3brJ9cGDo7cRUleRIjJGMF8+OZCpWlVKIRMl5PZt6ZXw2WfA119L9+coX36Z9osXkelFRgI1asjwgkWL1I4mzUpycpU5c2Y8+P9Z4t27d6P2/7smKYoS5/xXRGQE/v7S1atyZRkwT0JRZA6n8HCgbVtg+XK1I0o5Bw9K90itFujcWYpZsC+9+cibVxKsokWBZ88MD5SJYnr6VFoSChUC1q2TZc2asfAAGZ+VlRTFAoDx44Hnz1UNJ61KcnLVvHlztG/fHnXq1MGLFy9Qv359AMD58+fh7e1t9ACJCEDNmtLP/rff2M8+Jo0GWLlSSs3qdNKKE1UVKa3Lm1cmBm7WDFi6lNuFOXJ3lzGCf/0lUwkQxfTmjYyV9PKSloSICKBuXeDsWWDtWiBnTrUjpLSoa1egZEkgOBgYN07taNKkJP83njt3Lvr164ciRYrAz88PDg4OAICgoCD06dPH6AES0f9lzSrdjMiQpaWUIx42TG4PGSJn5NL6Wd88eYBjx+QgzCrRUxZSSsua1TCxunlTJnYmCg4Gpk+XJKtsWZm/b88e6RZIZCqWlsC8eXJ98WKZ/4qMKsn/ka2trTEs6iAmhsGDBxslICKKYeZMaaFo2ZJdvhKi0chnlSmTjDuaMEEOXObOTVuf2507wOXLwBdfyG1XV3XjoaQJCpIqn0FBUqCgTRu1I6KUpNNJsZMaNeR2zpySXHl4cB9PKatGDaB5c5k6YsgQSeq5/RlNsvqR/Pbbb6hatSo8PDxw7949AMC8efOwbds2owZHlK5duiSTRbZuDZw8qXY05k+jAUaPjp4HzMEhbf2ziDowb9YM2LRJ7WgoOVxcpLhFZCTQrh3wyy9qR0QpQVGktbJMGeniffx49H0DBwKtWqWtfRWlDrNmyQTUfn7Arl1qR5OmJDm5+umnnzBkyBDUr18fwcHB+iIWmTJlwryoZkYi+jQ6ncxppdUCLVoAlSqpHVHq0a+fJKOTJqkdifH89x/g6yvVxDw9uT2kVtbWMm4yar66Hj2A2bPVjopM6fRp4PPPZbLfCxek4t/du2pHRSTDDKJ6ekS1ppJRJDm5WrBgAZYuXYrRo0fD0tJSv7xs2bK4fPmyUYMjSreWLpUEwdFRJo2kpKlQIfpM8Lt3wLffAm/fqhtTcr19CzRsKN0B3d3lLKOHh9pRUXJZWgI//QSMGCG3hw2T6l1pfYxgenPjhnT1q1BBKnva2ABDh0aXWycyB2PGAGPHAhkyqB1JmpLk5OrOnTsoXbp0rOW2trZ4m1oPXojMyePHwDffyPXJk4EcOdSNJ7Xr3FnGNfj6yjis1CQsTLoBnjwJZM4M7N3LoiZpgUYj89VNmya3J08G5s9XNyYyHq0WqFdPuu9qNLIP8veX6RKyZlU7OqK4abWp9ySkmUlycpU3b15cuHAh1vLdu3ejcOHCxoiJKH0bMgR49Ur65/ftq3Y0qd/gwVLo4tgxGe/w9KnaESWOVisTh/r5ARkzAjt3AsWKqR0VGdPIkcCPPwKlSgEdO6odDX2KV6/kNwtI6+SYMUDjxjJ2dsUKIHdudeMjSsjp00C5cnL8QZ8s0cnVxIkT8e7dOwwZMgR9+/bF+vXroSgKTp8+jSlTpuDbb7/FiKhuDkSUPJcvyySSFhZSXjxG11tKpkqVpFtOtmwy5qFaNeD/E6GbNY1GugHa2ABbtwIVK6odEZlCnz7AqVPSMhkl6iCdzF9oqIyby5dPKkBG6dIF2L6dJ0QodQgLA86flyI7Fy+qHU2ql+jkasKECXjz5g26d++OGTNm4LvvvsO7d+/Qvn17/PTTT/jhhx/Qtm1bU8ZKlPYVLy5znUybJi1XZBwlSwJHj8rZ4xs3pGKbv7/aUSXMwkK6ip0/L1UCKe2ysYm+Pn++FD9g9xzzptVKi1SBAjJu7r//gN9/j76f1f8oNfHxkcrEOp309uAY0E+S6ORKifFBf/nll/D398ebN2/w+PFjPHz4EN26dTNJgETpTq1a0YPdyXjy5weOHJGDofv3gaZNzbOFYMsWIDxcrms0QJEi6sZDKefpU6ne5ecnEw+/fKl2RPQhRZEWqZIlga5dpRU8Z05g+XJgxw61oyNKvhkzAFtb4MAB6S1ByZakMVeaD87EZMiQAdmyZTNqQETp0u3bwMOHakeR9uXOLQlWtWrAypXm1+3yp59kYscmTWQuJEpfsmWTxCpzZuDECSmP/OSJ2lFRTAMHyu/z33/le5o1C7h5U7oBmtv+hCgpPD2lFRaQv2FhqoaTmiUpuSpQoACyZMmS4IWIkkhR5Axo4cJyRpRMK1s2GYNVrlz0sjdvVAtHb+3a6AIm5coBVlbqxkPqqFABOHQIyJ5diiH4+AD37qkdVfoWs4tU69aAvb0UI7l9Ww5C7e3Vi43ImEaOlLG+t29zGphPkKT/3hMmTICzs7OpYiFKn1atkoMpe3ugRAm1o0kfYrbCnz4t80gtXSpdBdXw119Ap05yENevn0zqSOlX8eIyRrB2bRkb6OMjLVoFC6odWfpy/77MAZQrV/Sk5FWrSldAllSntMjBQaYu6dQJ2L8fGD6c4weTIUnJVdu2bdkNkMiYnj+PboYfP16a5SllLV0q30PLljJA/auvUvb1Dx+W146MBDp0kLOF/GdG3t6SYNWpA1y/LnOcMblKGS9eAFOnSpn8sDCZYHXIkOiKjkysKC3r0EG29UaN+L8omRKdXH043oqIjGDECPlHXry4VOihlPfTT5LYrFwpcw29eiWtRynh3DmZCyc0VP4uXy5VAokAKZRw+DCwcSPQu7csu39fTgYAQGQknAMCpKJkVDdSFxfOqRSXmJ9bXFxcJGn64QcZ2B8SIstr1JDbMUvlE6VlFhby/4iSLdHJlcKyjETGdfiwtJQAwOLFgLW1uvGkV1ZWwLJlgLOzHFj17y8J1qhRpj9r9+aNdAWsXh1Yv57bAMXm6mqYWBUooB9obg2gxofr29nJdANMsKLdvy+tfqGh8a9jYyP7gGfP5HbJktI9yteXZ+8p/Xr9WuZv+/pr/g6SINHJlU6nM2UcROlLeLjsrACgZ0+gcmV140nvLCyAuXOBTJlkvNN33wHBwcDMmab9h1KtmnT98vTkoHj6uAcPPl7BKzRUWmiYXEV7/jzhxAqQfXJwMJA3LzB5MtC2LVuRKX2LiABKlZLiFi4uQKtWakeUanDPQaQGnQ744gsgRw45O0rq02hk3NucOXL76lXTlEN/+lTKOEcpUQJwcjL+61DawwqSpvXTTzK+rX17JlZE1tbRY5CHDwfev1c3nlSEe2oiNdjZSVI1dqwMlibzMXiwFBOoXdv43fRevZJuRvfuAbt3A+XLG/f5KW1LyvY4ZAiwb1/8958+LfshABg9Gvjzz/jXPXQoeszR5MnAH3/Ev+7u3YCHh1yfPVuqocZn61YgXz65vmiRdI+Oz7p1QNGicn35cmDevPjXXb4cKFs2+nFjx8a/bkylS0v3QCISI0ZIt/l796R3x6hRakeUKjC5IkpJiiKXqLOiTKzMU8zBvIoC/PyzFLv4lK57795J9aULF2SuLQ6QJ1O6fx+4fDn++2OOo374MOF1tdro64GBCa8bERF9PSgo4XVjdnF88iThdWOeNX/2LOF1376Nvv7iBXDrVvzrElH8MmSQE8EdOkgFzc6do0+eULyYXBGlpN9/B+bPlzO0JUuqHQ0lxoQJclm7Vs7uJ6cLX3i4lFs/elQGze/ZA+TPb/xYiaKMHRs9rjMuMVtoRoxIeAqCmNv8gAFAixbxrxtzupaePYF69eJfN+a4sI4dZT6v+BQoEH29TRugTJn41405X2CTJoClJdCnT/zrE1H82rcHFi4ETp6UlquVK9WOyOwxuSJKKcHB0uXsyRNg+3YmV6lFrVrSHeLwYbm+a5cM7k0srVYOHHftkpavv/6SQcJEppSUCcmLFo3ucvcxhQvLJTEKFDBMihLi5SWXxPD0TPycgLlyARUqJG5dIopNo5FKuhUqSDffvn2BcuXUjsqsccQmUUr59ltJrAoWlDPFlDr4+AAHDkhC9c8/UjY9MDBxj1UU+UcUVWZ982agShXTxktERGRM5cvLScIvvwTc3dWOxuwxuSJKCSdPAkuWyPXFiwFbW3XjoaT57DNpucqRQ6oI+vhIedqPCQ2V6mMajcwVklAXKaKPcXGJLkIRHzu7pLWspgf83Ig+3fLl8n8sZ061IzF77BZIZGoREUCvXtKK0akTUKOG2hFRchQuLGOmatcGAgLke7xxI+EiF/b20h3w0CEmVvTpcueWbe75cwBARGQkjh09iipVq8I6qky7iwvnuPrQB59bnPi5ESXM0tLwtqJwYuF4MLkiMrUffgAuXQKyZAG+/17taOhTeHoCR45IojR0qFQtizpgi4yEc0AAcP48cOeOFKyIOmBjYkXGkjt3dBIQEYFXQUFSQtzY0wakNTE/NyJKvrt3Zd6rEiWAMWPUjsYsMbkiMiVFAXbskOuzZrHbSVrg7i5jr4KCZPxcaCgAwBpAjQ/XtbOTM+Y8qCMiorTg9Glg40Zg506gSxd2E4wDx1wRmZJGA+zfLyXYO3dWOxoyFmtrabH6f2IVr9DQhLsiERERpSatWgFVq8rcjd9+q3Y0ZonJFZGpWVrKvCwW/LkRERFRKqbRAPPmRRdqOnlS7YjMDo/2iEzh9WuZzfz9e7UjISIiIjKeMmWie+MMGgTodGpGY3aYXBGZwrhxwOjRQJMmakdCREREZFxTpgAODsCpU8DatWpHY1aYXBEZ27lzUiEQkIpyRERERGmJuzswapRcX7hQCngRAFYLJDIurVbmtNLpZJyVr6/aEREREREZ3+DBgJUV0KcP57yKQfWWq8DAQHTo0AFZs2aFvb09ihcvjn/++Ud/f+fOnaHRaAwu9RIxZ8yPP/4IT09P2NnZoUKFCjh9+rQp3waR+OknKdPt5ATMnat2NERERESmYWcnc15lzKh2JGZF1eTq5cuXqFKlCqytrbFr1y5cvXoVs2fPRubMmQ3Wq1evHoKCgvSXdevWJfi869evx5AhQzBu3DicO3cOJUuWhK+vL54+fWrKt0Pp3aNH0U3k06dLk/n/2rvz8KjKQ4/jv8meQIiQQBYMEDc2WQ1QDPcWZYkLKDyAosFGpHCVIEuKCtSwiSg8l0VEiKAiCpRaucHUFhCCUlmNIiotsillCQlEIIEgyZDM/eNoNCVBkMm8k5nv53ny9MyZM5PfdF7j/Dxn3heeKyLC+hfL5QQFsbYZAMDzlZVJH35oOoVbMHpZ4IwZMxQbG6slS5aU74uLi7vkuMDAQEVFRV3x886ePVtDhw7V4MGDJUnp6en629/+pjfeeEPjxo279uBAZZ55xpolsFMn69JAeLZGjawFgn9Yx8p+8aK2bN6shC5d5O/3w5/WiAgWEAYAeLaSEmvtq+xs6eOPrW0vZrRcZWZmKjExUQMGDNCmTZvUsGFDDR8+XEOHDq1w3EcffaQGDRqobt26uvPOOzVt2jSFh4dX+pwlJSX67LPPNP5nC5v5+Pioe/fu2rZtW6WPKS4uVnFxcfntwsJCSZLdbpfdbr/WlwlDfnzvXPYeTpok34IClU6caH33qrTUNb8X5kRHl5+htNvtKjh+XPZbb7UWGf4Rf0NQTVz+Nw5ejfGGKtls8m3dWj7Z2SobNUqlW7c6ZW1PdxpzV5PB5nCYm94j6IdLalJTUzVgwABlZ2dr1KhRSk9PV3JysiRp5cqVCgkJUVxcnA4ePKgJEyaodu3a2rZtm3x9fS95zpycHDVs2FBbt25V586dy/c//fTT2rRpk3bs2HHJYyZPnqwpU6Zcsn/FihUKCQlx1ssFAAAAPE7AmTPq/sQT8v/+e+188kkd6dbNdCSnOn/+vB5++GEVFBSoTp06lz3WaLkKCAhQfHy8tm7dWr5v5MiRys7OrvIs0zfffKMbb7xRGzZsULdK3rhfU64qO3MVGxur/Pz8X/w/EO7Lbrdr/fr16tGjh/x/fibB2Q4f5tIvuG68AT9gzMGVGG/4JT6zZsl3/Hg5oqJ08Z//lEJDr+n53GnMFRYWKiIi4orKldHLAqOjo9WiRYsK+5o3b65Vq1ZV+ZgbbrhBEREROnDgQKXlKiIiQr6+vsrLy6uwPy8vr8rvbQUGBiowMPCS/f7+/sbfTFy7an0fd++W2reXHn5YWrRICgiont+DGoO/G3A1xhxcifGGKo0ZI732mmwHD8p/1ixroWEncIcxdzW/3+hsgQkJCdq7d2+Fffv27VPjxo2rfMzRo0f13XffKbqKmdgCAgJ02223KSsrq3xfWVmZsrKyKpzJAq5ZWZn0+OPWd2oKCihWAADAewUGSrNmWduzZknffms2jyFGy9WYMWO0fft2TZ8+XQcOHNCKFSu0aNEipaSkSJLOnTunp556Stu3b9ehQ4eUlZWl+++/XzfddJMSf7Y4a7du3TR//vzy26mpqVq8eLGWLl2qPXv26IknnlBRUVH57IGAU7zxhrRli1S7tjRvnuk0AAAAZt13n9Stm9SmjXT+vOk0Rhi9LLBDhw7KyMjQ+PHjNXXqVMXFxWnu3LlKSkqSJPn6+urLL7/U0qVLdebMGcXExKhnz5567rnnKlzGd/DgQeX/MB2yJD344IM6efKkJk6cqNzcXLVt21Zr165VZGSky18jPNSJE9LTT1vbU6dKsbFm8wAAAJhms0nvvCNdd51TZgysiYyWK0nq1auXevXqVel9wcHBWrdu3S8+x6FDhy7ZN2LECI0YMeJa4wGVGztWOn1aattWevJJ02kAAADcQ716phMY5Z2VErgWGzdKb79t/deZV1+V/Iz/NwoAAAD3cu6c9Mc/SkuXmk7iUnwqBK7W999LkZFS//5Sx46m0wAAALifpUul6dOlBg2kvn0lL1neiDNXwNW6917p66+tPxgAAAC41NCh0i23WN9Td9K07DUB5Qr4Na67zmv+CwwAAMBVCwiQZs+2tufOlQ4eNBrHVShXwJVwOKyFglessLYBAABweffcI/XsKZWUSE89ZTqNS1CugCuxbJn0pz9JQ4ZIOTmm0wAAALg/m02aM0fy9ZUyMqQPPzSdqNpRroBfcuqU9Ic/WNsTJ0oNG5rNAwAAUFO0aCE98YS1PWGC2SwuwGyBwC955hnp5Enrj8OPJQsAAABXZsoUa7bliRNNJ6l2lCvgcjZvll57zdp+9VXry5kAAAC4cvXq/fR5ysNxWSBQlZIS6fHHre0hQ6QuXczmAQAA8ASHD5tOUG0oV0BV1q+X/vlPKSJCmjHDdBoAAICarbhYevBB6aabpH37TKepFlwWCFTl3nuljz6SCgqk8HDTaQAAAGq2wEDp3DnJbre+x/7Xv5pO5HScuQIu57e/le67z3QKAAAAzzBrluTnJ73/vvTBB6bTOB3lCvhPH38sHTpkOgUAAIDnadZMSkmxtlNTpYsXzeZxMsoV8HMFBda1wC1bWjMFAgAAwLkmTbK+cvHPf1qzMXsQyhXwc88+Kx0/bi0UHB9vOg0AAIDnqVtXmjrV2p44UTp1ymweJ6JcAT/KzpZeecXaXrBACgoymwcAAMBTDRsm3Xqr9Xlr/37TaZyG2QIBybre93/+R3I4pEGDpO7dTScCAADwXH5+0qpVUkyMVLu26TROQ7kCJGn+fOnzz63T1LNmmU4DAADg+W65xXQCp+OyQODoUSktzdqeMUNq0MBsHgAAAG9SViYtXSplZZlOcs04cwWEh0ujR0tbtkhDhphOAwAA4F1eesmalr1ZM+nLLyV/f9OJfjXOXAHBwdJzz0kbNkg+/CMBAADgUo89JtWvL339tTWpWA3GJ0l4rwsXpNLSn25TrAAAAFwvLEyaNs3anjxZ+u47o3GuBZ8m4b3S0qROnayJLAAAAGDOkCFS8+bSmTPS8OHS558r7OBB63Pazp3Wz+HDplP+Ir5zBe/0xRfSnDnWmavjx6V27UwnAgAA8F7HjkkHD1rb77wj/3feUdf/PCYoSNq7V2rUyMXhrhxnruB9SkutNa1KS6X+/aV77jGdCAAAwLvl50slJZc/5sIF6zg3RrmC91m0SNqxQwoNtWanAQAAAJyAcgXvkpsrjR9vbT//vLUqOAAAAOAElCt4lzFjpIICKT7e+rIkAAAA4CSUK3iP8+elQ4esKddffVXy9TWdCAAAAB6E2QLhPUJCpC1bpO3bpfbtTacBAACAh+HMFbyLj490++2mUwAAAMADUa7g+fbskZ55RioqMp0EAAAAlYmIsNaxupygIOs4N8ZlgfAshw//tP7BxYsKO3BAvlOmWKt779tnTb3uxgvPAQAAeKVGjawFgn/4HGe/eFFbNm9WQpcu8vf7obJERLj95zjKFTzH4cNS06bWAnOS/KWKK3uvXi2tXev2K3sDAAB4pUaNfvqMZrer4PhxqV07yd/fbK6rwGWB8Bz5+eXFqko1YGVvAAAA1EyUKwAAAABwAsoVAAAAADgB5QoAAAAAnIByBQAAAABOQLkCAAAAACegXMFzpKebTgAAAAAvRrmCZ3jrLWnx4l8+rgas7A0AAICaiUWEUfNt2yYNHWptjxghDR4sqeau7A0AAICaiXKFmu3wYalPH6mkxPrfl16SfH44IVtDV/YGAABAzcRlgai5ioqk+++XTpyQWreW3n77p2IFAAAAuBifRFEzlZVJycnSrl1S/fpSZqZUu7bpVAAAAPBilCvUXDfdJAUESBkZUuPGptMAAADAy1GuUDP5+Egvvijt2SMlJJhOAwAAAFCuUMPs3y8VF/90+4YbzGUBAAAAfoZyhZojJ0fq2lW64w5rEgsAAADAjTAVO2qG77+3plrPyZHCwqTAQNOJAAAAgAo4cwX353BIv/+9lJ0t1atnzQwYFmY6FQAAAFCB8XJ17NgxDRo0SOHh4QoODlarVq306aefSpLsdrueeeYZtWrVSrVq1VJMTIx+97vfKScn57LPOXnyZNlstgo/zZo1c8XLQXV48UVpxQrJz096911rlkAAAADAzRi9LPD06dNKSEjQHXfcoTVr1qh+/frav3+/6tatK0k6f/68du7cqbS0NLVp00anT5/WqFGjdN9995UXsKq0bNlSGzZsKL/t58cVkDXSe+9JEyZY2y+/bH3fCgAAAHBDRhvHjBkzFBsbqyVLlpTvi4uLK98OCwvT+vXrKzxm/vz56tixow4fPqxGjRpV+dx+fn6Kiopyfmi4TnGx9OST1nZKivT442bzAAAAAJdhtFxlZmYqMTFRAwYM0KZNm9SwYUMNHz5cQ4cOrfIxBQUFstlsuu666y773Pv371dMTIyCgoLUuXNnvfDCC1WWseLiYhX/bHrvwsJCSdZliXa7/epfGJzDx0dau1Y+c+eqbOZM6Srfix/fO95DuALjDa7GmIMrMd7gau405q4mg83hcDiqMctlBQUFSZJSU1M1YMAAZWdna9SoUUpPT1dycvIlx1+4cEEJCQlq1qyZli9fXuXzrlmzRufOnVPTpk11/PhxTZkyRceOHdPu3bsVGhp6yfGTJ0/WlClTLtm/YsUKhYSEXMMrBAAAAFCTnT9/Xg8//LAKCgpUp06dyx5rtFwFBAQoPj5eW7duLd83cuRIZWdna9u2bRWOtdvt6tevn44ePaqPPvroF1/Yz505c0aNGzfW7NmzNWTIkEvur+zMVWxsrPLz86/q98AJHA75PPWUHImJcvTocU1PZbfbtX79evXo0UP+/v5OCghUjvEGV2PMwZUYb3A1dxpzhYWFioiIuKJyZfSywOjoaLVo0aLCvubNm2vVqlUV9tntdj3wwAP697//rY0bN1514bnuuut0yy236MCBA5XeHxgYqMBK1k3y9/c3/mZ6nTlzpHnzpPR06eBB6frrr/kpeR/hSow3uBpjDq7EeIOrucOYu5rfb3Qq9oSEBO3du7fCvn379qlx48blt38sVvv379eGDRsUHh5+1b/n3LlzOnjwoKKjo685M6rRmjXS2LHW9syZTilWAAAAgKsYLVdjxozR9u3bNX36dB04cEArVqzQokWLlJKSIskqVv3799enn36q5cuXq7S0VLm5ucrNzVVJSUn583Tr1k3z588vvz127Fht2rRJhw4d0tatW9W3b1/5+vrqoYcecvlrxBXas0caOFAqK7MWDB450nQiAAAA4KoYvSywQ4cOysjI0Pjx4zV16lTFxcVp7ty5SkpKkmQtMJyZmSlJatu2bYXHfvjhh+ratask6eDBg8rPzy+/7+jRo3rooYf03XffqX79+urSpYu2b9+u+vXru+R14Sp9953Uu7dUWCj9139Jr7wi2WymUwEAAABXxfjKur169VKvXr0qva9Jkya6kvk2Dh06VOH2ypUrnRENrmC3SwMGWN+vatJEWrVKCggwnQoAAAC4akYvCwTkcEg33CDVri1lZkqcXQQAAEANRbmCWQEB0uLF0q5dUqtWptMAAAAAvxrlCmbs3SuVllrbNpt0441m8wAAAADXiHIF19u/X+rcWbrvPunsWdNpAAAAAKegXMG1zpyxZgY8fVo6dUpiIUIAAAB4CMoVXOfiRWstq717rQWCMzKkoCDTqQAAAACnoFzBdZ5+Wlq3TgoJsWYGjIoynQgAAABwGsoVXOP116U5c6ztpUuldu3M5gEAAACcjHKF6ldQII0da21PmSL17282DwAAAFAN/EwHgBcIC5M2bpTefFNKSzOdBgAAAKgWlCu4Rrt2XAoIAAAAj8ZlgageZWXSsGHSjh2mkwAAAAAuQblC9fjjH6XFi6XEROs7VwAAAICHo1zB+ZYtk1580dp+5RXrO1cAAACAh6Ncwbm2b5d+/3tre/x4KSnJbB4AAADARShXcJ4jR6Q+faTiYun++6Vp00wnAgAAAFyGcgXnKCqyClVentSqlfT225IPwwsAAADeg0+/cA6HQ2rcWKpfX8rMlEJDTScCAAAAXIp1ruActWtLq1ZJhw5JTZqYTgMAAAC4HGeucG327LHOWknWZYA33GA2DwAAAGAI5Qq/3s6d0m23SYMGWZNYAAAAAF6McoVf5/hxawKL77+XTp2S/LjCFAAAAN6NcoWrd+GC1LevdPSo1KyZtHKl5OtrOhUAAABgFOUKV8fhkIYOlXbskOrWlf76VykszHQqAAAAwDjKFa7OzJnSsmXWmap335Vuusl0IgAAAMAtUK5w5Y4ckSZOtLZfflm6806zeQAAAAA3wiwEuHKxsdK6ddIHH0hPPGE6DQAAAOBWKFe4Ol27Wj8AAAAAKuCyQFxeSYmUnGwtFgwAAACgSpQrVM3hkIYPl956S7rrLqtoAQAAAKgU5QpVmzdPev11ycdHevVVKSDAdCIAAADAbVGuULl166TUVGv7f//XOnMFAAAAoEqUK1zq66+lBx+Uysqkxx6TRo82nQgAAABwe5QrVHTqlNS7t1RQIHXpIi1YINlsplMBAAAAbo+p2FGRwyE1bCjZ7dKqVVJgoOlEAAAAQI1AuUJF4eHWIsE5OVKDBqbTAAAAADUGlwXCsnfvT9sBAVKTJsaiAAAAADUR5QrSxo3SrbdaE1eUlppOAwAAANRIlCtvd+CA1L+/dPGilJ9vrWkFAAAA4KrxSdqbFRRYMwOePi117Ci99hozAwIAAAC/EuXKW5WWSgMHWmtaNWworV4tBQWZTgUAAADUWJQrb/X009LatVJwsPTee1J0tOlEAAAAQI1GufJGX34pzZ5tbS9dKt12m9k8AAAAgAdgnStv1Lq19H//Z10SOGCA6TQAAACAR6Bceau+fU0nAAAAADwKlwV6i3PnpEcekQ4fNp0EAAAA8EiUK29QViYNGiQtWybdd591GwAAAIBTUa68QVqaNSNgYKCUns5CwQAAAEA14FO2p1uxQpo+3dp+7TXpN78xmwcAAADwUJQrT7Zjh/TYY9b2uHHWpYEAAAAAqgXlylMdPSr16SMVF0u9e0vPP286EQAAAODRmIrdUzkcUlSUFBEhLV/O96wAAACAamb8E/exY8c0aNAghYeHKzg4WK1atdKnn35afr/D4dDEiRMVHR2t4OBgde/eXfv37//F533llVfUpEkTBQUFqVOnTvrkk0+q82W4n9hYafNmae1aKTTUdBoAAADA4xktV6dPn1ZCQoL8/f21Zs0a/etf/9KsWbNUt27d8mNmzpypefPmKT09XTt27FCtWrWUmJioCxcuVPm8f/7zn5WamqpJkyZp586datOmjRITE3XixAlXvCyz9u79abtWLalhQ3NZAAAAAC9itFzNmDFDsbGxWrJkiTp27Ki4uDj17NlTN954oyTrrNXcuXP17LPP6v7771fr1q311ltvKScnR6tXr67yeWfPnq2hQ4dq8ODBatGihdLT0xUSEqI33njDRa/MkHfflZo3l6ZNsy4LBAAAAOAyRr9zlZmZqcTERA0YMECbNm1Sw4YNNXz4cA0dOlSS9O233yo3N1fdu3cvf0xYWJg6deqkbdu2aeDAgZc8Z0lJiT777DONHz++fJ+Pj4+6d++ubdu2VZqjuLhYxcXF5bcLCwslSXa7XXa73Smvtdp9/rn8fvc72RwOlZ44obKLF00nMu7H967GvIeo0RhvcDXGHFyJ8QZXc6cxdzUZjJarb775RgsXLlRqaqomTJig7OxsjRw5UgEBAUpOTlZubq4kKTIyssLjIiMjy+/7T/n5+SotLa30MV9//XWlj3nhhRc0ZcqUS/Z/8MEHCgkJ+TUvzaUCT5/Wb8eOlf/33yuvXTvt6NpVjr//3XQst7F+/XrTEeBFGG9wNcYcXInxBldzhzF3/vz5Kz7WaLkqKytTfHy8pv+wyG27du20e/dupaenKzk52WU5xo8fr9TU1PLbhYWFio2NVc+ePVWnTh2X5fhVLlyQb48e8vnuOzluuUX11q3T3dddZzqVW7Db7Vq/fr169Oghf39/03Hg4RhvcDXGHFyJ8QZXc6cx9+NVbVfCaLmKjo5WixYtKuxr3ry5Vq1aJUmKioqSJOXl5Sk6Orr8mLy8PLVt27bS54yIiJCvr6/y8vIq7M/Lyyt/vv8UGBiowMDAS/b7+/sbfzMvy+GQUlKsxYLr1pXt/fflX7++6VRux+3fR3gUxhtcjTEHV2K8wdXcYcxdze83OqFFQkKC9v58djtJ+/btU+PGjSVJcXFxioqKUlZWVvn9hYWF2rFjhzp37lzpcwYEBOi2226r8JiysjJlZWVV+Zgaa+NG6e23JV9f6Z13pJtvNp0IAAAA8FpGz1yNGTNGt99+u6ZPn64HHnhAn3zyiRYtWqRFixZJkmw2m0aPHq1p06bp5ptvVlxcnNLS0hQTE6M+ffqUP0+3bt3Ut29fjRgxQpKUmpqq5ORkxcfHq2PHjpo7d66Kioo0ePBgEy+z+nTrJr32mlRSIv1s0g8AAAAArme0XHXo0EEZGRkaP368pk6dqri4OM2dO1dJSUnlxzz99NMqKirSsGHDdObMGXXp0kVr165VUFBQ+TEHDx5Ufn5++e0HH3xQJ0+e1MSJE5Wbm6u2bdtq7dq1l0xy4RGGDDGdAAAAAIAMlytJ6tWrl3r16lXl/TabTVOnTtXUqVOrPObQoUOX7BsxYkT5mSyPkp8vjRkjzZkjRUSYTgMAAADgB8bLFa5CSYnUr5/0j39IubmSG0xNCQAAAMBCuXJHhw9bZ6h+zuGQpk2zilWtWtJLL5nJBgAAAKBSlCt3c/iw1LSpdOFC1ceUlEi1a7suEwAAAIBfZHQqdlQiP//yxUqS7PZLz2wBAAAAMIpyBQAAAABOQLkCAAAAACegXAEAAACAE1CuAAAAAMAJKFcAAAAA4ASUKwAAAABwAsqVu4mIkIKCLn9MUJB1HAAAAAC3wSLC7qZRI2nv3suvYxURYR0HAAAAwG1QrtxRo0aUJwAAAKCG4bJAAAAAAHACyhUAAAAAOAHlCgAAAACcgHIFAAAAAE5AuQIAAAAAJ6BcAQAAAIATUK4AAAAAwAkoVwAAAADgBJQrAAAAAHACyhUAAAAAOAHlCgAAAACcgHIFAAAAAE5AuQIAAAAAJ/AzHcAdORwOSVJhYaHhJLgWdrtd58+fV2Fhofz9/U3HgYdjvMHVGHNwJcYbXM2dxtyPneDHjnA5lKtKnD17VpIUGxtrOAkAAAAAd3D27FmFhYVd9hib40oqmJcpKytTTk6OQkNDZbPZTMfBr1RYWKjY2FgdOXJEderUMR0HHo7xBldjzMGVGG9wNXcacw6HQ2fPnlVMTIx8fC7/rSrOXFXCx8dH119/vekYcJI6deoY/4cS3oPxBldjzMGVGG9wNXcZc790xupHTGgBAAAAAE5AuQIAAAAAJ6BcwWMFBgZq0qRJCgwMNB0FXoDxBldjzMGVGG9wtZo65pjQAgAAAACcgDNXAAAAAOAElCsAAAAAcALKFQAAAAA4AeUKAAAAAJyAcgWP8sILL6hDhw4KDQ1VgwYN1KdPH+3du9d0LHiJF198UTabTaNHjzYdBR7s2LFjGjRokMLDwxUcHKxWrVrp008/NR0LHqq0tFRpaWmKi4tTcHCwbrzxRj333HNiPjQ4yz/+8Q/17t1bMTExstlsWr16dYX7HQ6HJk6cqOjoaAUHB6t79+7av3+/mbBXgHIFj7Jp0yalpKRo+/btWr9+vex2u3r27KmioiLT0eDhsrOz9eqrr6p169amo8CDnT59WgkJCfL399eaNWv0r3/9S7NmzVLdunVNR4OHmjFjhhYuXKj58+drz549mjFjhmbOnKmXX37ZdDR4iKKiIrVp00avvPJKpffPnDlT8+bNU3p6unbs2KFatWopMTFRFy5ccHHSK8NU7PBoJ0+eVIMGDbRp0yb993//t+k48FDnzp1T+/bttWDBAk2bNk1t27bV3LlzTceCBxo3bpy2bNmijz/+2HQUeIlevXopMjJSr7/+evm+fv36KTg4WMuWLTOYDJ7IZrMpIyNDffr0kWSdtYqJidEf/vAHjR07VpJUUFCgyMhIvfnmmxo4cKDBtJXjzBU8WkFBgSSpXr16hpPAk6WkpOjee+9V9+7dTUeBh8vMzFR8fLwGDBigBg0aqF27dlq8eLHpWPBgt99+u7KysrRv3z5J0hdffKHNmzfr7rvvNpwM3uDbb79Vbm5uhX+/hoWFqVOnTtq2bZvBZFXzMx0AqC5lZWUaPXq0EhISdOutt5qOAw+1cuVK7dy5U9nZ2aajwAt88803WrhwoVJTUzVhwgRlZ2dr5MiRCggIUHJysul48EDjxo1TYWGhmjVrJl9fX5WWlur5559XUlKS6WjwArm5uZKkyMjICvsjIyPL73M3lCt4rJSUFO3evVubN282HQUe6siRIxo1apTWr1+voKAg03HgBcrKyhQfH6/p06dLktq1a6fdu3crPT2dcoVq8c4772j58uVasWKFWrZsqV27dmn06NGKiYlhzAGV4LJAeKQRI0bo/fff14cffqjrr7/edBx4qM8++0wnTpxQ+/bt5efnJz8/P23atEnz5s2Tn5+fSktLTUeEh4mOjlaLFi0q7GvevLkOHz5sKBE83VNPPaVx48Zp4MCBatWqlR555BGNGTNGL7zwgulo8AJRUVGSpLy8vAr78/Lyyu9zN5QreBSHw6ERI0YoIyNDGzduVFxcnOlI8GDdunXTV199pV27dpX/xMfHKykpSbt27ZKvr6/piPAwCQkJlywvsW/fPjVu3NhQIni68+fPy8en4sdFX19flZWVGUoEbxIXF6eoqChlZWWV7yssLNSOHTvUuXNng8mqxmWB8CgpKSlasWKF3nvvPYWGhpZfjxsWFqbg4GDD6eBpQkNDL/k+X61atRQeHs73/FAtxowZo9tvv13Tp0/XAw88oE8++USLFi3SokWLTEeDh+rdu7eef/55NWrUSC1bttTnn3+u2bNn67HHHjMdDR7i3LlzOnDgQPntb7/9Vrt27VK9evXUqFEjjR49WtOmTdPNN9+suLg4paWlKSYmpnxGQXfDVOzwKDabrdL9S5Ys0aOPPuraMPBKXbt2ZSp2VKv3339f48eP1/79+xUXF6fU1FQNHTrUdCx4qLNnzyotLU0ZGRk6ceKEYmJi9NBDD2nixIkKCAgwHQ8e4KOPPtIdd9xxyf7k5GS9+eabcjgcmjRpkhYtWqQzZ86oS5cuWrBggW655RYDaX8Z5QoAAAAAnIDvXAEAAACAE1CuAAAAAMAJKFcAAAAA4ASUKwAAAABwAsoVAAAAADgB5QoAAAAAnIByBQAAAABOQLkCAAAAACegXAEA4GQ2m02rV682HQMA4GKUKwCAR3n00Udls9ku+bnrrrtMRwMAeDg/0wEAAHC2u+66S0uWLKmwLzAw0FAaAIC34MwVAMDjBAYGKioqqsJP3bp1JVmX7C1cuFB33323goODdcMNN+jdd9+t8PivvvpKd955p4KDgxUeHq5hw4bp3LlzFY5544031LJlSwUGBio6OlojRoyocH9+fr769u2rkJAQ3XzzzcrMzKzeFw0AMI5yBQDwOmlpaerXr5+++OILJSUlaeDAgdqzZ48kqaioSImJiapbt66ys7P1l7/8RRs2bKhQnhYuXKiUlBQNGzZMX331lTIzM3XTTTdV+B1TpkzRAw88oC+//FL33HOPkpKSdOrUKZe+TgCAa9kcDofDdAgAAJzl0Ucf1bJlyxQUFFRh/4QJEzRhwgTZbDY9/vjjWrhwYfl9v/nNb9S+fXstWLBAixcv1jPPPKMjR46oVq1akqS///3v6t27t3JychQZGamGDRtq8ODBmjZtWqUZbDabnn32WT333HOSrMJWu3ZtrVmzhu9+AYAH4ztXAACPc8cdd1QoT5JUr1698u3OnTtXuK9z587atWuXJGnPnj1q06ZNebGSpISEBJWVlWnv3r2y2WzKyclRt27dLpuhdevW5du1atVSnTp1dOLEiV/7kgAANQDlCgDgcWrVqnXJZXrOEhwcfEXH+fv7V7hts9lUVlZWHZEAAG6C71wBALzO9u3bL7ndvHlzSVLz5s31xRdfqKioqPz+LVu2yMfHR02bNlVoaKiaNGmirKwsl2YGALg/zlwBADxOcXGxcnNzK+zz8/NTRESEJOkvf/mL4uPj1aVLFy1fvlyffPKJXn/9dUlSUlKSJk2apOTkZE2ePFknT57Uk08+qUceeUSRkZGSpMmTJ+vxxx9XgwYNdPfdd+vs2bPasmWLnnzySde+UACAW6FcAQA8ztq1axUdHV1hX9OmTfX1119LsmbyW7lypYYPH67o6Gj96U9/UosWLSRJISEhWrdunUaNGqUOHTooJCRE/fr10+zZs8ufKzk5WRcuXNCcOXM0duxYRUREqH///q57gQAAt8RsgQAAr2Kz2ZSRkaE+ffqYjgIA8DB85woAAAAAnIByBQAAAABOwHeuAABehavhAQDVhTNXAAAAAOAElCsAAAAAcALKFQAAAAA4AeUKAAAAAJyAcgUAAAAATkC5AgAAAAAnoFwBAAAAgBNQrgAAAADACf4fEuJDcazPoc8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 5 and 6 (2 points) Computing sequence embedding\n",
        "\n",
        "Task 5 (1 point): Modify `infer_loop` function so that it computes the sequence-level embedding by doing a global average pooling over the embedding of the nucleotide tokens and return the sequence embedding. For instance, embedding of 4 sequences each containig 500 tokens and 128 embedding dimension of 4, 500, 128. By doing a global averaging pooling, the resulting dimension should be 4 by 128.\n",
        "\n",
        "Task 6 (1 point): The `inference` function below can only infer sequence embedding using a pre-trained hyenadna-tiny-1k-seqlen model. Modify it so that you can enable the function to infer sequence embedding using\n",
        "1. pre-trained and fine-tuned hyenadna-tiny-1k-seqlen from Task 3\n",
        "2. trained from scratch hyenadna-tiny-1k-seqlen from Task 3\n",
        "\n",
        "The function will need to return the sequence embedding of the 242 test sequences from the dummy_mouse_enhancers_ensembl dataset and the corresponding binary target labels of the 242 test sequences.\n",
        "\n",
        "Note that the fine-tuned and trained-from-scratched model return only the output of the classification head. You will need to figure out a way to let them return the sequence embedding from the last layer of the Hyena backbone. - turn off the model.use_head to avoid attaching SequenceDecoder?\n",
        "\n",
        "All 3 models should return the sequence embedding matrix of dimension 242 by 128."
      ],
      "metadata": {
        "id": "1UIG-RnMOe_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arcIbXNzHmyj"
      },
      "outputs": [],
      "source": [
        "#@title Computing sequence embedding\n",
        "\"\"\"\n",
        "Let's say you want to do inference on a dataset to grab a lot of embeddings,\n",
        "you can just loop thru a dataloader like this.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import transformers\n",
        "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
        "from pdb import set_trace\n",
        "\n",
        "# example, if you want to do look thru a dataloader and get embeddings\n",
        "\n",
        "def infer_loop(model, device, test_loader):\n",
        "    \"\"\"inference loop.\"\"\"\n",
        "    # model.eval()\n",
        "    model.use_head = False # return only the sequence embedding, not classification head(logits)\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    with torch.inference_mode():\n",
        "        for data, target in test_loader: # run batch size times\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            embeddings.append(model(data))\n",
        "            labels.append(target)\n",
        "\n",
        "    return torch.mean(torch.cat(embeddings, dim=0), dim=1), torch.cat(labels, dim = 0)\n",
        "\n",
        "def inference(model = None):\n",
        "\n",
        "    '''\n",
        "    this selects which backbone to use, and grabs weights/ config from HF\n",
        "    4 options:\n",
        "      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok\n",
        "      'hyenadna-small-32k-seqlen'\n",
        "      'hyenadna-medium-160k-seqlen'  # inference only on colab\n",
        "      'hyenadna-medium-450k-seqlen'  # inference only on colab\n",
        "      'hyenadna-large-1m-seqlen'  # inference only on colab\n",
        "    '''\n",
        "\n",
        "    # select model\n",
        "    pretrained_model_name = 'hyenadna-tiny-1k-seqlen'  # use None if training from scratch\n",
        "\n",
        "    max_lengths = {\n",
        "        'hyenadna-tiny-1k-seqlen': 1024,\n",
        "        'hyenadna-small-32k-seqlen': 32768,\n",
        "        'hyenadna-medium-160k-seqlen': 160000,\n",
        "        'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
        "        'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
        "    }\n",
        "\n",
        "    # let's fix the max_length (to reduce the padding amount, conserves memory)\n",
        "    max_length = 500\n",
        "\n",
        "    # data settings:\n",
        "    # we need to choose the dataset and batch size to loop thru\n",
        "    dataset_name = 'dummy_mouse_enhancers_ensembl'\n",
        "    batch_size = 4\n",
        "    use_padding = True\n",
        "    rc_aug = False  # reverse complement augmentation\n",
        "    add_eos = False  # add end of sentence token\n",
        "\n",
        "    # we need these for the decoder head, if using\n",
        "    use_head = False\n",
        "    n_classes = 2  # not used for embeddings only\n",
        "\n",
        "    # you can override with your own backbone config here if you want,\n",
        "    # otherwise we'll load the HF one in None\n",
        "    backbone_cfg = None\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # instantiate the model (pretrained here)\n",
        "    if model == None:\n",
        "        # use the pretrained Huggingface wrapper (without fine-tuning)\n",
        "        model = HyenaDNAPreTrainedModel.from_pretrained(\n",
        "            './checkpoints',\n",
        "            pretrained_model_name,\n",
        "            download=True,\n",
        "            config=backbone_cfg,\n",
        "            device=device,\n",
        "            use_head=use_head,\n",
        "            n_classes=n_classes,\n",
        "        )\n",
        "\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = CharacterTokenizer(\n",
        "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
        "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
        "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
        "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
        "    )\n",
        "\n",
        "    # Sample small dataloader w/ GenomicBenchmarks\n",
        "\n",
        "    ds_test = GenomicBenchmarkDataset(\n",
        "        max_length = max_length,\n",
        "        use_padding = use_padding,\n",
        "        split = 'test',\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_name=dataset_name,\n",
        "        rc_aug=rc_aug,\n",
        "        add_eos=add_eos,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return infer_loop(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the inference function and save the embedding of all 3 models and the target labels for the following t-SNE plots."
      ],
      "metadata": {
        "id": "cptAytvmTOXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSkwVPfAHnAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e343c8-0218-4251-d9b2-77d091088ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "already downloaded test-dummy_mouse_enhancers_ensembl\n",
            "Using device: cuda\n",
            "already downloaded test-dummy_mouse_enhancers_ensembl\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-69bc72104001>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_ckpt = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights ok!\n",
            "already downloaded test-dummy_mouse_enhancers_ensembl\n"
          ]
        }
      ],
      "source": [
        "seq_emb_sc, targets = inference(fromscratch) # infer using from-scratch model\n",
        "seq_emb_ft, targets = inference(finetuned) # infer using pre-trained + finet-tuned model\n",
        "seq_emb_pt, targets = inference() # infer using pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq_emb_sc.shape) # expected: torch.Size([242, 128])\n",
        "print(seq_emb_ft.shape) # expected: torch.Size([242, 128])\n",
        "print(seq_emb_pt.shape) # expected: torch.Size([242, 128])\n",
        "print(targets.shape) # expected: torch.Size([242, 1])\n",
        "print(targets.sum().item()) # expected: 121 as the number of positive sequences"
      ],
      "metadata": {
        "id": "4VLRAFBhSbzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a52cf0-6ae4-421b-cba6-069ba02b2ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([242, 128])\n",
            "torch.Size([242, 128])\n",
            "torch.Size([242, 128])\n",
            "torch.Size([242, 1])\n",
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 7 (1 point) visualize the sequence embeddings from the 3 models with t-SNE\n",
        "Generate 3 t-SNE plots using the sequence embedding from the pre-trained-only, pretrrained + fine-tuned, and trained from scratch model. Your plots may not look the same as the ones below but we should see a slightly better clustering of the sequence embedding from the pretrrained + fine-tuned model compared to the other two models.\n",
        "\n",
        "You may also train for more epochs in Task 3 (e.g., 100 epochs, which takes about 10 minutes with T4 GPU) to see better clustering from pretrrained + fine-tuned and trained from scratch models."
      ],
      "metadata": {
        "id": "KT2axglaUDcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1rs0t9RIWKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1d238774-c5a5-4e71-be64-73dce889460b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cc79cf5dbe46>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplot_emb_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_emb_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t-SNE sequence embedding of pre-trained\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mplot_emb_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_emb_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t-SNE sequence embedding of from-scratch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplot_emb_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_emb_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t-SNE sequence embedding of pre-trained+fine-tuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cc79cf5dbe46>\u001b[0m in \u001b[0;36mplot_emb_tsne\u001b[0;34m(seq_emb, my_title)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsne_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsne_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "targets = targets.flatten()\n",
        "def plot_emb_tsne(seq_emb, my_title):\n",
        "  embedding = seq_emb.cpu().numpy()\n",
        "  tsne = TSNE(n_components=2, random_state=42, perplexity = 30)\n",
        "  tsne_embedding = tsne.fit_transform(embedding)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.scatter(tsne_embedding[targets == 0, 0], tsne_embedding[targets == 0, 1], alpha=0.6, color='b')\n",
        "  plt.scatter(tsne_embedding[targets == 1, 0], tsne_embedding[targets == 1, 1], alpha=0.6, color='r')\n",
        "\n",
        "  plt.title(my_title)\n",
        "  plt.xlabel('t-SNE Dimension 1')\n",
        "  plt.ylabel('t-SNE Dimension 2')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_emb_tsne(seq_emb_pt, my_title=\"t-SNE sequence embedding of pre-trained\")\n",
        "plot_emb_tsne(seq_emb_sc, my_title=\"t-SNE sequence embedding of from-scratch\")\n",
        "plot_emb_tsne(seq_emb_ft, my_title=\"t-SNE sequence embedding of pre-trained+fine-tuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3 Task 8 (1 point) Autoregressive next token prediction\n",
        "Lastly we will experiment use a pre-trained HyenaDNA to perform next token prediction on a sequence of ACTB gene (a housekeeping gene in the human and many species). This task is independent from the 7 tasks above."
      ],
      "metadata": {
        "id": "RFtghoHipQQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "#@title AutoModelForCausalLM\n",
        "hyena_model_url = \"LongSafari/hyenadna-medium-160k-seqlen-hf\"\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(hyena_model_url, trust_remote_code=True)\n",
        "# lm_model.eval() # uncomment to print the architecture of the pre-traiend HyendaDNA model"
      ],
      "metadata": {
        "id": "ZAvmfgjfo4V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the ACTB sequence saved in your Google Drive using the SeqIO library. There will be a prompt asking you \"Permit this notebook to access your Google Drive files\". Click continue and do not check any permission in the new prompt window and click continue again."
      ],
      "metadata": {
        "id": "OLZFlhFfdQ-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "QFAIvcO0jyIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/Colab_Notebooks/Comp 565/Assignment 3/ACTB_with_promoter_and_introns.fa\"\n",
        "record = SeqIO.read(filepath, \"fasta\")\n",
        "ACTB_sequence_id = record.id\n",
        "ACTB_sequence = str(record.seq).upper()\n",
        "print(len(ACTB_sequence))\n",
        "print(ACTB_sequence[:200])"
      ],
      "metadata": {
        "id": "uJbKzXDadIXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a shuffled sequence from the ACTB sequence as a negative control for assessing the next token prediction perplexity."
      ],
      "metadata": {
        "id": "YMXFFlp9jMf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle ACTB\n",
        "random.seed(42)\n",
        "shuffle_sequence = lambda sequence: \\\n",
        "  ''.join(random.sample(sequence, len(sequence)))\n",
        "ACTB_sequence_shuffled = shuffle_sequence(ACTB_sequence) # test sequence for next token prediction"
      ],
      "metadata": {
        "id": "cK7kYDnujLl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `convert_sequence` converts the string sequence to list of label IDs that HyenaDNA can operate on. The integers 7,  8,  9, 10, 11 correspond to A, C, G, T, and N. The number 0 and 1 correspond to the special tokens of the start of sequence (SOS) and end of sequence (EOS), respectively."
      ],
      "metadata": {
        "id": "oaV-9jhjlFiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_sequence(seq):\n",
        "\n",
        "  # Create tokenizer\n",
        "  tokenizer = CharacterTokenizer(\n",
        "      characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
        "      model_max_length=len(seq) + 2,  # to account for special tokens, like EOS\n",
        "      add_special_tokens=False,  # we handle special tokens elsewhere\n",
        "      padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
        "  )\n",
        "\n",
        "  # Tokenize sequence (automatically adds start and end)\n",
        "  ids = tokenizer(seq, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  # Discard end token from input as we want the model to predict EOS\n",
        "  input_ids = ids[:,:-1]\n",
        "\n",
        "  # Discard SOS token from output as the model starts predicting after EOS\n",
        "  label_ids = ids[:,1:]\n",
        "\n",
        "  # Flatten labels\n",
        "  label_ids = label_ids.view(-1).squeeze() # .view(-1) calculates the size of the dimension automatically so that the reshaped tensor contains the same number of elements as the original tensor.\n",
        "\n",
        "  return input_ids, label_ids"
      ],
      "metadata": {
        "id": "AGyTXMG2l6UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actb_input_ids, actb_label_ids = convert_sequence(ACTB_sequence)\n",
        "actb_input_ids_shuffled, actb_label_ids_shuffled = convert_sequence(ACTB_sequence_shuffled)"
      ],
      "metadata": {
        "id": "mzG_VS9wmmbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make autoregressive prediction of all nucleotide using the pre-trained HyenaDNA as follows, which outputs the logits (i.e., predictions before the softmax over the nucleotides). This outputs a 4455 by 16 matrix, corresponding to the logistic scores across the vocabulary of size 16. Note that for each token position, only 7, 8, 9, or 10 (i.e., A, C, G, T) is true positive for th ACTB sequence."
      ],
      "metadata": {
        "id": "QHm2cn8jNfBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = lm_model(actb_input_ids).logits.squeeze(0)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "t37Lmu5TNeHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the function below to compute the perplexity. Perplexity is defined as the exponentiated average negative log-likelihood or equivalently, cross-entropy loss over a set of predictions. In the context of nucleotides or any sequence modeling, perplexity measures how well a probabilistic model predicts the next nucleotide in a sequence."
      ],
      "metadata": {
        "id": "D890Sr4fNEiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def compute_perplexity(model, input_ids, labels_ids):\n",
        "\n",
        "    # Predict token-level logits from LM model\n",
        "    logits = model(input_ids).logits.squeeze(0)\n",
        "\n",
        "    # (optional) you may ignore ambiguous_token 'N' when computing the perplexity\n",
        "    ambiguous_token_id = 11\n",
        "    mask = labels_ids != ambiguous_token_id\n",
        "    logits = logits[mask]\n",
        "    labels_ids = labels_ids[mask]\n",
        "\n",
        "    # COMPLETE THIS FUNCTION BELOW\n",
        "    ce_loss = F.cross_entropy(logits, labels_ids, reduction='mean')\n",
        "    perplexity = torch.exp(ce_loss)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "Uk2x-Y1xchW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the average perplexity of the NTP on the original and shuffled ACTB sequences. The former should be lower than the latter."
      ],
      "metadata": {
        "id": "ytBw54qSkn-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original ACTB ppl: {compute_perplexity(lm_model, actb_input_ids, actb_label_ids)}\")\n",
        "print(f\"Shuffled ACTB ppl: {compute_perplexity(lm_model, actb_input_ids_shuffled, actb_label_ids_shuffled)}\")"
      ],
      "metadata": {
        "id": "0ZsWnuYykYdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not for grading, but some insights why the perplexity is higher for the randomly shuffled seq: the randomly shuffled seq contains no meaningful biological pattern the the model can learn from (randomly shuffling breaks any of the regular motifs, dependencies or other structures), making the model less certain in its predictions, thereby leading to higher perplexity."
      ],
      "metadata": {
        "id": "Gn2Q16R1vbXT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qsRya6hEryfg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}